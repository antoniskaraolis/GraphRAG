id,title,abstract,year,update_date,url,version_count
0704.0061,Intersection Bodies and Generalized Cosine Transforms,"Intersection bodies represent a remarkable class of geometric objects associated with sections of star bodies and invoking Radon transforms, generalized cosine transforms, and the relevant Fourier analysis. The main focus of this article is interrelation between generalized cosine transforms of different kinds in the context of their application to investigation of a certain family of intersection bodies, which we call $\lam$-intersection bodies. The latter include $k$-intersection bodies (in the sense of A. Koldobsky) and unit balls of finite-dimensional subspaces of $L_p$-spaces. In particular, we show that restrictions onto lower dimensional subspaces of the spherical Radon transforms and the generalized cosine transforms preserve their integral-geometric structure. We apply this result to the study of sections of $\lam$-intersection bodies. New characterizations of this class of bodies are obtained and examples are given. We also review some known facts and give them new proofs.",2007,2007-05-23,https://arxiv.org/abs/0704.0061,2
0705.2363,Lasso type classifiers with a reject option,"We consider the problem of binary classification where one can, for a particular cost, choose not to classify an observation. We present a simple proof for the oracle inequality for the excess risk of structural risk minimizers using a lasso type penalty.",2007,2009-09-29,https://arxiv.org/abs/0705.2363,1
0704.0309,The Complexity of HCP in Digraps with Degree Bound Two,"The Hamiltonian cycle problem (HCP) in digraphs D with degree bound two is solved by two mappings in this paper. The first bijection is between an incidence matrix C_{nm} of simple digraph and an incidence matrix F of balanced bipartite undirected graph G; The second mapping is from a perfect matching of G to a cycle of D. It proves that the complexity of HCP in D is polynomial, and finding a second non-isomorphism Hamiltonian cycle from a given Hamiltonian digraph with degree bound two is also polynomial. Lastly it deduces P=NP base on the results.",2007,2011-11-09,https://arxiv.org/abs/0704.0309,3
0704.0010,"Partial cubes: structures, characterizations, and constructions","Partial cubes are isometric subgraphs of hypercubes. Structures on a graph defined by means of semicubes, and Djokovi\'{c}'s and Winkler's relations play an important role in the theory of partial cubes. These structures are employed in the paper to characterize bipartite graphs and partial cubes of arbitrary dimension. New characterizations are established and new proofs of some known results are given. The operations of Cartesian product and pasting, and expansion and contraction processes are utilized in the paper to construct new partial cubes from old ones. In particular, the isometric and lattice dimensions of finite partial cubes obtained by means of these operations are calculated.",2007,2007-05-23,https://arxiv.org/abs/0704.0010,1
0704.0322,Emergence of spatiotemporal chaos driven by far-field breakup of spiral waves in the plankton ecological systems,"Alexander B. Medvinsky \emph{et al} [A. B. Medvinsky, I. A. Tikhonova, R. R. Aliev, B.-L. Li, Z.-S. Lin, and H. Malchow, Phys. Rev. E \textbf{64}, 021915 (2001)] and Marcus R. Garvie \emph{et al} [M. R. Garvie and C. Trenchea, SIAM J. Control. Optim. \textbf{46}, 775-791 (2007)] shown that the minimal spatially extended reaction-diffusion model of phytoplankton-zooplankton can exhibit both regular, chaotic behavior, and spatiotemporal patterns in a patchy environment. Based on that, the spatial plankton model is furtherly investigated by means of computer simulations and theoretical analysis in the present paper when its parameters would be expected in the case of mixed Turing-Hopf bifurcation region. Our results show that the spiral waves exist in that region and the spatiotemporal chaos emerge, which arise from the far-field breakup of the spiral waves over large ranges of diffusion coefficients of phytoplankton and zooplankton. Moreover, the spatiotemporal chaos arising from the far-field breakup of spiral waves does not gradually involve the whole space within that region. Our results are confirmed by means of computation spectra and nonlinear bifurcation of wave trains. Finally, we give some explanations about the spatially structured patterns from the community level.",2007,2009-05-29,https://arxiv.org/abs/0704.0322,3
0704.0022,Stochastic Lie group integrators,"We present Lie group integrators for nonlinear stochastic differential equations with non-commutative vector fields whose solution evolves on a smooth finite dimensional manifold. Given a Lie group action that generates transport along the manifold, we pull back the stochastic flow on the manifold to the Lie group via the action, and subsequently pull back the flow to the corresponding Lie algebra via the exponential map. We construct an approximation to the stochastic flow in the Lie algebra via closed operations and then push back to the Lie group and then to the manifold, thus ensuring our approximation lies in the manifold. We call such schemes stochastic Munthe-Kaas methods after their deterministic counterparts. We also present stochastic Lie group integration schemes based on Castell--Gaines methods. These involve using an underlying ordinary differential integrator to approximate the flow generated by a truncated stochastic exponential Lie series. They become stochastic Lie group integrator schemes if we use Munthe-Kaas methods as the underlying ordinary differential integrator. Further, we show that some Castell--Gaines methods are uniformly more accurate than the corresponding stochastic Taylor schemes. Lastly we demonstrate our methods by simulating the dynamics of a free rigid body such as a satellite and an autonomous underwater vehicle both perturbed by two independent multiplicative stochastic noise processes.",2007,2007-10-16,https://arxiv.org/abs/0704.0022,2
0708.1627,Rearranging Edgeworth-Cornish-Fisher Expansions,"This paper applies a regularization procedure called increasing rearrangement to monotonize Edgeworth and Cornish-Fisher expansions and any other related approximations of distribution and quantile functions of sample statistics. Besides satisfying the logical monotonicity, required of distribution and quantile functions, the procedure often delivers strikingly better approximations to the distribution and quantile functions of the sample mean than the original Edgeworth-Cornish-Fisher expansions.",2007,2017-11-23,https://arxiv.org/abs/0708.1627,2
0704.0217,Capacity of a Multiple-Antenna Fading Channel with a Quantized Precoding Matrix,"Given a multiple-input multiple-output (MIMO) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. Here we analyze the performance of Random Vector Quantization (RVQ), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. We assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using B bits. We first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of B, where large system refers to the limit as B and the number of transmit and receive antennas all go to infinity with fixed ratios. With beamforming RVQ is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. The performance of RVQ is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. We subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic RVQ performance with optimal and linear receivers (matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. Given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear MMSE receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.",2007,2010-08-27,https://arxiv.org/abs/0704.0217,2
0708.4376,Fast estimation of multivariate stochastic volatility,"In this paper we develop a Bayesian procedure for estimating multivariate stochastic volatility (MSV) using state space models. A multiplicative model based on inverted Wishart and multivariate singular beta distributions is proposed for the evolution of the volatility, and a flexible sequential volatility updating is employed. Being computationally fast, the resulting estimation procedure is particularly suitable for on-line forecasting. Three performance measures are discussed in the context of model selection: the log-likelihood criterion, the mean of standardized one-step forecast errors, and sequential Bayes factors. Finally, the proposed methods are applied to a data set comprising eight exchange rates vis-a-vis the US dollar.",2007,2008-12-02,https://arxiv.org/abs/0708.4376,2
0704.0002,Sparsity-certifying Graph Decompositions,"We describe a new algorithm, the $(k,\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. Special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. In particular, our colored pebbles generalize and strengthen the previous results of Lee and Streinu and give a new proof of the Tutte-Nash-Williams characterization of arboricity. We also present a new decomposition that certifies sparsity based on the $(k,\ell)$-pebble game with colors. Our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and Westermann and Hendrickson.",2007,2008-12-13,https://arxiv.org/abs/0704.0002,2
1105.6154,Conditional Quantile Processes based on Series or Many Regressors,"Quantile regression (QR) is a principal regression method for analyzing the impact of covariates on outcomes. The impact is described by the conditional quantile function and its functionals. In this paper we develop the nonparametric QR-series framework, covering many regressors as a special case, for performing inference on the entire conditional quantile function and its linear functionals. In this framework, we approximate the entire conditional quantile function by a linear combination of series terms with quantile-specific coefficients and estimate the function-valued coefficients from the data. We develop large sample theory for the QR-series coefficient process, namely we obtain uniform strong approximations to the QR-series coefficient process by conditionally pivotal and Gaussian processes. Based on these strong approximations, or couplings, we develop four resampling methods (pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be used for inference on the entire QR-series coefficient function. We apply these results to obtain estimation and inference methods for linear functionals of the conditional quantile function, such as the conditional quantile function itself, its partial derivatives, average partial derivatives, and conditional average partial derivatives. Specifically, we obtain uniform rates of convergence and show how to use the four resampling methods mentioned above for inference on the functionals. All of the above results are for function-valued parameters, holding uniformly in both the quantile index and the covariate value, and covering the pointwise case as a by-product. We demonstrate the practical utility of these results with an example, where we estimate the price elasticity function and test the Slutsky condition of the individual demand for gasoline, as indexed by the individual unobserved propensity for gasoline consumption.",2011,2018-08-13,https://arxiv.org/abs/1105.6154,4
0704.0615,Parsimony via concensus,"The parsimony score of a character on a tree equals the number of state changes required to fit that character onto the tree. We show that for unordered, reversible characters this score equals the number of tree rearrangements required to fit the tree onto the character. We discuss implications of this connection for the debate over the use of consensus trees or total evidence, and show how it provides a link between incongruence of characters and recombination.",2007,2013-10-02,https://arxiv.org/abs/0704.0615,5
1707.07625,Restoring a smooth function from its noisy integrals,"Numerical (and experimental) data analysis often requires the restoration of a smooth function from a set of sampled integrals over finite bins. We present the bin hierarchy method that efficiently computes the maximally smooth function from the sampled integrals using essentially all the information contained in the data. We perform extensive tests with different classes of functions and levels of data quality, including Monte Carlo data suffering from a severe sign problem and physical data for the Green's function of the Fr\""ohlich polaron.",2017,2018-05-14,https://arxiv.org/abs/1707.07625,2
0704.0014,Iterated integral and the loop product,In this article we discuss a relation between the string topology and differential forms based on the theory of Chen's iterated integrals and the cyclic bar complex.,2007,2009-09-29,https://arxiv.org/abs/0704.0014,1
0704.0648,Behavioral response to strong aversive stimuli: A neurodynamical model,"In this paper a theoretical model of functioning of a neural circuit during a behavioral response has been proposed. A neural circuit can be thought of as a directed multigraph whose each vertex is a neuron and each edge is a synapse. It has been assumed in this paper that the behavior of such circuits is manifested through the collective behavior of neurons belonging to that circuit. Behavioral information of each neuron is contained in the coefficients of the fast Fourier transform (FFT) over the output spike train. Those coefficients form a vector in a multidimensional vector space. Behavioral dynamics of a neuronal network in response to strong aversive stimuli has been studied in a vector space in which a suitable pseudometric has been defined. The neurodynamical model of network behavior has been formulated in terms of existing memory, synaptic plasticity and feelings. The model has an analogy in classical electrostatics, by which the notion of force and potential energy has been introduced. Since the model takes input from each neuron in a network and produces a behavior as the output, it would be extremely difficult or may even be impossible to implement. But with the help of the model a possible explanation for an hitherto unexplained neurological observation in human brain has been offered. The model is compatible with a recent model of sequential behavioral dynamics. The model is based on electrophysiology, but its relevance to hemodynamics has been outlined.",2007,2007-05-23,https://arxiv.org/abs/0704.0648,1
0807.3059,Agent-based model of competition in a social structure,"Indirect competition emerged from the complex organization of human societies, and knowledge of the existing network topology may aid in developing effective strategies for success. Here, we propose an agent-based model of competition with systems co-existing in a `small-world' social network. We show that within the range of parameter values obtained from the model and empirical data, the network evolution is highly dependent on $k$, the local parameter describing the density of neighbors in the network. The model applied to language death and competition of telecommunication companies show strong correspondence with empirical data.",2008,2011-06-28,https://arxiv.org/abs/0807.3059,2
0704.0046,A limit relation for entropy and channel capacity per unit cost,"In a quantum mechanical model, Diosi, Feldmann and Kosloff arrived at a conjecture stating that the limit of the entropy of certain mixtures is the relative entropy as system size goes to infinity. The conjecture is proven in this paper for density matrices. The first proof is analytic and uses the quantum law of large numbers. The second one clarifies the relation to channel capacity per unit cost for classical-quantum channels. Both proofs lead to generalization of the conjecture.",2007,2009-11-13,https://arxiv.org/abs/0704.0046,1
0704.0429,"Quantitative Resolution to some ""Absolute Discrepancies"" in Cancer Theories: a View from Phage lambda Genetic Switch","Is it possible to understand cancer? Or more specifically, is it possible to understand cancer from genetic side? There already many answers in literature. The most optimistic one has claimed that it is mission-possible. Duesberg and his colleagues reviewed the impressive amount of research results on cancer accumulated over 100 years. It confirms the a general opinion that considering all available experimental results and clinical observations there is no cancer theory without major difficulties, including the prevailing gene-based cancer theories. They have then listed 9 ""absolute discrepancies"" for such cancer theory. In this letter the quantitative evidence against one of their major reasons for dismissing mutation cancer theory, by both in vivo experiment and a first principle computation, is explicitly pointed out.",2007,2008-11-26,https://arxiv.org/abs/0704.0429,1
0709.2694,Innovation Success and Structural Change: An Abstract Agent Based Study,"A model is developed to study the effectiveness of innovation and its impact on structure creation and structure change on agent-based societies. The abstract model that is developed is easily adapted to any particular field. In any interacting environment, the agents receive something from the environment (the other agents) in exchange for their effort and pay the environment a certain amount of value for the fulfilling of their needs or for the very price of existence in that environment. This is coded by two bit strings and the dynamics of the exchange is based on the matching of these strings to those of the other agents. Innovation is related to the adaptation by the agents of their bit strings to improve some utility function.",2007,2010-08-31,https://arxiv.org/abs/0709.2694,1
0704.1546,Optimal flexibility for conformational transitions in macromolecules,"Conformational transitions in macromolecular complexes often involve the reorientation of lever-like structures. Using a simple theoretical model, we show that the rate of such transitions is drastically enhanced if the lever is bendable, e.g. at a localized ""hinge''. Surprisingly, the transition is fastest with an intermediate flexibility of the hinge. In this intermediate regime, the transition rate is also least sensitive to the amount of ""cargo'' attached to the lever arm, which could be exploited by molecular motors. To explain this effect, we generalize the Kramers-Langer theory for multi-dimensional barrier crossing to configuration dependent mobility matrices.",2007,2009-11-13,https://arxiv.org/abs/0704.1546,1
0704.0492,Refuting the Pseudo Attack on the REESSE1+ Cryptosystem,"We illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition Z/M - L/Ak < 1/(2 Ak^2) is not sufficient for f(i) + f(j) = f(k). Illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. Demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * D at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. Further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. We explain why Cx = Ax * W^f(x) (% M) is changed to Cx = (Ax * W^f(x))^d (% M) in REESSE1+ v2.1. To the signature fraud, we point out that [8] misunderstands the existence of T^-1 and Q^-1 % (M-1), and forging of Q can be easily avoided through moving H. Therefore, the conclusion of [8] that REESSE1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in REESSE1+) is fully incorrect, and as long as the parameter Omega is fitly selected, REESSE1+ with Cx = Ax * W^f(x) (% M) is secure.",2007,2010-02-04,https://arxiv.org/abs/0704.0492,2
1709.06072,Bounds on Discrete Fourier Transform of Random Mask,"This paper proposes some bounds on the maximum of magnitude of a random mask in Fourier domain. The random mask is used in random sampling scheme. Having a bound on the maximum value of a random mask in Fourier domain is very useful for some iterative recovery methods that use thresholding operator. In this paper, we propose some different bounds and compare them with the empirical examples.",2017,2017-09-19,https://arxiv.org/abs/1709.06072,1
0801.3353,"Evolutionarily stable strategies of random games, and the vertices of random polygons","An evolutionarily stable strategy (ESS) is an equilibrium strategy that is immune to invasions by rare alternative (``mutant'') strategies. Unlike Nash equilibria, ESS do not always exist in finite games. In this paper we address the question of what happens when the size of the game increases: does an ESS exist for ``almost every large'' game? Letting the entries in the $n\times n$ game matrix be independently randomly chosen according to a distribution $F$, we study the number of ESS with support of size $2.$ In particular, we show that, as $n\to \infty$, the probability of having such an ESS: (i) converges to 1 for distributions $F$ with ``exponential and faster decreasing tails'' (e.g., uniform, normal, exponential); and (ii) converges to $1-1/\sqrt{e}$ for distributions $F$ with ``slower than exponential decreasing tails'' (e.g., lognormal, Pareto, Cauchy). Our results also imply that the expected number of vertices of the convex hull of $n$ random points in the plane converges to infinity for the distributions in (i), and to 4 for the distributions in (ii).",2008,2022-09-22,https://arxiv.org/abs/0801.3353,1
0704.0008,Numerical solution of shock and ramp compression for general material properties,"A general formulation was developed to represent material models for applications in dynamic loading. Numerical methods were devised to calculate response to shock and ramp compression, and ramp decompression, generalizing previous solutions for scalar equations of state. The numerical methods were found to be flexible and robust, and matched analytic results to a high accuracy. The basic ramp and shock solution methods were coupled to solve for composite deformation paths, such as shock-induced impacts, and shock interactions with a planar interface between different materials. These calculations capture much of the physics of typical material dynamics experiments, without requiring spatially-resolving simulations. Example calculations were made of loading histories in metals, illustrating the effects of plastic work on the temperatures induced in quasi-isentropic and shock-release experiments, and the effect of a phase transition.",2007,2009-02-05,https://arxiv.org/abs/0704.0008,3
0704.1711,"Dynamical Equilibrium, trajectories study in an economical system. The case of the labor market","The paper deals with the study of labor market dynamics, and aims to characterize its equilibriums and possible trajectories. The theoretical background is the theory of the segmented labor market. The main idea is that this theory is well adapted to interpret the observed trajectories, due to the heterogeneity of the work situations.",2007,2007-07-09,https://arxiv.org/abs/0704.1711,2
0705.0568,Bivariate linear mixed models using SAS proc MIXED,"Bivariate linear mixed models are useful when analyzing longitudinal data of two associated markers. In this paper, we present a bivariate linear mixed model including random effects or first-order auto-regressive process and independent measurement error for both markers. Codes and tricks to fit these models using SAS Proc MIXED are provided. Limitations of this program are discussed and an example in the field of HIV infection is shown. Despite some limitations, SAS Proc MIXED is a useful tool that may be easily extendable to multivariate response in longitudinal studies.",2007,2007-05-23,https://arxiv.org/abs/0705.0568,1
0705.0304,Mod\'elisations prospectives de l'occupation du sol. Le cas d'une montagne m\'editerran\'eenne,"The authors apply three methods of prospective modelling to high resolution georeferenced land cover data in a Mediterranean mountain area: GIS approach, non linear parametric model and neuronal network. Land cover prediction to the latest known date is used to validate the models. In the frame of spatial-temporal dynamics in open systems results are encouraging and comparable. Correct prediction scores are about 73 %. The results analysis focuses on geographic location, land cover categories and parametric distance to reality of the residues. Crossing the three models show the high degree of convergence and a relative similitude of the results obtained by the two statistic approaches compared to the GIS supervised model. Steps under work are the application of the models to other test areas and the identification of respective advantages to develop an integrated model.",2007,2007-05-23,https://arxiv.org/abs/0705.0304,2
1605.02869,An Efficient and Flexible Spike Train Model via Empirical Bayes,"Accurate statistical models of neural spike responses can characterize the information carried by neural populations. But the limited samples of spike counts during recording usually result in model overfitting. Besides, current models assume spike counts to be Poisson-distributed, which ignores the fact that many neurons demonstrate over-dispersed spiking behaviour. Although the Negative Binomial Generalized Linear Model (NB-GLM) provides a powerful tool for modeling over-dispersed spike counts, the maximum likelihood-based standard NB-GLM leads to highly variable and inaccurate parameter estimates. Thus, we propose a hierarchical parametric empirical Bayes method to estimate the neural spike responses among neuronal population. Our method integrates both Generalized Linear Models (GLMs) and empirical Bayes theory, which aims to (1) improve the accuracy and reliability of parameter estimation, compared to the maximum likelihood-based method for NB-GLM and Poisson-GLM; (2) effectively capture the over-dispersion nature of spike counts from both simulated data and experimental data; and (3) provide insight into both neural interactions and spiking behaviours of the neuronal populations. We apply our approach to study both simulated data and experimental neural data. The estimation of simulation data indicates that the new framework can accurately predict mean spike counts simulated from different models and recover the connectivity weights among neural populations. The estimation based on retinal neurons demonstrate the proposed method outperforms both NB-GLM and Poisson-GLM in terms of the predictive log-likelihood of held-out data. Codes are available in https://doi.org/10.5281/zenodo.4704423",2016,2021-06-17,https://arxiv.org/abs/1605.02869,6
0704.2200,Boolean network model predicts cell cycle sequence of fission yeast,"A Boolean network model of the cell-cycle regulatory network of fission yeast (Schizosaccharomyces Pombe) is constructed solely on the basis of the known biochemical interaction topology. Simulating the model in the computer, faithfully reproduces the known sequence of regulatory activity patterns along the cell cycle of the living cell. Contrary to existing differential equation models, no parameters enter the model except the structure of the regulatory circuitry. The dynamical properties of the model indicate that the biological dynamical sequence is robustly implemented in the regulatory network, with the biological stationary state G1 corresponding to the dominant attractor in state space, and with the biological regulatory sequence being a strongly attractive trajectory. Comparing the fission yeast cell-cycle model to a similar model of the corresponding network in S. cerevisiae, a remarkable difference in circuitry, as well as dynamics is observed. While the latter operates in a strongly damped mode, driven by external excitation, the S. pombe network represents an auto-excited system with external damping.",2007,2015-05-13,https://arxiv.org/abs/0704.2200,1
0704.0050,Intelligent location of simultaneously active acoustic emission sources: Part II,"Part I describes an intelligent acoustic emission locator, while Part II discusses blind source separation, time delay estimation and location of two continuous acoustic emission sources. Acoustic emission (AE) analysis is used for characterization and location of developing defects in materials. AE sources often generate a mixture of various statistically independent signals. A difficult problem of AE analysis is separation and characterization of signal components when the signals from various sources and the mode of mixing are unknown. Recently, blind source separation (BSS) by independent component analysis (ICA) has been used to solve these problems. The purpose of this paper is to demonstrate the applicability of ICA to locate two independent simultaneously active acoustic emission sources on an aluminum band specimen. The method is promising for non-destructive testing of aircraft frame structures by acoustic emission analysis.",2007,2007-05-23,https://arxiv.org/abs/0704.0050,1
1707.09453,Single-shot multispectral imaging with a monochromatic camera,"Multispectral imaging plays an important role in many applications from astronomical imaging, earth observation to biomedical imaging. However, the current technologies are complex with multiple alignment-sensitive components, predetermined spatial and spectral parameters by manufactures. Here, we demonstrate a single-shot multispectral imaging technique that gives flexibility to end-users with a very simple optical setup, thank to spatial correlation and spectral decorrelation of speckle patterns. These seemingly random speckle patterns are point spreading functions (PSFs) generated by light from point sources propagating through a strongly scattering medium. The spatial correlation of PSFs allows image recovery with deconvolution techniques, while the spectral decorrelation allows them to play the role of tune-able spectral filters in the deconvolution process. Our demonstrations utilizing optical physics of strongly scattering media and computational imaging present the most cost-effective approach for multispectral imaging with great advantages.",2017,2021-09-17,https://arxiv.org/abs/1707.09453,1
0704.3686,Improving Estimates of Monotone Functions by Rearrangement,"Suppose that a target function is monotonic, namely, weakly increasing, and an original estimate of the target function is available, which is not weakly increasing. Many common estimation methods used in statistics produce such estimates. We show that these estimates can always be improved with no harm using rearrangement techniques: The rearrangement methods, univariate and multivariate, transform the original estimate to a monotonic estimate, and the resulting estimate is closer to the true curve in common metrics than the original estimate. We illustrate the results with a computational example and an empirical example dealing with age-height growth charts.",2007,2017-11-23,https://arxiv.org/abs/0704.3686,2
0704.0229,Geometric Complexity Theory VI: the flip via saturated and positive integer programming in representation theory and algebraic geometry,"This article belongs to a series on geometric complexity theory (GCT), an approach to the P vs. NP and related problems through algebraic geometry and representation theory. The basic principle behind this approach is called the flip. In essence, it reduces the negative hypothesis in complexity theory (the lower bound problems), such as the P vs. NP problem in characteristic zero, to the positive hypothesis in complexity theory (the upper bound problems): specifically, to showing that the problems of deciding nonvanishing of the fundamental structural constants in representation theory and algebraic geometry, such as the well known plethysm constants--or rather certain relaxed forms of these decision probelms--belong to the complexity class P. In this article, we suggest a plan for implementing the flip, i.e., for showing that these relaxed decision problems belong to P. This is based on the reduction of the preceding complexity-theoretic positive hypotheses to mathematical positivity hypotheses: specifically, to showing that there exist positive formulae--i.e. formulae with nonnegative coefficients--for the structural constants under consideration and certain functions associated with them. These turn out be intimately related to the similar positivity properties of the Kazhdan-Lusztig polynomials and the multiplicative structural constants of the canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum groups. The known proofs of these positivity properties depend on the Riemann hypothesis over finite fields and the related results. Thus the reduction here, in conjunction with the flip, in essence, says that the validity of the P vs. NP conjecture in characteristic zero is intimately linked to the Riemann hypothesis over finite fields and related problems.",2007,2009-01-22,https://arxiv.org/abs/0704.0229,4
0704.3649,Quantile and Probability Curves Without Crossing,"This paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem. The method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. We show that the rearranged curve is closer to the true quantile curve in finite samples than the original curve, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. We also establish validity of the bootstrap for estimating the limit law of the the entire rearranged curve and its functionals. Our limit results are generic in that they apply to every estimator of a monotone econometric function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. Consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. We illustrate the results with an application to estimation of structural quantile functions using data on Vietnam veteran status and earnings.",2007,2017-10-04,https://arxiv.org/abs/0704.3649,3
0704.0005,From dyadic $\Lambda_{\alpha}$ to $\Lambda_{\alpha}$,"In this paper we show how to compute the $\Lambda_{\alpha}$ norm, $\alpha\ge 0$, using the dyadic grid. This result is a consequence of the description of the Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.",2007,2013-10-15,https://arxiv.org/abs/0704.0005,1
0705.2938,Codage arithmetique pour la description d'une distribution,"Using predictive adaptive arithmetic coding and the Minimum Description Length principle, we derive an efficient tool for model selection problems : the RIC information criterion. We then present an extension of these coding techniques to non-parametrical estimation of a distribution and illustrate it on the gray scales histogram of an image. Key-words : Information criteria, MDL, model selection, non-parametrical estimation, histograms.",2007,2007-05-23,https://arxiv.org/abs/0705.2938,1
0704.2191,Mismatch Repair Error Implies Chargaff's Second Parity Rule,"Chargaff's second parity rule holds empirically for most types of DNA that along single strands of DNA the base contents are equal for complimentary bases, A = T, G = C. A Markov chain model is constructed to track the evolution of any single base position along single strands of genomes whose organisms are equipped with replication mismatch repair. Under the key assumptions that mismatch error rates primarily depend the number of hydrogen bonds of nucleotides and that the mismatch repairing process itself makes strand recognition error, the model shows that the steady state probabilities for any base position to take on one of the 4 nucleotide bases are equal for complimentary bases. As a result, Chargaff's second parity rule is the manifestation of the Law of Large Number acting on the steady state probabilities. More importantly, because the model pinpoints mismatch repair as a basis of the rule, it is suitable for experimental verification.",2007,2007-09-20,https://arxiv.org/abs/0704.2191,2
0704.0004,A determinant of Stirling cycle numbers counts unlabeled acyclic single-source automata,We show that a determinant of Stirling cycle numbers counts unlabeled acyclic single-source automata. The proof involves a bijection from these automata to certain marked lattice paths and a sign-reversing involution to evaluate the determinant.,2007,2007-05-23,https://arxiv.org/abs/0704.0004,1
0705.0418,Various Approaches for Predicting Land Cover in Mountain Areas,"Using former maps, geographers intend to study the evolution of the land cover in order to have a prospective approach on the future landscape; predictions of the future land cover, by the use of older maps and environmental variables, are usually done through the GIS (Geographic Information System). We propose here to confront this classical geographical approach with statistical approaches: a linear parametric model (polychotomous regression modeling) and a nonparametric one (multilayer perceptron). These methodologies have been tested on two real areas on which the land cover is known at various dates; this allows us to emphasize the benefit of these two statistical approaches compared to GIS and to discuss the way GIS could be improved by the use of statistical models.",2007,2007-05-23,https://arxiv.org/abs/0705.0418,1
1709.06073,Adaptive Nonlinear RF Cancellation for Improved Isolation in Simultaneous Transmit-Receive Systems,"This paper proposes an active radio frequency (RF) cancellation solution to suppress the transmitter (TX) passband leakage signal in radio transceivers supporting simultaneous transmission and reception. The proposed technique is based on creating an opposite-phase baseband equivalent replica of the TX leakage signal in the transceiver digital front-end through adaptive nonlinear filtering of the known transmit data, to facilitate highly accurate cancellation under a nonlinear TX power amplifier (PA). The active RF cancellation is then accomplished by employing an auxiliary transmitter chain, to generate the actual RF cancellation signal, and combining it with the received signal at the receiver (RX) low noise amplifier (LNA) input. A closed-loop parameter learning approach, based on the decorrelation principle, is also developed to efficiently estimate the coefficients of the nonlinear cancellation filter in the presence of a nonlinear TX PA with memory, finite passive isolation, and a nonlinear RX LNA. The performance of the proposed cancellation technique is evaluated through comprehensive RF measurements adopting commercial LTE-Advanced transceiver hardware components. The results show that the proposed technique can provide an additional suppression of up to 54 dB for the TX passband leakage signal at the RX LNA input, even at considerably high transmit power levels and with wide transmission bandwidths. Such novel cancellation solution can therefore substantially improve the TX-RX isolation, hence reducing the requirements on passive isolation and RF component linearity, as well as increasing the efficiency and flexibility of the RF spectrum use in the emerging 5G radio networks.",2017,2018-05-23,https://arxiv.org/abs/1709.06073,2
0704.0016,Lifetime of doubly charmed baryons,"In this work, we evaluate the lifetimes of the doubly charmed baryons $\Xi_{cc}^{+}$, $\Xi_{cc}^{++}$ and $\Omega_{cc}^{+}$. We carefully calculate the non-spectator contributions at the quark level where the Cabibbo-suppressed diagrams are also included. The hadronic matrix elements are evaluated in the simple non-relativistic harmonic oscillator model. Our numerical results are generally consistent with that obtained by other authors who used the diquark model. However, all the theoretical predictions on the lifetimes are one order larger than the upper limit set by the recent SELEX measurement. This discrepancy would be clarified by the future experiment, if more accurate experiment still confirms the value of the SELEX collaboration, there must be some unknown mechanism to be explored.",2007,2008-12-18,https://arxiv.org/abs/0704.0016,1
0704.0027,Filling-Factor-Dependent Magnetophonon Resonance in Graphene,"We describe a peculiar fine structure acquired by the in-plane optical phonon at the Gamma-point in graphene when it is brought into resonance with one of the inter-Landau-level transitions in this material. The effect is most pronounced when this lattice mode (associated with the G-band in graphene Raman spectrum) is in resonance with inter-Landau-level transitions 0 -> (+,1) and (-,1) -> 0, at a magnetic field B_0 ~ 30 T. It can be used to measure the strength of the electron-phonon coupling directly, and its filling-factor dependence can be used experimentally to detect circularly polarized lattice modes.",2007,2009-09-24,https://arxiv.org/abs/0704.0027,4
0704.0331,Symmetries by base substitutions in the genetic code predict 2' or 3' aminoacylation of tRNAs,"This letter reports complete sets of two-fold symmetries between partitions of the universal genetic code. By substituting bases at each position of the codons according to a fixed rule, it happens that properties of the degeneracy pattern or of tRNA aminoacylation specificity are exchanged.",2007,2007-05-23,https://arxiv.org/abs/0704.0331,1
0704.0108,Reducing SAT to 2-SAT,Description of a polynomial time reduction of SAT to 2-SAT of polynomial size.,2007,2007-05-23,https://arxiv.org/abs/0704.0108,1
0706.0787,Construction of Bayesian Deformable Models via Stochastic Approximation Algorithm: A Convergence Study,"The problem of the definition and the estimation of generative models based on deformable templates from raw data is of particular importance for modelling non aligned data affected by various types of geometrical variability. This is especially true in shape modelling in the computer vision community or in probabilistic atlas building for Computational Anatomy (CA). A first coherent statistical framework modelling the geometrical variability as hidden variables has been given by Allassonni\`ere, Amit and Trouv\'e (JRSS 2006). Setting the problem in a Bayesian context they proved the consistency of the MAP estimator and provided a simple iterative deterministic algorithm with an EM flavour leading to some reasonable approximations of the MAP estimator under low noise conditions. In this paper we present a stochastic algorithm for approximating the MAP estimator in the spirit of the SAEM algorithm. We prove its convergence to a critical point of the observed likelihood with an illustration on images of handwritten digits.",2007,2009-01-16,https://arxiv.org/abs/0706.0787,2
0705.3257,Evaluating Throwing Ability in Baseball,We present a quantitative analysis of throwing ability for major league outfielders and catchers. We use detailed game event data to tabulate success and failure events in outfielder and catcher throwing opportunities. We attribute a run contribution to each success or failure which are tabulated for each player in each season. We use four seasons of data to estimate the overall throwing ability of each player using a Bayesian hierarchical model. This model allows us to shrink individual player estimates towards an overall population mean depending on the number of opportunities for each player. We use the posterior distribution of player abilities from this model to identify players with significant positive and negative throwing contributions.,2007,2007-06-13,https://arxiv.org/abs/0705.3257,2
0704.0030,Tuning correlation effects with electron-phonon interactions,"We investigate the effect of tuning the phonon energy on the correlation effects in models of electron-phonon interactions using DMFT. In the regime where itinerant electrons, instantaneous electron-phonon driven correlations and static distortions compete on similar energy scales, we find several interesting results including (1) A crossover from band to Mott behavior in the spectral function, leading to hybrid band/Mott features in the spectral function for phonon frequencies slightly larger than the band width. (2) Since the optical conductivity depends sensitively on the form of the spectral function, we show that such a regime should be observable through the low frequency form of the optical conductivity. (3) The resistivity has a double kondo peak arrangement",2007,2015-05-13,https://arxiv.org/abs/0704.0030,1
0912.5013,"Inference for Extremal Conditional Quantile Models, with an Application to Market and Birthweight Risks","Quantile regression is an increasingly important empirical tool in economics and other sciences for analyzing the impact of a set of regressors on the conditional distribution of an outcome. Extremal quantile regression, or quantile regression applied to the tails, is of interest in many economic and financial applications, such as conditional value-at-risk, production efficiency, and adjustment bands in (S,s) models. In this paper we provide feasible inference tools for extremal conditional quantile models that rely upon extreme value approximations to the distribution of self-normalized quantile regression statistics. The methods are simple to implement and can be of independent interest even in the non-regression case. We illustrate the results with two empirical examples analyzing extreme fluctuations of a stock return and extremely low percentiles of live infants' birthweights in the range between 250 and 1500 grams.",2009,2018-01-08,https://arxiv.org/abs/0912.5013,1
0704.0041,Quantum Group of Isometries in Classical and Noncommutative Geometry,"We formulate a quantum generalization of the notion of the group of Riemannian isometries for a compact Riemannian manifold, by introducing a natural notion of smooth and isometric action by a compact quantum group on a classical or noncommutative manifold described by spectral triples, and then proving the existence of a universal object (called the quantum isometry group) in the category of compact quantum groups acting smoothly and isometrically on a given (possibly noncommutative) manifold satisfying certain regularity assumptions. In fact, we identify the quantum isometry group with the universal object in a bigger category, namely the category of `quantum families of smooth isometries', defined along the line of Woronowicz and Soltan. We also construct a spectral triple on the Hilbert space of forms on a noncommutative manifold which is equivariant with respect to a natural unitary representation of the quantum isometry group. We give explicit description of quantum isometry groups of commutative and noncommutative tori, and in this context, obtain the quantum double torus defined in \cite{hajac} as the universal quantum group of holomorphic isometries of the noncommutative torus.",2007,2009-11-13,https://arxiv.org/abs/0704.0041,4
0805.2713,Coherence-based multivariate analysis of high frequency stock market values,"The paper tackles the problem of deriving a topological structure among stock prices from high frequency historical values. Similar studies using low frequency data have already provided valuable insights. However, in those cases data need to be collected for a longer period and then they have to be detrended. An effective technique based on averaging a metric function on short subperiods of the observation horizon is suggested. Since a standard correlation-based metric is not capable of catching dependencies at different time instants, it is not expected to perform the best when dealing with high frequency data. Hence, the choice of a more suitable metric is discussed. In particular, a coherence-based metric is proposed, for it is able to detect any possible linear relation between two times series, even at different time instants. The averaging technique is employed to analyze a set of 100 high volume stocks of the New York Stock Exchange, observed during March 2008.",2008,2008-12-02,https://arxiv.org/abs/0805.2713,1
0801.1599,Parametric and nonparametric models and methods in financial econometrics,"Financial econometrics has become an increasingly popular research field. In this paper we review a few parametric and nonparametric models and methods used in this area. After introducing several widely used continuous-time and discrete-time models, we study in detail dependence structures of discrete samples, including Markovian property, hidden Markovian structure, contaminated observations, and random samples. We then discuss several popular parametric and nonparametric estimation methods. To avoid model mis-specification, model validation plays a key role in financial modeling. We discuss several model validation techniques, including pseudo-likelihood ratio test, nonparametric curve regression based test, residuals based test, generalized likelihood ratio test, simultaneous confidence band construction, and density based test. Finally, we briefly touch on tools for studying large sample properties.",2008,2008-12-02,https://arxiv.org/abs/0801.1599,2
1707.09255,SPARCOM: Sparsity Based Super-Resolution Correlation Microscopy,"In traditional optical imaging systems, the spatial resolution is limited by the physics of diffraction, which acts as a low-pass filter. The information on sub-wavelength features is carried by evanescent waves, never reaching the camera, thereby posing a hard limit on resolution: the so-called diffraction limit. Modern microscopic methods enable super-resolution, by employing florescence techniques. State-of-the-art localization based fluorescence subwavelength imaging techniques such as PALM and STORM achieve sub-diffraction spatial resolution of several tens of nano-meters. However, they require tens of thousands of exposures, which limits their temporal resolution. We have recently proposed SPARCOM (sparsity based super-resolution correlation microscopy), which exploits the sparse nature of the fluorophores distribution, alongside a statistical prior of uncorrelated emissions, and showed that SPARCOM achieves spatial resolution comparable to PALM/STORM, while capturing the data hundreds of times faster. Here, we provide a detailed mathematical formulation of SPARCOM, which in turn leads to an efficient numerical implementation, suitable for large-scale problems. We further extend our method to a general framework for sparsity based super-resolution imaging, in which sparsity can be assumed in other domains such as wavelet or discrete-cosine, leading to improved reconstructions in a variety of physical settings.",2017,2018-12-13,https://arxiv.org/abs/1707.09255,2
0704.0009,"The Spitzer c2d Survey of Large, Nearby, Insterstellar Clouds. IX. The Serpens YSO Population As Observed With IRAC and MIPS","We discuss the results from the combined IRAC and MIPS c2d Spitzer Legacy observations of the Serpens star-forming region. In particular we present a set of criteria for isolating bona fide young stellar objects, YSO's, from the extensive background contamination by extra-galactic objects. We then discuss the properties of the resulting high confidence set of YSO's. We find 235 such objects in the 0.85 deg^2 field that was covered with both IRAC and MIPS. An additional set of 51 lower confidence YSO's outside this area is identified from the MIPS data combined with 2MASS photometry. We describe two sets of results, color-color diagrams to compare our observed source properties with those of theoretical models for star/disk/envelope systems and our own modeling of the subset of our objects that appear to be star+disks. These objects exhibit a very wide range of disk properties, from many that can be fit with actively accreting disks to some with both passive disks and even possibly debris disks. We find that the luminosity function of YSO's in Serpens extends down to at least a few x .001 Lsun or lower for an assumed distance of 260 pc. The lower limit may be set by our inability to distinguish YSO's from extra-galactic sources more than by the lack of YSO's at very low luminosities. A spatial clustering analysis shows that the nominally less-evolved YSO's are more highly clustered than the later stages and that the background extra-galactic population can be fit by the same two-point correlation function as seen in other extra-galactic studies. We also present a table of matches between several previous infrared and X-ray studies of the Serpens YSO population and our Spitzer data set.",2007,2010-03-18,https://arxiv.org/abs/0704.0009,1
0704.0047,Intelligent location of simultaneously active acoustic emission sources: Part I,"The intelligent acoustic emission locator is described in Part I, while Part II discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources. The location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. This article describes an intelligent acoustic emission source locator. The intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. Locator performance was tested on different test specimens. Tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. The location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. This is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.",2007,2009-09-29,https://arxiv.org/abs/0704.0047,1
0802.0220,Forecasting with time-varying vector autoregressive models,"The purpose of this paper is to propose a time-varying vector autoregressive model (TV-VAR) for forecasting multivariate time series. The model is casted into a state-space form that allows flexible description and analysis. The volatility covariance matrix of the time series is modelled via inverted Wishart and singular multivariate beta distributions allowing a fully conjugate Bayesian inference. Model performance and model comparison is done via the likelihood function, sequential Bayes factors, the mean of squared standardized forecast errors, the mean of absolute forecast errors (known also as mean absolute deviation), and the mean forecast error. Bayes factors are also used in order to choose the autoregressive order of the model. Multi-step forecasting is discussed in detail and a flexible formula is proposed to approximate the forecast function. Two examples, consisting of bivariate data of IBM shares and of foreign exchange (FX) rates for 8 currencies, illustrate the methods. For the IBM data we discuss model performance and multi-step forecasting in some detail. For the FX data we discuss sequential portfolio allocation; for both data sets our empirical findings suggest that the TV-VAR models outperform the widely used VAR models.",2008,2008-12-02,https://arxiv.org/abs/0802.0220,2
0704.0011,Computing genus 2 Hilbert-Siegel modular forms over $\Q(\sqrt{5})$ via the Jacquet-Langlands correspondence,"In this paper we present an algorithm for computing Hecke eigensystems of Hilbert-Siegel cusp forms over real quadratic fields of narrow class number one. We give some illustrative examples using the quadratic field $\Q(\sqrt{5})$. In those examples, we identify Hilbert-Siegel eigenforms that are possible lifts from Hilbert eigenforms.",2007,2008-08-20,https://arxiv.org/abs/0704.0011,3
0704.0062,On-line Viterbi Algorithm and Its Relationship to Random Walks,"In this paper, we introduce the on-line Viterbi algorithm for decoding hidden Markov models (HMMs) in much smaller than linear space. Our analysis on two-state HMMs suggests that the expected maximum memory used to decode sequence of length $n$ with $m$-state HMM can be as low as $\Theta(m\log n)$, without a significant slow-down compared to the classical Viterbi algorithm. Classical Viterbi algorithm requires $O(mn)$ space, which is impractical for analysis of long DNA sequences (such as complete human genome chromosomes) and for continuous data streams. We also experimentally demonstrate the performance of the on-line Viterbi algorithm on a simple HMM for gene finding on both simulated and real DNA sequences.",2007,2010-01-25,https://arxiv.org/abs/0704.0062,1
0704.0001,Calculation of prompt diphoton production cross sections at Tevatron and LHC energies,"A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron, and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC, showing that enhanced sensitivity to the signal can be obtained with judicious selection of events.",2007,2008-11-26,https://arxiv.org/abs/0704.0001,2
1410.1272,Cramer-Rao Lower Bounds of Joint Delay-Doppler Estimation for an Extended Target,"The problem on the Cramer-Rao Lower Bounds (CRLBs) for the joint time delay and Doppler stretch estimation of an extended target is considered in this paper. The integral representations of the CRLBs for both the time delay and the Doppler stretch are derived. To facilitate computation and analysis, series representations and approximations of the CRLBs are introduced. According to these series representations, the impact of several waveform parameters on the estimation accuracy is investigated, which reveals that the CRLB of the Doppler stretch is inversely proportional to the effective time-bandwidth product of the waveform. This conclusion generalizes a previous result in the narrowband case. The popular wideband ambiguity function (WBAF) based delay-Doppler estimator is evaluated and compared with the CRLBs through numerical experiments. Our results indicate that the WBAF estimator, originally derived from a single scatterer model, is not suitable for the parameter estimation of an extended target.",2014,2018-08-30,https://arxiv.org/abs/1410.1272,2
0904.3132,Posterior Inference in Curved Exponential Families under Increasing Dimensions,"This work studies the large sample properties of the posterior-based inference in the curved exponential family under increasing dimension. The curved structure arises from the imposition of various restrictions on the model, such as moment restrictions, and plays a fundamental role in econometrics and others branches of data analysis. We establish conditions under which the posterior distribution is approximately normal, which in turn implies various good properties of estimation and inference procedures based on the posterior. In the process we also revisit and improve upon previous results for the exponential family under increasing dimension by making use of concentration of measure. We also discuss a variety of applications to high-dimensional versions of the classical econometric models including the multinomial model with moment restrictions, seemingly unrelated regression equations, and single structural equation models. In our analysis, both the parameter dimension and the number of moments are increasing with the sample size.",2009,2017-10-05,https://arxiv.org/abs/0904.3132,4
0709.3884,Flexible least squares for temporal data mining and statistical arbitrage,"A number of recent emerging applications call for studying data streams, potentially infinite flows of information updated in real-time. When multiple co-evolving data streams are observed, an important task is to determine how these streams depend on each other, accounting for dynamic dependence patterns without imposing any restrictive probabilistic law governing this dependence. In this paper we argue that flexible least squares (FLS), a penalized version of ordinary least squares that accommodates for time-varying regression coefficients, can be deployed successfully in this context. Our motivating application is statistical arbitrage, an investment strategy that exploits patterns detected in financial data streams. We demonstrate that FLS is algebraically equivalent to the well-known Kalman filter equations, and take advantage of this equivalence to gain a better understanding of FLS and suggest a more efficient algorithm. Promising experimental results obtained from a FLS-based algorithmic trading system for the S&P 500 Futures Index are reported.",2007,2009-02-08,https://arxiv.org/abs/0709.3884,1
1707.08701,Enhancing security of optical cryptosystem with position-multiplexing and ultra-broadband illumination,"A position-multiplexing based cryptosystem is proposed to enhance the information security with an ultra-broadband illumination. The simplified optical encryption system only contains one diffuser acting as the random phase mask (RPM). Light coming from a plaintext passes through this RPM and generates the corresponding ciphertext on a camera. The proposed system effectively reduces problems of misalignment and coherent noise that are found in the coherent illumination. Here, the use of ultra-broadband illumination has the advantage of making a strong scattering and complex ciphertext by reducing the speckle contrast. Reduction of the ciphertext size further increases the strength of the ciphering. The unique spatial keys are utilized for the individual decryption as the plaintext locates at different spatial positions, and a complete decrypted image could be concatenated with high fidelity. Benefiting from the ultra-broadband illumination and position-multiplexing, the information of interest is scrambled together in a small ciphertext. Only the authorized user can decrypt this information with the correct keys. Therefore, a high performance security for a cryptosystem could be achieved.",2017,2021-09-17,https://arxiv.org/abs/1707.08701,1
0704.0040,Multilinear function series in conditionally free probability with amalgamation,"As in the cases of freeness and monotonic independence, the notion of conditional freeness is meaningful when complex-valued states are replaced by positive conditional expectations. In this framework, the paper presents several positivity results, a version of the central limit theorem and an analogue of the conditionally free R-transform constructed by means of multilinear function series.",2007,2008-09-05,https://arxiv.org/abs/0704.0040,3
0704.0006,Bosonic characters of atomic Cooper pairs across resonance,"We study the two-particle wave function of paired atoms in a Fermi gas with tunable interaction strengths controlled by Feshbach resonance. The Cooper pair wave function is examined for its bosonic characters, which is quantified by the correction of Bose enhancement factor associated with the creation and annihilation composite particle operators. An example is given for a three-dimensional uniform gas. Two definitions of Cooper pair wave function are examined. One of which is chosen to reflect the off-diagonal long range order (ODLRO). Another one corresponds to a pair projection of a BCS state. On the side with negative scattering length, we found that paired atoms described by ODLRO are more bosonic than the pair projected definition. It is also found that at $(k_F a)^{-1} \ge 1$, both definitions give similar results, where more than 90% of the atoms occupy the corresponding molecular condensates.",2007,2015-05-13,https://arxiv.org/abs/0704.0006,1
0704.0020,Measurement of the Hadronic Form Factor in D0 --> K- e+ nue Decays,"The shape of the hadronic form factor f+(q2) in the decay D0 --> K- e+ nue has been measured in a model independent analysis and compared with theoretical calculations. We use 75 fb(-1) of data recorded by the BABAR detector at the PEPII electron-positron collider. The corresponding decay branching fraction, relative to the decay D0 --> K- pi+, has also been measured to be RD = BR(D0 --> K- e+ nue)/BR(D0 --> K- pi+) = 0.927 +/- 0.007 +/- 0.012. From these results, and using the present world average value for BR(D0 --> K- pi+), the normalization of the form factor at q2=0 is determined to be f+(0)=0.727 +/- 0.007 +/- 0.005 +/- 0.007 where the uncertainties are statistical, systematic, and from external inputs, respectively.",2007,2015-06-30,https://arxiv.org/abs/0704.0020,1
0704.0361,Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo Codes,"It has been observed that particular rate-1/2 partially systematic parallel concatenated convolutional codes (PCCCs) can achieve a lower error floor than that of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can only be identified by means of an exhaustive search, whilst convergence towards low bit error probabilities can be problematic when the systematic output of a rate-1/2 partially systematic PCCC is heavily punctured. In this paper, we present and study a family of rate-1/2 partially systematic PCCCs, which we call pseudo-randomly punctured codes. We evaluate their bit error rate performance and we show that they always yield a lower error floor than that of their rate-1/3 parent codes. Furthermore, we compare analytic results to simulations and we demonstrate that their performance converges towards the error floor region, owning to the moderate puncturing of their systematic output. Consequently, we propose pseudo-random puncturing as a means of improving the bandwidth efficiency of a PCCC and simultaneously lowering its error floor.",2007,2016-11-18,https://arxiv.org/abs/0704.0361,1
0712.2088,Financial Variables Effect on the U.S. Gross Private Domestic Investment (GPDI) 1959-2001,"I studied what role the US stock markets and money markets have possibly played in the Gross Private Domestic Investment (GPDI) of the United States from the year 1959 to the year 2001, Gross Private Domestic Investment refers to the total amount of investment spending by businesses and firms located within the borders of a nation. It includes both the values of the purchases of non-residential fixed investment, which include capital goods used for production, and the values of the purchases of residential fixed investment, which include construction spending for factories or offices. And I created a Multiple Linear Regression Model of the GDPI. To see if companies and private citizens use the stock market and money markets as a way of financing capital projects (business ventures, buying commercial and noncommercial property, etc). Keywords: Gross Private Domestic Investment, Pearson Correlation, SP 500, TB3",2007,2008-12-02,https://arxiv.org/abs/0712.2088,1
1104.4580,Quantile Regression with Censoring and Endogeneity,"In this paper, we develop a new censored quantile instrumental variable (CQIV) estimator and describe its properties and computation. The CQIV estimator combines Powell (1986) censored quantile regression (CQR) to deal with censoring, with a control variable approach to incorporate endogenous regressors. The CQIV estimator is obtained in two stages that are non-additive in the unobservables. The first stage estimates a non-additive model with infinite dimensional parameters for the control variable, such as a quantile or distribution regression model. The second stage estimates a non-additive censored quantile regression model for the response variable of interest, including the estimated control variable to deal with endogeneity. For computation, we extend the algorithm for CQR developed by Chernozhukov and Hong (2002) to incorporate the estimation of the control variable. We give generic regularity conditions for asymptotic normality of the CQIV estimator and for the validity of resampling methods to approximate its asymptotic distribution. We verify these conditions for quantile and distribution regression estimation of the control variable. Our analysis covers two-stage (uncensored) quantile regression with non-additive first stage as an important special case. We illustrate the computation and applicability of the CQIV estimator with a Monte-Carlo numerical example and an empirical application on estimation of Engel curves for alcohol.",2011,2018-01-16,https://arxiv.org/abs/1104.4580,3
1012.1297,LASSO Methods for Gaussian Instrumental Variables Models,"In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO, sqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate optimal instruments in linear instrumental variables (IV) models with many instruments in the canonical Gaussian case. The methods apply even when the number of instruments is much larger than the sample size. We derive asymptotic distributions for the resulting IV estimators and provide conditions under which these sparsity-based IV estimators are asymptotically oracle-efficient. In simulation experiments, a sparsity-based IV estimator with a data-driven penalty performs well compared to recently advocated many-instrument-robust procedures. We illustrate the procedure in an empirical example using the Angrist and Krueger (1991) schooling data.",2010,2017-10-05,https://arxiv.org/abs/1012.1297,2
0904.1990,Average and Quantile Effects in Nonseparable Panel Models,"Nonseparable panel models are important in a variety of economic settings, including discrete choice. This paper gives identification and estimation results for nonseparable models under time homogeneity conditions that are like ""time is randomly assigned"" or ""time is an instrument."" Partial identification results for average and quantile effects are given for discrete regressors, under static or dynamic conditions, in fully nonparametric and in semiparametric models, with time effects. It is shown that the usual, linear, fixed-effects estimator is not a consistent estimator of the identified average effect, and a consistent estimator is given. A simple estimator of identified quantile treatment effects is given, providing a solution to the important problem of estimating quantile treatment effects from panel data. Bounds for overall effects in static and dynamic models are given. The dynamic bounds provide a partial identification solution to the important problem of estimating the effect of state dependence in the presence of unobserved heterogeneity. The impact of $T$, the number of time periods, is shown by deriving shrinkage rates for the identified set as $T$ grows. We also consider semiparametric, discrete-choice models and find that semiparametric panel bounds can be much tighter than nonparametric bounds. Computationally-convenient methods for semiparametric models are presented. We propose a novel inference method that applies in panel data and other settings and show that it produces uniformly valid confidence regions in large samples. We give empirical illustrations.",2009,2018-01-08,https://arxiv.org/abs/0904.1990,4
0704.0282,On Punctured Pragmatic Space-Time Codes in Block Fading Channel,This paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. We show that good performance can be achieved even when puncturation is adopted and that we can still employ the same Viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.,2007,2007-07-13,https://arxiv.org/abs/0704.0282,1
0704.0468,Inapproximability of Maximum Weighted Edge Biclique and Its Applications,"Given a bipartite graph $G = (V_1,V_2,E)$ where edges take on {\it both} positive and negative weights from set $\mathcal{S}$, the {\it maximum weighted edge biclique} problem, or $\mathcal{S}$-MWEB for short, asks to find a bipartite subgraph whose sum of edge weights is maximized. This problem has various applications in bioinformatics, machine learning and databases and its (in)approximability remains open. In this paper, we show that for a wide range of choices of $\mathcal{S}$, specifically when $| \frac{\min\mathcal{S}} {\max \mathcal{S}} | \in \Omega(\eta^{\delta-1/2}) \cap O(\eta^{1/2-\delta})$ (where $\eta = \max\{|V_1|, |V_2|\}$, and $\delta \in (0,1/2]$), no polynomial time algorithm can approximate $\mathcal{S}$-MWEB within a factor of $n^{\epsilon}$ for some $\epsilon > 0$ unless $\mathsf{RP = NP}$. This hardness result gives justification of the heuristic approaches adopted for various applied problems in the aforementioned areas, and indicates that good approximation algorithms are unlikely to exist. Specifically, we give two applications by showing that: 1) finding statistically significant biclusters in the SAMBA model, proposed in \cite{Tan02} for the analysis of microarray data, is $n^{\epsilon}$-inapproximable; and 2) no polynomial time algorithm exists for the Minimum Description Length with Holes problem \cite{Bu05} unless $\mathsf{RP=NP}$.",2007,2009-03-23,https://arxiv.org/abs/0704.0468,2
0705.2214,Bagging multiple comparisons from microarray data,"The problem of large-scale simultaneous hypothesis testing is re-visited. Bagging and subagging procedures are put forth with the purpose of improving the discovery power of the tests. The procedures are implemented in both simulated and real data. It is shown that bagging and subagging significantly improve power at the cost of a small increase in false discovery rate with the proposed `maximum contrast' subagging having an edge over bagging, i.e., yielding similar power but significantly smaller false discovery rates.",2007,2007-05-23,https://arxiv.org/abs/0705.2214,1
0704.1362,Fast recursive filters for simulating nonlinear dynamic systems,"A fast and accurate computational scheme for simulating nonlinear dynamic systems is presented. The scheme assumes that the system can be represented by a combination of components of only two different types: first-order low-pass filters and static nonlinearities. The parameters of these filters and nonlinearities may depend on system variables, and the topology of the system may be complex, including feedback. Several examples taken from neuroscience are given: phototransduction, photopigment bleaching, and spike generation according to the Hodgkin-Huxley equations. The scheme uses two slightly different forms of autoregressive filters, with an implicit delay of zero for feedforward control and an implicit delay of half a sample distance for feedback control. On a fairly complex model of the macaque retinal horizontal cell it computes, for a given level of accuracy, 1-2 orders of magnitude faster than 4th-order Runge-Kutta. The computational scheme has minimal memory requirements, and is also suited for computation on a stream processor, such as a GPU (Graphical Processing Unit).",2007,2008-06-20,https://arxiv.org/abs/0704.1362,2
0704.3474,Missing Data: A Comparison of Neural Network and Expectation Maximisation Techniques,"The estimation of missing input vector elements in real time processing applications requires a system that possesses the knowledge of certain characteristics such as correlations between variables, which are inherent in the input space. Computational intelligence techniques and maximum likelihood techniques do possess such characteristics and as a result are important for imputation of missing data. This paper compares two approaches to the problem of missing data estimation. The first technique is based on the current state of the art approach to this problem, that being the use of Maximum Likelihood (ML) and Expectation Maximisation (EM. The second approach is the use of a system based on auto-associative neural networks and the Genetic Algorithm as discussed by Adbella and Marwala3. The estimation ability of both of these techniques is compared, based on three datasets and conclusions are made.",2007,2007-05-23,https://arxiv.org/abs/0704.3474,1
1601.07624,Analysis of Random Pulse Repetition Interval Radar,"Random pulse repetition interval (PRI) waveform arouses great interests in the field of modern radars due to its ability to alleviate range and Doppler ambiguities as well as enhance electronic counter-countermeasures (ECCM) capabilities. Theoretical results pertaining to the statistical characteristics of ambiguity function (AF) are derived in this work, indicating that the range and Doppler ambiguities can be effectively suppressed by increasing the number of pulses and the range of PRI jitters. This provides an important guidance in terms of waveform design. As is well known, the significantly lifted sidelobe pedestal induced by PRI randomization will degrade the performance of weak target detection. Proceeding from that, we propose to employ orthogonal matching pursuit (OMP) to overcome this issue. Simulation results demonstrate that the OMP method can effectively lower the sidelobe pedestal of strong target and improve the performance of weak target estimation.",2016,2018-10-15,https://arxiv.org/abs/1601.07624,1
0704.0049,An algorithm for the classification of smooth Fano polytopes,"We present an algorithm that produces the classification list of smooth Fano d-polytopes for any given d. The input of the algorithm is a single number, namely the positive integer d. The algorithm has been used to classify smooth Fano d-polytopes for d<=7. There are 7622 isomorphism classes of smooth Fano 6-polytopes and 72256 isomorphism classes of smooth Fano 7-polytopes.",2007,2007-05-23,https://arxiv.org/abs/0704.0049,1
0704.0031,Crystal channeling of LHC forward protons with preserved distribution in phase space,"We show that crystal can trap a broad (x, x', y, y', E) distribution of particles and channel it preserved with a high precision. This sampled-and-hold distribution can be steered by a bent crystal for analysis downstream. In simulations for the 7 TeV Large Hadron Collider, a crystal adapted to the accelerator lattice traps 90% of diffractively scattered protons emerging from the interaction point with a divergence 100 times the critical angle. We set the criterion for crystal adaptation improving efficiency ~100-fold. Proton angles are preserved in crystal transmission with accuracy down to 0.1 microrad. This makes feasible a crystal application for measuring very forward protons at the LHC.",2007,2008-11-26,https://arxiv.org/abs/0704.0031,1
0710.4010,A stochastic theory for temporal fluctuations in self-organized critical systems,"A stochastic theory for the toppling activity in sandpile models is developed, based on a simple mean-field assumption about the toppling process. The theory describes the process as an anti-persistent Gaussian walk, where the diffusion coefficient is proportional to the activity. It is formulated as a generalization of the It\^{o} stochastic differential equation with an anti-persistent fractional Gaussian noise source. An essential element of the theory is re-scaling to obtain a proper thermodynamic limit, and it captures all temporal features of the toppling process obtained by numerical simulation of the Bak-Tang-Wiesenfeld sandpile in this limit.",2007,2009-11-13,https://arxiv.org/abs/0710.4010,2
0704.0003,The evolution of the Earth-Moon system based on the dark matter field fluid model,"The evolution of Earth-Moon system is described by the dark matter field fluid model proposed in the Meeting of Division of Particle and Field 2004, American Physical Society. The current behavior of the Earth-Moon system agrees with this model very well and the general pattern of the evolution of the Moon-Earth system described by this model agrees with geological and fossil evidence. The closest distance of the Moon to Earth was about 259000 km at 4.5 billion years ago, which is far beyond the Roche's limit. The result suggests that the tidal friction may not be the primary cause for the evolution of the Earth-Moon system. The average dark matter field fluid constant derived from Earth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts that the Mars's rotation is also slowing with the angular acceleration rate about -4.38 x 10^(-22) rad s^(-2).",2007,2008-01-13,https://arxiv.org/abs/0704.0003,3
0710.0745,"Mixing Kohonen Algorithm, Markov Switching Model and Detection of Multiple Change-Points: An Application to Monetary History","The present paper aims at locating the breakings of the integration process of an international system observed during about 50 years in the 19th century. A historical study could link them to special events, which operated as exogenous shocks on this process. The indicator of integration used is the spread between the highest and the lowest among the London, Hamburg and Paris gold-silver prices. Three algorithms are combined to study this integration: a periodization obtained with the SOM algorithm is confronted to the estimation of a two-regime Markov switching model, in order to give an interpretation of the changes of regime; in the same time change-points are identified over the whole period providing a more precise interpretation of the various types of regulation.",2007,2008-12-02,https://arxiv.org/abs/0710.0745,1
0705.0700,Inflated Beta Distributions,"This paper considers the issue of modeling fractional data observed in the interval [0,1), (0,1] or [0,1]. Mixed continuous-discrete distributions are proposed. The beta distribution is used to describe the continuous component of the model since its density can have quite diferent shapes depending on the values of the two parameters that index the distribution. Properties of the proposed distributions are examined. Also, maximum likelihood and method of moments estimation is discussed. Finally, practical applications that employ real data are presented.",2007,2008-03-19,https://arxiv.org/abs/0705.0700,3
0704.1390,Velocity oscillations in actin-based motility,"We present a simple and generic theoretical description of actin-based motility, where polymerization of filaments maintains propulsion. The dynamics is driven by polymerization kinetics at the filaments' free ends, crosslinking of the actin network, attachment and detachment of filaments to the obstacle interfaces and entropic forces. We show that spontaneous oscillations in the velocity emerge in a broad range of parameter values, and compare our findings with experiments.",2007,2015-05-13,https://arxiv.org/abs/0704.1390,1
0704.0634,A Finite Element framework for computation of protein normal modes and mechanical response,"A coarse-grained computational procedure based on the Finite Element Method is proposed to calculate the normal modes and mechanical response of proteins and their supramolecular assemblies. Motivated by the elastic network model, proteins are modeled as homogeneous isotropic elastic solids with volume defined by their solvent-excluded surface. The discretized Finite Element representation is obtained using a surface simplification algorithm that facilitates the generation of models of arbitrary prescribed spatial resolution. The procedure is applied to compute the normal modes of a mutant of T4 phage lysozyme and of filamentous actin, as well as the critical Euler buckling load of the latter when subject to axial compression. Results compare favorably with all-atom normal mode analysis, the Rotation Translation Blocks procedure, and experiment. The proposed methodology establishes a computational framework for the calculation of protein mechanical response that facilitates the incorporation of specific atomic-level interactions into the model, including aqueous-electrolyte-mediated electrostatic effects. The procedure is equally applicable to proteins with known atomic coordinates as it is to electron density maps of proteins, protein complexes, and supramolecular assemblies of unknown atomic structure.",2007,2007-05-23,https://arxiv.org/abs/0704.0634,1
1709.06074,Optimal Linear Precoding for Indoor Visible Light Communication System,"Visible light communication (VLC) is an emerging technique that uses light-emitting diodes (LED) to combine communication and illumination. It is considered as a promising scheme for indoor wireless communication that can be deployed at reduced costs while offering high data rate performance. In this paper, we focus on the design of the downlink of a multi-user VLC system. Inherent to multi-user systems is the interference caused by the broadcast nature of the medium. Linear precoding based schemes are among the most popular solutions that have recently been proposed to mitigate inter-user interference. This paper focuses on the design of the optimal linear precoding scheme that solves the max-min signal-to-interference-plus-noise ratio (SINR) problem. The performance of the proposed precoding scheme is studied under different working conditions and compared with the classical zero-forcing precoding. Simulations have been provided to illustrate the high gain of the proposed scheme.",2017,2017-09-19,https://arxiv.org/abs/1709.06074,1
0910.2465,Complete Characterization of Functions Satisfying the Conditions of Arrow's Theorem,"Arrow's theorem implies that a social choice function satisfying Transitivity, the Pareto Principle (Unanimity) and Independence of Irrelevant Alternatives (IIA) must be dictatorial. When non-strict preferences are allowed, a dictatorial social choice function is defined as a function for which there exists a single voter whose strict preferences are followed. This definition allows for many different dictatorial functions. In particular, we construct examples of dictatorial functions which do not satisfy Transitivity and IIA. Thus Arrow's theorem, in the case of non-strict preferences, does not provide a complete characterization of all social choice functions satisfying Transitivity, the Pareto Principle, and IIA. The main results of this article provide such a characterization for Arrow's theorem, as well as for follow up results by Wilson. In particular, we strengthen Arrow's and Wilson's result by giving an exact if and only if condition for a function to satisfy Transitivity and IIA (and the Pareto Principle). Additionally, we derive formulas for the number of functions satisfying these conditions.",2009,2018-07-27,https://arxiv.org/abs/0910.2465,2
1707.09577,Single shot large field of view imaging with scattering media by spatial demultiplexing,"Optically focusing and imaging through strongly scattering media are challenging tasks but have widespread applications from scientific research to biomedical applications and daily life. Benefiting from the memory effect (ME) for speckle intensity correlations, only one single-shot speckle pattern can be used for the high quality recovery of the objects and avoiding some complicated procedures to reduce scattering effects. In spite of all the spatial information from a large object being embedded in a single speckle image, ME gives a strict limitation to the field of view (FOV) for imaging through scattering media. Objects beyond the ME region cannot be recovered and only produce unwanted speckle patterns, causing reduction in the speckle contrast and recovery quality. Here, we extract the spatial information by utilizing these unavoidable speckle patterns, and enlarge the FOV of the optical imaging system. Regional point spreading functions (PSFs), which are fixed and only need to be recorded once for all time use, are employed to recover corresponding spatial regions of an object by deconvolution algorithm. Then an automatic weighted averaging in an iterative process is performed to obtain the object with significantly enlarged FOV. Our results present an important step toward an advanced imaging technique with strongly scattering media.",2017,2021-09-17,https://arxiv.org/abs/1707.09577,2
1201.0220,Inference for High-Dimensional Sparse Econometric Models,"This article is about estimation and inference methods for high dimensional sparse (HDS) regression models in econometrics. High dimensional sparse models arise in situations where many regressors (or series terms) are available and the regression function is well-approximated by a parsimonious, yet unknown set of regressors. The latter condition makes it possible to estimate the entire regression function effectively by searching for approximately the right set of regressors. We discuss methods for identifying this set of regressors and estimating their coefficients based on $\ell_1$-penalization and describe key theoretical results. In order to capture realistic practical situations, we expressly allow for imperfect selection of regressors and study the impact of this imperfect selection on estimation and inference results. We focus the main part of the article on the use of HDS models and methods in the instrumental variables model and the partially linear model. We present a set of novel inference results for these models and illustrate their use with applications to returns to schooling and growth regression.",2011,2017-10-05,https://arxiv.org/abs/1201.0220,1
0704.0025,Spectroscopic Properties of Polarons in Strongly Correlated Systems by Exact Diagrammatic Monte Carlo Method,"We present recent advances in understanding of the ground and excited states of the electron-phonon coupled systems obtained by novel methods of Diagrammatic Monte Carlo and Stochastic Optimization, which enable the approximation-free calculation of Matsubara Green function in imaginary times and perform unbiased analytic continuation to real frequencies. We present exact numeric results on the ground state properties, Lehmann spectral function and optical conductivity of different strongly correlated systems: Frohlich polaron, Rashba-Pekar exciton-polaron, pseudo Jahn-Teller polaron, exciton, and interacting with phonons hole in the t-J model.",2007,2015-05-13,https://arxiv.org/abs/0704.0025,1
0802.0223,Multivariate stochastic volatility using state space models,"A Bayesian procedure is developed for multivariate stochastic volatility, using state space models. An autoregressive model for the log-returns is employed. We generalize the inverted Wishart distribution to allow for different correlation structure between the observation and state innovation vectors and we extend the convolution between the Wishart and the multivariate singular beta distribution. A multiplicative model based on the generalized inverted Wishart and multivariate singular beta distributions is proposed for the evolution of the volatility and a flexible sequential volatility updating is employed. The proposed algorithm for the volatility is fast and computationally cheap and it can be used for on-line forecasting. The methods are illustrated with an example consisting of foreign exchange rates data of 8 currencies. The empirical results suggest that time-varying correlations can be estimated efficiently, even in situations of high dimensional data.",2008,2008-12-02,https://arxiv.org/abs/0802.0223,1
0704.0023,ALMA as the ideal probe of the solar chromosphere,"The very nature of the solar chromosphere, its structuring and dynamics, remains far from being properly understood, in spite of intensive research. Here we point out the potential of chromospheric observations at millimeter wavelengths to resolve this long-standing problem. Computations carried out with a sophisticated dynamic model of the solar chromosphere due to Carlsson and Stein demonstrate that millimeter emission is extremely sensitive to dynamic processes in the chromosphere and the appropriate wavelengths to look for dynamic signatures are in the range 0.8-5.0 mm. The model also suggests that high resolution observations at mm wavelengths, as will be provided by ALMA, will have the unique property of reacting to both the hot and the cool gas, and thus will have the potential of distinguishing between rival models of the solar atmosphere. Thus, initial results obtained from the observations of the quiet Sun at 3.5 mm with the BIMA array (resolution of 12 arcsec) reveal significant oscillations with amplitudes of 50-150 K and frequencies of 1.5-8 mHz with a tendency toward short-period oscillations in internetwork and longer periods in network regions. However higher spatial resolution, such as that provided by ALMA, is required for a clean separation between the features within the solar atmosphere and for an adequate comparison with the output of the comprehensive dynamic simulations.",2007,2009-06-23,https://arxiv.org/abs/0704.0023,1
0704.0357,Evolutionary games on minimally structured populations,"Population structure induced by both spatial embedding and more general networks of interaction, such as model social networks, have been shown to have a fundamental effect on the dynamics and outcome of evolutionary games. These effects have, however, proved to be sensitive to the details of the underlying topology and dynamics. Here we introduce a minimal population structure that is described by two distinct hierarchical levels of interaction. We believe this model is able to identify effects of spatial structure that do not depend on the details of the topology. We derive the dynamics governing the evolution of a system starting from fundamental individual level stochastic processes through two successive meanfield approximations. In our model of population structure the topology of interactions is described by only two parameters: the effective population size at the local scale and the relative strength of local dynamics to global mixing. We demonstrate, for example, the existence of a continuous transition leading to the dominance of cooperation in populations with hierarchical levels of unstructured mixing as the benefit to cost ratio becomes smaller then the local population size. Applying our model of spatial structure to the repeated prisoner's dilemma we uncover a novel and counterintuitive mechanism by which the constant influx of defectors sustains cooperation. Further exploring the phase space of the repeated prisoner's dilemma and also of the ""rock-paper-scissor"" game we find indications of rich structure and are able to reproduce several effects observed in other models with explicit spatial embedding, such as the maintenance of biodiversity and the emergence of global oscillations.",2007,2009-11-13,https://arxiv.org/abs/0704.0357,3
0804.2441,Topological identification in networks of dynamical systems,"The paper deals with the problem of reconstructing the topological structure of a network of dynamical systems. A distance function is defined in order to evaluate the ""closeness"" of two processes and a few useful mathematical properties are derived. Theoretical results to guarantee the correctness of the identification procedure for networked linear systems with tree topology are provided as well. Finally, the application of the techniques to the analysis of an actual complex network, i.e. to high frequency time series of the stock market, is illustrated.",2008,2008-12-02,https://arxiv.org/abs/0804.2441,2
1308.0400,Cognitive Random Stepped Frequency Radar with Sparse Recovery,"Random stepped frequency (RSF) radar, which transmits random-frequency pulses, can suppress the range ambiguity, improve convert detection, and possess excellent electronic counter-countermeasures (ECCM) ability [1]. In this paper, we apply a sparse recovery method to estimate the range and Doppler of targets. We also propose a cognitive mechanism for RSF radar to further enhance the performance of the sparse recovery method. The carrier frequencies of transmitted pulses are adaptively designed in response to the observed circumstance. We investigate the criterion to design carrier frequencies, and efficient methods are then devised. Simulation results demonstrate that the adaptive frequency-design mechanism significantly improves the performance of target reconstruction in comparison with the non-adaptive mechanism.",2013,2018-08-30,https://arxiv.org/abs/1308.0400,1
0704.0012,Distribution of integral Fourier Coefficients of a Modular Form of Half Integral Weight Modulo Primes,"Recently, Bruinier and Ono classified cusp forms $f(z) := \sum_{n=0}^{\infty} a_f(n)q ^n \in S_{\lambda+1/2}(\Gamma_0(N),\chi)\cap \mathbb{Z}[[q]]$ that does not satisfy a certain distribution property for modulo odd primes $p$. In this paper, using Rankin-Cohen Bracket, we extend this result to modular forms of half integral weight for primes $p \geq 5$. As applications of our main theorem we derive distribution properties, for modulo primes $p\geq5$, of traces of singular moduli and Hurwitz class number. We also study an analogue of Newman's conjecture for overpartitions.",2007,2007-05-23,https://arxiv.org/abs/0704.0012,1
0705.0569,Mixed models for longitudinal left-censored repeated measures,"Longitudinal studies could be complicated by left-censored repeated measures. For example, in Human Immunodeficiency Virus infection, there is a detection limit of the assay used to quantify the plasma viral load. Simple imputation of the limit of the detection or of half of this limit for left-censored measures biases estimations and their standard errors. In this paper, we review two likelihood-based methods proposed to handle left-censoring of the outcome in linear mixed model. We show how to fit these models using SAS Proc NLMIXED and we compare this tool with other programs. Indications and limitations of the programs are discussed and an example in the field of HIV infection is shown.",2007,2007-05-23,https://arxiv.org/abs/0705.0569,1
