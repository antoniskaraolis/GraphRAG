id,title,abstract,year,update_date,url,version_count
0909.4239,mRNA diffusion explains protein gradients in \textit{Drosophila} early development,"We propose a new model describing the production and the establishment of the stable gradient of the Bicoid protein along the antero-posterior axis of the embryo of \textit{Drosophila}. In this model, we consider that \textit{bicoid} mRNA diffuses along the antero-posterior axis of the embryo and the protein is produced in the ribosomes localized near the syncytial nuclei. Bicoid protein stays localized near the syncytial nuclei as observed in experiments. We calibrate the parameters of the mathematical model with experimental data taken during the cleavage stages 11 to 14 of the developing embryo of \textit{Drosophila}. We obtain good agreement between the experimental and the model gradients, with relative errors in the range 5-8%. The inferred diffusion coefficient of \textit{bicoid} mRNA is in the range 4.6\times 10^{-12}-1.5\times 10^{-11}m^2s^{-1}, in agreement with the theoretical predictions and experimental measurements for the diffusion of macromolecules in the cytoplasm. We show that the model based on the mRNA diffusion hypothesis is consistent with the known observational data, supporting the recent experimental findings of the gradient of \textit{bicoid} mRNA in \textit{Drosophila} [Spirov et al. (2009) Development 136:605-614].",2009,2009-09-24,https://arxiv.org/abs/0909.4239,1
0905.2926,One-Dimensional Pricing of CPPI,Constant Proportion Portfolio Insurance (CPPI) is an investment strategy designed to give participation in the performance of a risky asset while protecting the invested capital. This protection is however not perfect and the gap risk must be quantified. CPPI strategies are path-dependent and may have American exercise which makes their valuation complex. A naive description of the state of the portfolio would involve three or even four variables. In this paper we prove that the system can be described as a discrete-time Markov process in one single variable if the underlying asset follows a homogeneous process. This yields an efficient pricing scheme using transition probabilities. Our framework is flexible enough to handle most features of traded CPPIs including profit lock-in and other kinds of strategies with discrete-time reallocation.,2009,2010-02-10,https://arxiv.org/abs/0905.2926,2
0704.0872,Spectral perturbation bounds for selfadjoint operators,We give general spectral and eigenvalue perturbation bounds for a selfadjoint operator perturbed in the sense of the pseudo-Friedrichs extension. We also give several generalisations of the aforementioned extension. The spectral bounds for finite eigenvalues are obtained by using analyticity and monotonicity properties (rather than variational principles) and they are general enough to include eigenvalues in gaps of the essential spectrum.,2007,2008-01-21,https://arxiv.org/abs/0704.0872,2
0705.0038,Prime Graphs and Exponential Composition of Species,"In this paper, we enumerate prime graphs with respect to the Cartesian multiplication of graphs. We use the unique factorization of a connected graph into the product of prime graphs given by Sabidussi to find explicit formulas for labeled and unlabeled prime graphs. In the case of species, we construct the exponential composition of species based on the arithmetic product of species of Maia and M\'endez and the quotient species, and express the species of connected graphs as the exponential composition of the species of prime graphs.",2007,2009-11-10,https://arxiv.org/abs/0705.0038,4
0905.0727,Statistical Methods for Determining Optimal Rifle Cartridge Dimensions,"We have designed and carried out a statistical study to determine the optimal cartridge dimensions for a Savage 10FLP law enforcement grade rifle. Optimal performance is defined as minimal group diameter. A full factorial block design with two main factors and one blocking factor was used. The two main factors were bullet seating depth and powder charge. The experimental units were individual shots taken from a bench-rest position and fired into separate targets. Additionally, thirteen covariates describing various cartridge dimensions were recorded. The data analysis includes ANOVA and ANCOVA. We will describe the experiment, the analysis, and some results.",2009,2009-05-07,https://arxiv.org/abs/0905.0727,1
0704.2626,Transport measurements across a tunable potential barrier in graphene,"The peculiar nature of electron scattering in graphene is among many exciting theoretical predictions for the physical properties of this material. To investigate electron scattering properties in a graphene plane, we have created a gate-tunable potential barrier within a single-layer graphene sheet. We report measurements of electrical transport across this structure as the tunable barrier potential is swept through a range of heights. When the barrier is sufficiently strong to form a bipolar junctions (npn or pnp) within the graphene sheet, the resistance across the barrier sharply increases. We compare these results to predictions for both diffusive and ballistic transport, as the barrier rises on a length scale comparable to the mean free path. Finally, we show how a magnetic field modifies transport across the barrier.",2007,2007-06-19,https://arxiv.org/abs/0704.2626,2
1209.2298,The Future Has Thicker Tails than the Past: Model Error As Branching Counterfactuals,"Ex ante forecast outcomes should be interpreted as counterfactuals (potential histories), with errors as the spread between outcomes. Reapplying measurements of uncertainty about the estimation errors of the estimation errors of an estimation leads to branching counterfactuals. Such recursions of epistemic uncertainty have markedly different distributial properties from conventional sampling error. Nested counterfactuals of error rates invariably lead to fat tails, regardless of the probability distribution used, and to powerlaws under some conditions. A mere .01% branching error rate about the STD (itself an error rate), and .01% branching error rate about that error rate, etc. (recursing all the way) results in explosive (and infinite) higher moments than 1. Missing any degree of regress leads to the underestimation of small probabilities and concave payoffs (a standard example of which is Fukushima). The paper states the conditions under which higher order rates of uncertainty (expressed in spreads of counterfactuals) alters the shapes the of final distribution and shows which a priori beliefs about conterfactuals are needed to accept the reliability of conventional probabilistic methods (thin tails or mildly fat tails).",2012,2012-09-12,https://arxiv.org/abs/1209.2298,1
0705.0658,Central Limit Theorem for the Excited Random Walk in dimension $d \geq 2$,We prove that a law of large numbers and a central limit theorem hold for the excited random walk model in every dimension $d \geq 2$.,2007,2007-06-06,https://arxiv.org/abs/0705.0658,3
1908.00099,Testing for Externalities in Network Formation Using Simulation,"We discuss a simplified version of the testing problem considered by Pelican and Graham (2019): testing for interdependencies in preferences over links among N (possibly heterogeneous) agents in a network. We describe an exact test which conditions on a sufficient statistic for the nuisance parameter characterizing any agent-level heterogeneity. Employing an algorithm due to Blitzstein and Diaconis (2011), we show how to simulate the null distribution of the test statistic in order to estimate critical values and/or p-values. We illustrate our methods using the Nyakatoke risk-sharing network. We find that the transitivity of the Nyakatoke network far exceeds what can be explained by degree heterogeneity across households alone.",2019,2019-08-02,https://arxiv.org/abs/1908.00099,1
1710.09676,Near-Optimal Sparse Sensing for Gaussian Detection with Correlated Observations,"Detection of a signal under noise is a classical signal processing problem. When monitoring spatial phenomena under a fixed budget, i.e., either physical, economical or computational constraints, the selection of a subset of available sensors, referred to as sparse sensing, that meets both the budget and performance requirements is highly desirable. Unfortunately, the subset selection problem for detection under dependent observations is combinatorial in nature and suboptimal subset selection algorithms must be employed. In this work, different from the widely used convex relaxation of the problem, we leverage submodularity, the diminishing returns property, to provide practical near optimal algorithms suitable for large-scale subset selection. This is achieved by means of low-complexity greedy algorithms, which incur a reduced computational complexity compared to their convex counterparts.",2017,2018-08-01,https://arxiv.org/abs/1710.09676,1
0705.4080,Aperiodic substitutional systems and their Bratteli diagrams,In the paper we study aperiodic substitutional dynamical systems arisen from non-primitive substitutions. We prove that the Vershik homeomorphism $\phi$ of a stationary ordered Bratteli diagram is homeomorphic to an aperiodic substitutional system if and only if no restriction of $\phi$ to a minimal component is homeomorphic to an odometer. We also show that every aperiodic substitutional system generated by a substitution with nesting property is homeomorphic to the Vershik map of a stationary ordered Bratteli diagram. It is proved that every aperiodic substitutional system is recognizable. The classes of $m$-primitive substitutions and associated to them derivative substitutions are studied. We discuss also the notion of expansiveness for Cantor dynamical systems of finite rank.,2007,2007-05-29,https://arxiv.org/abs/0705.4080,1
1908.10065,Future competitive bioenergy technologies in the German heat sector: Findings from an economic optimization approach,"Meeting the defined greenhouse gas (GHG) reduction targets in Germany is only possible by switching to renewable technologies in the energy sector. A major share of that reduction needs to be covered by the heat sector, which accounts for ~35% of the energy based emissions in Germany. Biomass is the renewable key player in the heterogeneous heat sector today. Its properties such as weather independency, simple storage and flexible utilization open up a wide field of applications for biomass. However, in a future heat sector fulfilling GHG reduction targets and energy sectors being increasingly connected: which bioenergy technology concepts are competitive options against other renewable heating systems? In this paper, the cost optimal allocation of the limited German biomass potential is investigated under longterm scenarios using a mathematical optimization approach. The model results show that bioenergy can be a competitive option in the future. Especially the use of biomass from residues can be highly competitive in hybrid combined heat and power (CHP) pellet combustion plants in the private household sector. However, towards 2050, wood based biomass use in high temperature industry applications is found to be the most cost efficient way to reduce heat based emissions by 95% in 2050.",2019,2020-03-24,https://arxiv.org/abs/1908.10065,2
0903.4219,"Genome-scale reconstruction of the metabolic network in Yersinia pestis, strain 91001","The gram-negative bacterium Yersinia pestis, the aetiological agent of bubonic plague, is one the deadliest pathogens known to man. Despite its historical reputation, plague is a modern disease which annually afflicts thousands of people. Public safety considerations greatly limit clinical experimentation on this organism and thus development of theoretical tools to analyze the capabilities of this pathogen is of utmost importance. Here, we report the first genome-scale metabolic model of Yersinia pestis biovar Mediaevalis based both on its recently annotated genome, and physiological and biochemical data from literature. Our model demonstrates excellent agreement with Y. pestis known metabolic needs and capabilities. Since Y. pestis is a meiotrophic organism, we have developed CryptFind, a systematic approach to identify all candidate cryptic genes responsible for known and theoretical meiotrophic phenomena. In addition to uncovering every known cryptic gene for Y. pestis, our analysis of the rhamnose fermentation pathway suggests that betB is the responsible cryptic gene. Despite all of our medical advances, we still do not have a vaccine for bubonic plague. Recent discoveries of antibiotic resistant strains of Yersinia pestis coupled with the threat of plague being used as a bioterrorism weapon compel us to develop new tools for studying the physiology of this deadly pathogen. Using our theoretical model, we can study the cells phenotypic behavior under different circumstances and identify metabolic weaknesses which may be harnessed for the development of therapeutics. Additionally, the automatic identification of cryptic genes expands the usage of genomic data for pharmaceutical purposes.",2009,2009-03-26,https://arxiv.org/abs/0903.4219,1
1807.11382,Bayesian Calibration using Different Prior Distributions: an Iterative Maximum A Posteriori Approach for Radio Interferometers,"In this paper, we aim to design robust estimation techniques based on the compound-Gaussian (CG) process and adapted for calibration of radio interferometers. The motivation beyond this is due to the presence of outliers leading to an unrealistic traditional Gaussian noise assumption. Consequently, to achieve robustness, we adopt a maximum a posteriori (MAP) approach which exploits Bayesian statistics and follows a sequential updating procedure here. The proposed algorithm is applied in a multi-frequency scenario in order to enhance the estimation and correction of perturbation effects. Numerical simulations assess the performance of the proposed algorithm for different noise models, Student's t, K, Laplace, Cauchy and inverse-Gaussian compound-Gaussian distributions w.r.t. the classical non-robust Gaussian noise assumption.",2018,2018-07-31,https://arxiv.org/abs/1807.11382,1
1810.10408,Multi-Agent Reinforcement Learning Based Resource Allocation for UAV Networks,"Unmanned aerial vehicles (UAVs) are capable of serving as aerial base stations (BSs) for providing both cost-effective and on-demand wireless communications. This article investigates dynamic resource allocation of multiple UAVs enabled communication networks with the goal of maximizing long-term rewards. More particularly, each UAV communicates with a ground user by automatically selecting its communicating users, power levels and subchannels without any information exchange among UAVs. To model the uncertainty of environments, we formulate the long-term resource allocation problem as a stochastic game for maximizing the expected rewards, where each UAV becomes a learning agent and each resource allocation solution corresponds to an action taken by the UAVs. Afterwards, we develop a multi-agent reinforcement learning (MARL) framework that each agent discovers its best strategy according to its local observations using learning. More specifically, we propose an agent-independent method, for which all agents conduct a decision algorithm independently but share a common structure based on Q-learning. Finally, simulation results reveal that: 1) appropriate parameters for exploitation and exploration are capable of enhancing the performance of the proposed MARL based resource allocation algorithm; 2) the proposed MARL algorithm provides acceptable performance compared to the case with complete information exchanges among UAVs. By doing so, it strikes a good tradeoff between performance gains and information exchange overheads.",2018,2018-10-25,https://arxiv.org/abs/1810.10408,1
1807.09531,A Generalized Spectral Shaping Method for OFDM Signals,"Orthogonal frequency division multiplexing (OFDM) signals with rectangularly windowed pulses exhibit low spectral confinement. Two approaches usually referred to as pulse-shaping and active interference cancellation (AIC) are classically employed to reduce the out-of-band emission (OOBE) without affecting the receiver. This paper proposes a spectral shaping method that generalizes and unifies these two strategies. To this end, the OFDM carriers are shaped with novel pulses, referred to as generalized pulses, that consist of the ones used in conventional OFDM systems plus a series of cancellation terms aimed at reducing the OOBE of the former. Hence, each generalized pulse embeds all the terms required to reduce its spectrum in the desired bands. This leads to a data-independent optimization problem that notably simplifies the implementation complexity and allows the analytical calculation of the resulting power spectral density (PSD), which in most methods found in the literature can only be estimated by means of simulations. As an example of its performance, the proposed technique allows complying with the stringent PSD mask imposed by the EN 50561-1 with a data carrier loss lower than 4%. By contrasts, 28% of the data carriers have to be nulled when pulse-shaping is employed in this scenario.",2018,2018-07-26,https://arxiv.org/abs/1807.09531,1
1003.0182,Product-limit estimators of the gap time distribution of a renewal process under different sampling patterns,"Nonparametric estimation of the gap time distribution in a simple renewal process may be considered a problem in survival analysis under particular sampling frames corresponding to how the renewal process is observed. This note describes several such situations where simple product limit estimators, though inefficient, may still be useful.",2010,2023-05-02,https://arxiv.org/abs/1003.0182,2
1002.2702,Bayesian computational methods,"In this chapter, we will first present the most standard computational challenges met in Bayesian Statistics, focussing primarily on mixture estimation and on model choice issues, and then relate these problems with computational solutions. Of course, this chapter is only a terse introduction to the problems and solutions related to Bayesian computations. For more complete references, see Robert and Casella (2004, 2009), or Marin and Robert (2007), among others. We also restrain from providing an introduction to Bayesian Statistics per se and for comprehensive coverage, address the reader to Robert (2007), (again) among others.",2010,2010-02-16,https://arxiv.org/abs/1002.2702,1
1711.04256,Power System Transient Stability Assessment Using Couple Machines Method,"Analyzing the stability of the power system by using a few machines is promising for transient stability assessment. A hybrid direct-time-domain method that is fully based on the thinking of partial energy function is proposed in this paper. During post-fault period, a pair of machines with high rotor speed difference is defined as couple machines, and the stability analysis of the system is transformed into that of several pairs of couple machines. Based on the prediction of power-angle curve of couple machines within a sampling window after fault clearing, the proposed method avoids the definition of Center of Inertia (COI) and it can also evaluate the stability margin of the system by using the predicted power-angle curve. Simulation results demonstrate its effectiveness in transient stability assessment.",2017,2017-11-15,https://arxiv.org/abs/1711.04256,1
1011.2005,Likelihood inference for particle location in fluorescence microscopy,"We introduce a procedure to automatically count and locate the fluorescent particles in a microscopy image. Our procedure employs an approximate likelihood estimator derived from a Poisson random field model for photon emission. Estimates of standard errors are generated for each image along with the parameter estimates, and the number of particles in the image is determined using an information criterion and likelihood ratio tests. Realistic simulations show that our procedure is robust and that it leads to accurate estimates, both of parameters and of standard errors. This approach improves on previous ad hoc least squares procedures by giving a more explicit stochastic model for certain fluorescence images and by employing a consistent framework for analysis.",2010,2010-11-10,https://arxiv.org/abs/1011.2005,1
0707.4524,Image Authentication Based on Neural Networks,"Neural network has been attracting more and more researchers since the past decades. The properties, such as parameter sensitivity, random similarity, learning ability, etc., make it suitable for information protection, such as data encryption, data authentication, intrusion detection, etc. In this paper, by investigating neural networks' properties, the low-cost authentication method based on neural networks is proposed and used to authenticate images or videos. The authentication method can detect whether the images or videos are modified maliciously. Firstly, this chapter introduces neural networks' properties, such as parameter sensitivity, random similarity, diffusion property, confusion property, one-way property, etc. Secondly, the chapter gives an introduction to neural network based protection methods. Thirdly, an image or video authentication scheme based on neural networks is presented, and its performances, including security, robustness and efficiency, are analyzed. Finally, conclusions are drawn, and some open issues in this field are presented.",2007,2007-08-01,https://arxiv.org/abs/0707.4524,1
0704.3980,A construction of generalized Harish-Chandra modules for locally reductive Lie algebras,"We study cohomological induction for a pair $(\frak g,\frak k)$, $\frak g$ being an infinite dimensional locally reductive Lie algebra and $\frak k \subset\frak g$ being of the form $\frak k_0 + C_\gg(\frak k_0)$, where $\frak k_0\subset\frak g$ is a finite dimensional reductive in $\frak g$ subalgebra and $C_{\gg} (\frak k_0)$ is the centralizer of $\frak k_0$ in $\frak g$. We prove a general non-vanishing and $\frak k$-finiteness theorem for the output. This yields in particular simple $(\frak g,\frak k)$-modules of finite type over $\frak k$ which are analogs of the fundamental series of generalized Harish-Chandra modules constructed in \cite{PZ1} and \cite{PZ2}. We study explicit versions of the construction when $\frak g$ is a root-reductive or diagonal locally simple Lie algebra.",2007,2007-05-23,https://arxiv.org/abs/0704.3980,1
0909.5194,Dirichlet Process Mixtures of Generalized Linear Models,"We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM), a new method of nonparametric regression that accommodates continuous and categorical inputs, and responses that can be modeled by a generalized linear model. We prove conditions for the asymptotic unbiasedness of the DP-GLM regression mean function estimate. We also give examples for when those conditions hold, including models for compactly supported continuous distributions and a model with continuous covariates and categorical response. We empirically analyze the properties of the DP-GLM and why it provides better results than existing Dirichlet process mixture regression models. We evaluate DP-GLM on several data sets, comparing it to modern methods of nonparametric regression like CART, Bayesian trees and Gaussian processes. Compared to existing techniques, the DP-GLM provides a single model (and corresponding inference algorithms) that performs well in many regression settings.",2009,2010-07-16,https://arxiv.org/abs/0909.5194,2
0710.5085,"William H. Kruskal, Mentor and Friend","Discussion of ``The William Kruskal Legacy: 1919--2005'' by Stephen E. Fienberg, Stephen M. Stigler and Judith M. Tanur [arXiv:0710.5063]",2007,2007-11-06,https://arxiv.org/abs/0710.5085,1
0904.4131,Executing large orders in a microscopic market model,"In a recent paper, Alfonsi, Fruth and Schied (AFS) propose a simple order book based model for the impact of large orders on stock prices. They use this model to derive optimal strategies for the execution of large orders. We apply these strategies to an agent-based stochastic order book model that was recently proposed by Bovier, \v{C}ern\'{y} and Hryniv, but already the calibration fails. In particular, from our simulations the recovery speed of the market after a large order is clearly dependent on the order size, whereas the AFS model assumes a constant speed. For this reason, we propose a generalization of the AFS model, the GAFS model, that incorporates this dependency, and prove the optimal investment strategies. As a corollary, we find that we can derive the ``correct'' constant resilience speed for the AFS model from the GAFS model such that the optimal strategies of the AFS and the GAFS model coincide. Finally, we show that the costs of applying the optimal strategies of the GAFS model to the artificial market environment still differ significantly from the model predictions, indicating that even the improved model does not capture all of the relevant details of a real market.",2009,2010-01-11,https://arxiv.org/abs/0904.4131,2
0705.1912,Necessary Conditions for Geometric Realizability of Simplicial Complexes,We associate with any simplicial complex $\K$ and any integer $m$ a system of linear equations and inequalities. If $\K$ has a simplicial embedding in $\R^m$ then the system has an integer solution. This result extends the work of I. Novik (2000).,2007,2007-06-21,https://arxiv.org/abs/0705.1912,2
1007.5519,Phylogenetic Proximity and Nestedness in Mutualistic Ecosystems,"We investigate how the pattern of contacts between species in mutualistic ecosystems is affected by the phylogenetic proximity between the species of each guild. We develop several theoretical tools to measure that effect and we use them to examine some real mutualistic sytems. We aim at establishing the role of such proximity in the emergence of a nested pattern of contacts. We conclude that although phylogenetic proximity is compatible with nestedness it can not be claimed to determine it. We find that nestedness can instead be attributed to a general rule by which species tend to behave as generalists holding contacts with counterparts that already have a large number of contacts. A nested ecosystem generated by this rule, shows high phylogenetic diversity. This is to say, the counterparts of species having similar degrees are not phylogenetic neighbours.",2010,2010-08-02,https://arxiv.org/abs/1007.5519,1
1309.1871,Statistical inference of co-movements of stocks during a financial crisis,"In order to figure out and to forecast the emergence phenomena of social systems, we propose several probabilistic models for the analysis of financial markets, especially around a crisis. We first attempt to visualize the collective behaviour of markets during a financial crisis through cross-correlations between typical Japanese daily stocks by making use of multi- dimensional scaling. We find that all the two-dimensional points (stocks) shrink into a single small region when a economic crisis takes place. By using the properties of cross-correlations in financial markets especially during a crisis, we next propose a theoretical framework to predict several time-series simultaneously. Our model system is basically described by a variant of the multi-layered Ising model with random fields as non-stationary time series. Hyper-parameters appearing in the probabilistic model are estimated by means of minimizing the 'cumulative error' in the past market history. The justification and validity of our approaches are numerically examined for several empirical data sets.",2013,2015-06-17,https://arxiv.org/abs/1309.1871,1
1103.6119,"Auto-associative models, nonlinear Principal component analysis, manifolds and projection pursuit","In this paper, auto-associative models are proposed as candidates to the generalization of Principal Component Analysis. We show that these models are dedicated to the approximation of the dataset by a manifold. Here, the word ""manifold"" refers to the topology properties of the structure. The approximating manifold is built by a projection pursuit algorithm. At each step of the algorithm, the dimension of the manifold is incremented. Some theoretical properties are provided. In particular, we can show that, at each step of the algorithm, the mean residuals norm is not increased. Moreover, it is also established that the algorithm converges in a finite number of steps. Some particular auto-associative models are exhibited and compared to the classical PCA and some neural networks models. Implementation aspects are discussed. We show that, in numerous cases, no optimization procedure is required. Some illustrations on simulated and real data are presented.",2011,2011-04-01,https://arxiv.org/abs/1103.6119,1
1212.4279,"A Note on ""A Family of Maximum Entropy Densities Matching Call Option Prices""","In Neri and Schneider (2012) we presented a method to recover the Maximum Entropy Density (MED) inferred from prices of call and digital options on a set of n strikes. To find the MED we need to numerically invert a one-dimensional function for n values and a Newton-Raphson method is suggested. In this note we revisit this inversion problem and show that it can be rewritten in terms of the Langevin function for which numerical approximations of its inverse are known. The approach is very similar to that of Buchen and Kelly (BK) with the difference that BK only requires call option prices. Then, in continuation of our first paper, we presented another approach which uses call prices only and recovers the same density as BK with a few advantages, notably, numerical stability. This second paper provides a detailed analysis of convergence and, in particular, gives various estimates of how far (in different senses) the iterative algorithm is from the solution. These estimates rely on a constant m > 0. The larger m is the better the estimates will be. A concrete value of m is suggested in the second paper, and this note provides a sharper value.",2012,2012-12-19,https://arxiv.org/abs/1212.4279,1
0908.1082,Strict Local Martingale Deflators and Pricing American Call-Type Options,"We solve the problem of pricing and optimal exercise of American call-type options in markets which do not necessarily admit an equivalent local martingale measure. This resolves an open question proposed by Fernholz and Karatzas [Stochastic Portfolio Theory: A Survey, Handbook of Numerical Analysis, 15:89-168, 2009].",2009,2009-12-21,https://arxiv.org/abs/0908.1082,4
0705.1629,Lie antialgebras: premices,"The main purpose of this work is to develop the basic notions of the Lie theory for commutative algebras. We introduce a class of $\mathbbZ_2$-graded commutative but not associative algebras that we call ``Lie antialgebras''. These algebras are closely related to Lie (super)algebras and, in some sense, link together commutative and Lie algebras. The main notions we define in this paper are: representations of Lie antialgebras, an analog of the Lie-Poisson bivector (which is not Poisson) and central extensions. We also classify simple finite-dimensional Lie antialgebras.",2007,2010-10-18,https://arxiv.org/abs/0705.1629,6
1004.1758,Consistent Valuation of Bespoke CDO Tranches,"This paper describes a consistent and arbitrage-free pricing methodology for bespoke CDO tranches. The proposed method is a multi-factor extension to the (Li 2009) model, and it is free of the known flaws in the current standard pricing method of base correlation mapping. This method assigns a distinct market factor to each liquid credit index and models the correlation between these market factors explicitly. A low-dimensional semi-analytical Monte Carlo is shown to be very efficient in computing the PVs and risks of bespoke tranches. Numerical examples show that resulting bespoke tranche prices are generally in line with the current standard method of base correlation with TLP mapping. Practical issues such as model deltas and quanto adjustment are also discussed as numerical examples.",2010,2010-04-13,https://arxiv.org/abs/1004.1758,1
0705.2739,Generalized functions as sequence spaces with ultranorms,"We review our recent formulation of Colombeau type algebras as Hausdorff sequence spaces with ultranorms, defined by sequences of exponential weights. We extend previous results and give new perspectives related to echelon type spaces, possible generalisations, asymptotic algebras, concepts of association, and applications thereof.",2007,2007-05-23,https://arxiv.org/abs/0705.2739,1
1106.1999,Pricing of average strike Asian call option using numerical PDE methods,"In this paper, a standard PDE for the pricing of arithmetic average strike Asian call option is presented. A Crank-Nicolson Implicit Method and a Higher Order Compact finite difference scheme for this pricing problem is derived. Both these schemes were implemented for various values of risk free rate and volatility. The option prices for the same set of values of risk free rate and volatility was also computed using Monte Carlo simulation. The comparative results of the two numerical PDE methods shows close match with the Monte Carlo results, with the Higher Order Compact scheme exhibiting a better match. To the best of our knowledge, this is the first work to use the numerical PDE approach for pricing Asian call options with average strike.",2011,2011-06-13,https://arxiv.org/abs/1106.1999,1
0704.1487,"Wavelet frames, Bergman spaces and Fourier transforms of Laguerre functions","The Fourier transforms of Laguerre functions play the same canonical role in wavelet analysis as do the Hermite functions in Gabor analysis. We will use them as analyzing wavelets in a similar way the Hermite functions were recently by K. Groechenig and Y. Lyubarskii in ""Gabor frames with Hermite functions, C. R. Acad. Sci. Paris, Ser. I 344 157-162 (2007)"". Building on the work of K. Seip, ""Beurling type density theorems in the unit disc, Invent. Math., 113, 21-39 (1993)"", concerning sampling sequences on weighted Bergman spaces, we find a sufficient density condition for constructing frames by translations and dilations of the Fourier transform of the nth Laguerre function. As in Groechenig-Lyubarskii theorem, the density increases with n, and in the special case of the hyperbolic lattice in the upper half plane it is given by b\log a<\frac{4\pi}{2n+\alpha}, where alpha is the parameter of the Laguerre function.",2007,2007-05-23,https://arxiv.org/abs/0704.1487,1
0704.2865,Classical and quantum randomness and the financial market,"We analyze complexity of financial (and general economic) processes by comparing classical and quantum-like models for randomness. Our analysis implies that it might be that a quantum-like probabilistic description is more natural for financial market than the classical one. A part of our analysis is devoted to study the possibility of application of the quantum probabilistic model to agents of financial market. We show that, although the direct quantum (physical) reduction (based on using the scales of quantum mechanics) is meaningless, one may apply so called quantum-like models. In our approach quantum-like probabilistic behaviour is a consequence of contextualy of statistical data in finances (and economics in general). However, our hypothesis on ""quantumness"" of financial data should be tested experimentally (as opposed to the conventional description based on the noncontextual classical probabilistic approach). We present a new statistical test based on a generalization of the well known in quantum physics Bell's inequality.",2007,2014-03-13,https://arxiv.org/abs/0704.2865,1
0804.0221,Thermodynamics of a model for RNA folding,"We analyze the thermodynamic properties of a simplified model for folded RNA molecules recently studied by G. Vernizzi, H. Orland, A. Zee (in {\it Phys. Rev. Lett.} {\bf 94} (2005) 168103). The model consists of a chain of one-flavor base molecules with a flexible backbone and all possible pairing interactions equally allowed. The spatial pseudoknot structure of the model can be efficiently studied by introducing a $N \times N$ hermitian random matrix model at each chain site, and associating Feynman diagrams of these models to spatial configurations of the molecules. We obtain an exact expression for the topological expansion of the partition function of the system. We calculate exact and asymptotic expressions for the free energy, specific heat, entanglement and chemical potential and study their behavior as a function of temperature. Our results are consistent with the interpretation of $1/N$ as being a measure of the concentration of $\rm{Mg}^{++}$ in solution.",2008,2009-11-13,https://arxiv.org/abs/0804.0221,4
0708.0603,Public Cluster : parallel machine with multi-block approach,"We introduce a new approach to enable an open and public parallel machine which is accessible for multi users with multi jobs belong to different blocks running at the same time. The concept is required especially for parallel machines which are dedicated for public use as implemented at the LIPI Public Cluster. We have deployed the simplest technique by running multi daemons of parallel processing engine with different configuration files specified for each user assigned to access the system, and also developed an integrated system to fully control and monitor the whole system over web. A brief performance analysis is also given for Message Parsing Interface (MPI) engine. It is shown that the proposed approach is quite reliable and affect the whole performances only slightly.",2007,2007-08-07,https://arxiv.org/abs/0708.0603,1
0704.2402,Indices of the iterates of $R^3$-homeomorphisms at Lyapunov stable fixed points,"Given any positive sequence (\{c_n\}_{n \in {\Bbb N}}), we construct orientation preserving homeomorphisms (f:{\Bbb R}^3 \to {\Bbb R}^3) such that (Fix(f)=Per(f)=\{0\}), (0) is Lyapunov stable and (\limsup \frac{|i(f^m, 0)|}{c_m}= \infty). We will use our results to discuss and to point out some strong differences with respect to the computation and behavior of the sequences of the indices of planar homeomorphisms.",2007,2007-05-23,https://arxiv.org/abs/0704.2402,1
0704.2725,Exploiting Heavy Tails in Training Times of Multilayer Perceptrons: A Case Study with the UCI Thyroid Disease Database,"The random initialization of weights of a multilayer perceptron makes it possible to model its training process as a Las Vegas algorithm, i.e. a randomized algorithm which stops when some required training error is obtained, and whose execution time is a random variable. This modeling is used to perform a case study on a well-known pattern recognition benchmark: the UCI Thyroid Disease Database. Empirical evidence is presented of the training time probability distribution exhibiting a heavy tail behavior, meaning a big probability mass of long executions. This fact is exploited to reduce the training time cost by applying two simple restart strategies. The first assumes full knowledge of the distribution yielding a 40% cut down in expected time with respect to the training without restarts. The second, assumes null knowledge, yielding a reduction ranging from 9% to 23%.",2007,2011-11-09,https://arxiv.org/abs/0704.2725,2
0704.3940,An obstruction to a knot being deform-spun via Alexander polynomials,"We show that if a co-dimension two knot is deform-spun from a lower-dimensional co-dimension 2 knot, there are constraints on the Alexander polynomials. In particular this shows, for all n, that not all co-dimension 2 knots in S^n are deform-spun from knots in S^{n-1}.",2007,2009-08-11,https://arxiv.org/abs/0704.3940,2
1103.5414,Modeling Long Memory in REITs,"One stylized feature of financial volatility impacting the modeling process is long memory. This paper examines long memory for alternative risk measures, observed absolute and squared returns for Daily REITs and compares the findings for a non- REIT equity index. The paper utilizes a variety of tests for long memory finding evidence that REIT volatility does display persistence, in contrast to the actual return series. Trading volume is found to be strongly associated with long memory. The results do however suggest differences in the findings with regard to REITs in comparison to the broader equity sector which may be due to relatively thin trading during the sample period.",2011,2011-03-29,https://arxiv.org/abs/1103.5414,1
1002.2740,Arbitrage strategy,"An arbitrage strategy allows a financial agent to make certain profit out of nothing, i.e., out of zero initial investment. This has to be disallowed on economic basis if the market is in equilibrium state, as opportunities for riskless profit would result in an instantaneous movement of prices of certain financial instruments. The principle of not allowing for arbitrage opportunities in financial markets has far-reaching consequences, most notably the option-pricing and hedging formulas in complete markets.",2010,2010-02-16,https://arxiv.org/abs/1002.2740,1
0704.0307,Periodic accretion from a circumbinary disk in the young binary UZ Tau E,"Close pre-main-sequence binary stars are expected to clear central holes in their protoplanetary disks, but the extent to which material can flow from the circumbinary disk across the gap onto the individual circumstellar disks has been unclear. In binaries with eccentric orbits, periodic perturbation of the outer disk is predicted to induce mass flow across the gap, resulting in accretion that varies with the binary period. This accretion may manifest itself observationally as periodic changes in luminosity. Here we present a search for such periodic accretion in the pre-main-sequence spectroscopic binary UZ Tau E. We present BVRI photometry spanning three years; we find that the brightness of UZ Tau E is clearly periodic, with a best-fit period of 19.16 +/- 0.04 days. This is consistent with the spectroscopic binary period of 19.13 days, refined here from analysis of new and existing radial velocity data. The brightness of UZ Tau E shows significant random variability, but the overall periodic pattern is a broad peak in enhanced brightness, spanning more than half the binary orbital period. The variability of the H-alpha line is not as clearly periodic, but given the sparseness of the data, some periodic component is not ruled out. The photometric variations are in good agreement with predictions from simulations of binaries with orbital parameters similar to those of UZ Tau E, suggesting that periodic accretion does occur from circumbinary disks, replenishing the inner disks and possibly extending the timescale over which they might form planets.",2007,2009-06-23,https://arxiv.org/abs/0704.0307,2
1812.07776,Constrained Sampling: Optimum Reconstruction in Subspace with Minimax Regret Constraint,"This paper considers the problem of optimum reconstruction in generalized sampling-reconstruction processes (GSRPs). We propose constrained GSRP, a novel framework that minimizes the reconstruction error for inputs in a subspace, subject to a constraint on the maximum regret-error for any other signal in the entire signal space. This framework addresses the primary limitation of existing GSRPs (consistent, subspace and minimax regret), namely, the assumption that the \emph{a priori} subspace is either fully known or fully ignored. We formulate constrained GSRP as a constrained optimization problem, the solution to which turns out to be a convex combination of the subspace and the minimax regret samplings. Detailed theoretical analysis on the reconstruction error shows that constrained sampling achieves a reconstruction that is 1) (sub)optimal for signals in the input subspace, 2) robust for signals around the input subspace, and 3) reasonably bounded for any other signals with a simple choice of the constraint parameter. Experimental results on sampling-reconstruction of a Gaussian input and a speech signal demonstrate the effectiveness of the proposed scheme.",2018,2019-10-23,https://arxiv.org/abs/1812.07776,3
0706.4004,End-to-End Available Bandwidth Measurement Tools : A Comparative Evaluation of Performances,"In recent years, there has been a strong interest in measuring the available bandwidth of network paths. Several methods and techniques have been proposed and various measurement tools have been developed and evaluated. However, there have been few comparative studies with regards to the actual performance of these tools. This paper presents a study of available bandwidth measurement techniques and undertakes a comparative analysis in terms of accuracy, intrusiveness and response time of active probing tools. Finally, measurement errors and the uncertainty of the tools are analysed and overall conclusions made.",2007,2007-06-28,https://arxiv.org/abs/0706.4004,1
0704.0449,"Worldsheet Instantons and Torsion Curves, Part B: Mirror Symmetry","We apply mirror symmetry to the problem of counting holomorphic rational curves in a Calabi-Yau threefold X with Z_3 x Z_3 Wilson lines. As we found in Part A [hep-th/0703182], the integral homology group H_2(X,Z)=Z^3 + Z_3 + Z_3 contains torsion curves. Using the B-model on the mirror of X as well as its covering spaces, we compute the instanton numbers. We observe that X is self-mirror even at the quantum level. Using the self-mirror property, we derive the complete prepotential on X, going beyond the results of Part A. In particular, this yields the first example where the instanton number depends on the torsion part of its homology class. Another consequence is that the threefold X provides a non-toric example for the conjectured exchange of torsion subgroups in mirror manifolds.",2007,2016-09-08,https://arxiv.org/abs/0704.0449,1
1804.05937,Enhancement of Throat Microphone Recordings Using Gaussian Mixture Model Probabilistic Estimator,"The throat microphone is a body-attached transducer that is worn against the neck. It captures the signals that are transmitted through the vocal folds, along with the buzz tone of the larynx. Due to its skin contact, it is more robust to the environmental noise compared to the acoustic microphone that picks up the vibrations through air pressure, and hence the all interventions. The throat speech is partly intelligible, but gives unnatural and croaky sound. This thesis tries to recover missing frequency bands of the throat speech and investigates envelope and excitation mapping problem with joint analysis of throat- and acoustic-microphone recordings. A new phone-dependent GMM-based spectral envelope mapping scheme, which performs the minimum mean square error (MMSE) estimation of the acoustic-microphone spectral envelope, has been proposed. In the source-filter decomposition framework, we observed that the spectral envelope difference of the excitation signals of throat- and acoustic-microphone recordings is an important source of the degradation in the throat-microphone voice quality. Thus, we also model spectral envelope difference of the excitation signals as a spectral tilt vector, and propose a new phone-dependent GMM-based spectral tilt mapping scheme to enhance throat excitation signal. Experimental evaluations are performed to compare the proposed mapping scheme using both objective and subjective evaluations. Objective evaluations are performed with the log-spectral distortion (LSD) and the wide-band perceptual evaluation of speech quality (PESQ) metrics. Subjective evaluations are performed with A/B pair comparison listening test. Both objective and subjective evaluations yield that the proposed phone-dependent mapping consistently improves performances over the state-of-the-art GMM estimators.",2018,2018-04-18,https://arxiv.org/abs/1804.05937,1
0705.0659,Linear systems on a class of anticanonical rational threefolds,"Let X be the blow-up of the three dimensional complex projective space along r general points of a smooth elliptic quartic curve B of P^3 and let L be any line bundle of X. The aim of this paper is to provide an explicit algorithm for determining the dimension of H^0(X,L).",2007,2007-05-23,https://arxiv.org/abs/0705.0659,1
0704.0823,Chromospheric Flares,"In this topical review I revisit the ""chromospheric flare."" This should currently be an outdated concept, because modern data seem to rule out the possiblity of a major flare happening independently in the chromosphere alone, but the chromosphere still plays a major observational role in many ways. It is the source of the bulk of a flare's radiant energy - in particular the visible/UV continuum radiation. It also provides tracers that guide us to the coronal source of the energy, even though we do not yet understand the propagation of the energy from its storage in the corona to its release in the chromosphere. The formation of chromospheric radiations during a flare presents several difficult and interesting physical problems.",2007,2007-05-23,https://arxiv.org/abs/0704.0823,1
0711.0350,Intermittent estimation of stationary time series,"Let $\{X_n\}_{n=0}^{\infty}$ be a stationary real-valued time series with unknown distribution. Our goal is to estimate the conditional expectation of $X_{n+1}$ based on the observations $X_i$, $0\le i\le n$ in a strongly consistent way. Bailey and Ryabko proved that this is not possible even for ergodic binary time series if one estimates at all values of $n$. We propose a very simple algorithm which will make prediction infinitely often at carefully selected stopping times chosen by our rule. We show that under certain conditions our procedure is strongly (pointwise) consistent, and $L_2$ consistent without any condition. An upper bound on the growth of the stopping times is also presented in this paper.",2007,2008-06-19,https://arxiv.org/abs/0711.0350,1
1108.3177,Detecting simultaneous variant intervals in aligned sequences,"Given a set of aligned sequences of independent noisy observations, we are concerned with detecting intervals where the mean values of the observations change simultaneously in a subset of the sequences. The intervals of changed means are typically short relative to the length of the sequences, the subset where the change occurs, the ""carriers,"" can be relatively small, and the sizes of the changes can vary from one sequence to another. This problem is motivated by the scientific problem of detecting inherited copy number variants in aligned DNA samples. We suggest a statistic based on the assumption that for any given interval of changed means there is a given fraction of samples that carry the change. We derive an analytic approximation for the false positive error probability of a scan, which is shown by simulations to be reasonably accurate. We show that the new method usually improves on methods that analyze a single sample at a time and on our earlier multi-sample method, which is most efficient when the carriers form a large fraction of the set of sequences. The proposed procedure is also shown to be robust with respect to the assumed fraction of carriers of the changes.",2011,2011-08-17,https://arxiv.org/abs/1108.3177,1
1810.10724,Generalized Beamspace Modulation Using Multiplexing: A Breakthrough in mmWave MIMO,"Spatial multiplexing (SMX) multiple-input multiple-output (MIMO) over the best beamspace was considered as the best solution for millimeter wave (mmWave) communications regarding spectral efficiency (SE), referred as the best beamspace selection (BBS) solution. The equivalent MIMO water-filling (WF-MIMO) channel capacity was treated as an unsurpassed SE upper bound. Recently, researchers have proposed various schemes trying to approach the benchmark and the performance bound. But, are they the real limit of mmWave MIMO systems with reduced radio-frequency (RF) chains? In this paper, we challenge the benchmark and the corresponding bound by proposing a better transmission scheme that achieves higher SE, namely the Generalized Beamspace Modulation using Multiplexing (GBMM). Inspired by the concept of spatial modulation, besides the selected beamspace, the selection operation is used to carry information. We prove that GBMM is superior to BBS in terms of SE and can break through the well known `upper bound'. That is, GBMM renews the upper bound of the SE. We investigate SE-oriented precoder activation probability optimization, fully-digital precoder design, optimal power allocation and hybrid precoder design for GBMM. A gradient ascent algorithm is developed to find the optimal solution, which is applicable in all signal-to-noise-ratio (SNR) regimes. The best solution is derived in the high SNR regime. Additionally, we investigate the hybrid receiver design and deduce the minimum number of receive RF chains configured to gain from GBMM in achievable SE. We propose a coding approach to realize the optimized precoder activation. An extension to mmWave broadband communications is also discussed. Comparisons with the benchmark (i.e., WF-MIMO channel capacity) are made under different system configurations to show the superiority of GBMM.",2018,2018-10-26,https://arxiv.org/abs/1810.10724,1
0705.4079,Molecular Clock on a Neutral Network,"The number of fixed mutations accumulated in an evolving population often displays a variance that is significantly larger than the mean (the overdispersed molecular clock). By examining a generic evolutionary process on a neutral network of high-fitness genotypes, we establish a formalism for computing all cumulants of the full probability distribution of accumulated mutations in terms of graph properties of the neutral network, and use the formalism to prove overdispersion of the molecular clock. We further show that significant overdispersion arises naturally in evolution when the neutral network is highly sparse, exhibits large global fluctuations in neutrality, and small local fluctuations in neutrality. The results are also relevant for elucidating the topological structure of a neutral network from empirical measurements of the substitution process.",2007,2009-11-13,https://arxiv.org/abs/0705.4079,1
1512.06290,On the Non-Asymptotic Properties of Regularized M-estimators,"We propose a general framework for regularization in M-estimation problems under time dependent (absolutely regular-mixing) data which encompasses many of the existing estimators. We derive non-asymptotic concentration bounds for the regularized M-estimator. Our results exhibit a variance-bias trade-off, with the variance term being governed by a novel measure of the complexity of the parameter set. We also show that the mixing structure affect the variance term by scaling the number of observations; depending on the decay rate of the mixing coefficients, this scaling can even affect the asymptotic behavior. Finally, we propose a data-driven method for choosing the tuning parameters of the regularized estimator which yield the same (up to constants) concentration bound as one that optimally balances the (squared) bias and variance terms. We illustrate the results with several canonical examples.",2015,2018-01-04,https://arxiv.org/abs/1512.06290,3
0705.0827,A Metric for Gradient RG Flow of the Worldsheet Sigma Model Beyond First Order,"Tseytlin has recently proposed that an action functional exists whose gradient generates to all orders in perturbation theory the Renormalization Group (RG) flow of the target space metric in the worldsheet sigma model. The gradient is defined with respect to a metric on the space of coupling constants which is explicitly known only to leading order in perturbation theory, but at that order is positive semi-definite, as follows from Perelman's work on the Ricci flow. This gives rise to a monotonicity formula for the flow which is expected to fail only if the beta function perturbation series fails to converge, which can happen if curvatures or their derivatives grow large. We test the validity of the monotonicity formula at next-to-leading order in perturbation theory by explicitly computing the second-order terms in the metric on the space of coupling constants. At this order, this metric is found not to be positive semi-definite. In situations where this might spoil monotonicity, derivatives of curvature become large enough for higher order perturbative corrections to be significant.",2007,2008-11-26,https://arxiv.org/abs/0705.0827,3
0705.1597,Weight 2 blocks of general linear groups and modular Alvis-Curtis duality,"We obtain the structure of weight 2 blocks and [2:1]-pairs of q-Schur algebras, and compute explicitly the modular Alvis-Curtis duality for weight 2 blocks of finite general linear groups in non-defining characteristic.",2007,2007-10-24,https://arxiv.org/abs/0705.1597,2
1903.08025,"Bayesian MIDAS Penalized Regressions: Estimation, Selection, and Prediction","We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian techniques for estimation and inference. In particular, to improve the prediction properties of the model and its sparse recovery ability, we consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. We establish good frequentist asymptotic properties of the posterior of the in-sample and out-of-sample prediction error, we recover the optimal posterior contraction rate, and we show optimality of the posterior predictive density. Simulations show that the proposed models have good selection and forecasting performance in small samples, even when the design matrix presents cross-correlation. When applied to forecasting U.S. GDP, our penalized regressions can outperform many strong competitors. Results suggest that financial variables may have some, although very limited, short-term predictive content.",2019,2020-06-12,https://arxiv.org/abs/1903.08025,3
0706.0484,"Motivation, Design, and Ubiquity: A Discussion of Research Ethics and Computer Science","Modern society is permeated with computers, and the software that controls them can have latent, long-term, and immediate effects that reach far beyond the actual users of these systems. This places researchers in Computer Science and Software Engineering in a critical position of influence and responsibility, more than any other field because computer systems are vital research tools for other disciplines. This essay presents several key ethical concerns and responsibilities relating to research in computing. The goal is to promote awareness and discussion of ethical issues among computer science researchers. A hypothetical case study is provided, along with questions for reflection and discussion.",2007,2007-06-05,https://arxiv.org/abs/0706.0484,1
1404.5203,Towards a Monotonicity-Compliant Price Index for the Art Market,"Notwithstanding almost forty years of efforts, the market for paintings still lacks a widely accepted price index. In this paper, we introduce a simple and intuitive metric to construct such index. Our metric is based on the price of a painting divided by its area. This formulation rests on a solid mathematical foundation as it corresponds to a particular type of hedonic model. However, unlike indexes based on the time-dummy coefficients of conventional hedonic models, this index satisfies the monotonicity condition. We demonstrate with a simple example the advantages of our metric. We also show the dangers of relying on the time-dummy coefficients of conventional hedonic models to estimate returns and generate price indexes.",2014,2014-04-22,https://arxiv.org/abs/1404.5203,1
1101.0917,Remembering Leo Breiman,"Leo Breiman was a highly creative, influential researcher with a down-to-earth personal style and an insistence on working on important real world problems and producing useful solutions. This paper is a short review of Breiman's extensive contributions to the field of applied statistics.",2011,2011-01-06,https://arxiv.org/abs/1101.0917,1
1812.11067,"Predicting ""Design Gaps"" in the Market: Deep Consumer Choice Models under Probabilistic Design Constraints","Predicting future successful designs and corresponding market opportunity is a fundamental goal of product design firms. There is accordingly a long history of quantitative approaches that aim to capture diverse consumer preferences, and then translate those preferences to corresponding ""design gaps"" in the market. We extend this work by developing a deep learning approach to predict design gaps in the market. These design gaps represent clusters of designs that do not yet exist, but are predicted to be both (1) highly preferred by consumers, and (2) feasible to build under engineering and manufacturing constraints. This approach is tested on the entire U.S. automotive market using of millions of real purchase data. We retroactively predict design gaps in the market, and compare predicted design gaps with actual known successful designs. Our preliminary results give evidence it may be possible to predict design gaps, suggesting this approach has promise for early identification of market opportunity.",2018,2018-12-31,https://arxiv.org/abs/1812.11067,1
1803.04607,A Perceptual Based Motion Compensation Technique for Video Coding,"Motion estimation is one of the important procedures in the all video encoders. Most of the complexity of the video coder depends on the complexity of the motion estimation step. The original motion estimation algorithm has a remarkable complexity and therefore many improvements were proposed to enhance the crude version of the motion estimation. The basic idea of many of these works were to optimize some distortion function for mean squared error (MSE) or sum of absolute difference (SAD) in block matching But it is shown that these metrics do not conclude the quality as it is, on the other hand, they are not compatible with the human visual system (HVS). In this paper we explored the usage of the image quality metrics in the video coding and more specific in the motion estimation. We have utilized the perceptual image quality metrics instead of MSE or SAD in the block based motion estimation. Three different metrics have used: structural similarity or SSIM, complex wavelet structural similarity or CW-SSIM, visual information fidelity or VIF. Experimental results showed that usage of the quality criterions can improve the compression rate while the quality remains fix and thus better quality in coded video at the same bit budget.",2018,2018-03-14,https://arxiv.org/abs/1803.04607,1
0705.1715,Holomorphic fiber bundle with Stein base and Stein fibers,"In this article, we prove that if $\Pi: X\to \Omega$ is a surjective holomorphic map, with $\Omega$ a Stein space and $X$ a complex manifold of dimension $n\geq 3,$ and if, for every $x\in \Omega$ there exists an open neighborhood $U$ such that $\Pi^{-1}(U)$ is Stein, then $X$ is Stein",2007,2007-05-23,https://arxiv.org/abs/0705.1715,1
1811.10045,"Generalized Dynamic Factor Models and Volatilities: Consistency, rates, and prediction intervals","Volatilities, in high-dimensional panels of economic time series with a dynamic factor structure on the levels or returns, typically also admit a dynamic factor decomposition. We consider a two-stage dynamic factor model method recovering the common and idiosyncratic components of both levels and log-volatilities. Specifically, in a first estimation step, we extract the common and idiosyncratic shocks for the levels, from which a log-volatility proxy is computed. In a second step, we estimate a dynamic factor model, which is equivalent to a multiplicative factor structure for volatilities, for the log-volatility panel. By exploiting this two-stage factor approach, we build one-step-ahead conditional prediction intervals for large $n \times T$ panels of returns. Those intervals are based on empirical quantiles, not on conditional variances; they can be either equal- or unequal- tailed. We provide uniform consistency and consistency rates results for the proposed estimators as both $n$ and $T$ tend to infinity. We study the finite-sample properties of our estimators by means of Monte Carlo simulations. Finally, we apply our methodology to a panel of asset returns belonging to the S&P100 index in order to compute one-step-ahead conditional prediction intervals for the period 2006-2013. A comparison with the componentwise GARCH benchmark (which does not take advantage of cross-sectional information) demonstrates the superiority of our approach, which is genuinely multivariate (and high-dimensional), nonparametric, and model-free.",2018,2022-02-03,https://arxiv.org/abs/1811.10045,2
0711.4262,Toward a quantitative analysis of virus and plasmid trafficking in cells,"Intracellular transport of DNA carriers is a fundamental step of gene delivery. We present here a theoretical approach to study generically a single virus or DNA particle trafficking in a cell cytoplasm. Cellular trafficking has been studied experimentally mostly at the macroscopic level, but very little has been done so far at the microscopic level. We present here a physical model to account for certain aspects of cellular organization, starting with the observation that a viral particle trajectory consists of epochs of pure diffusion and epochs of active transport along microtubules. We define a general degradation rate to describe the limitations of the delivery of plasmid or viral particles to the nucleus imposed by various types of direct and indirect hydrolysis activity inside the cytoplasm. Following a homogenization procedure, which consists of replacing the switching dynamics by a single steady state stochastic description, not only can we study the spatio-temporal dynamics of moving objects in the cytosol, but also estimate the probability and the mean time to go from the cell membrane to a nuclear pore. Computational simulations confirm that our model can be used to analyze and interpret viral trajectories and estimate quantitatively the success of nuclear delivery.",2007,2007-11-28,https://arxiv.org/abs/0711.4262,1
1009.5736,Kernel Bayes' rule,"A nonparametric kernel-based method for realizing Bayes' rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes' rule are presented, including Baysian computation without likelihood and filtering with a nonparametric state-space model.",2010,2011-09-29,https://arxiv.org/abs/1009.5736,4
0704.1721,An Exotic Approach to Hadron Physics,"An exotic approach to hadrons is discussed. It is based on the recently developed open-closed string duality explicitly conjectured as the AdS/CFT correspondence. Mesons as well as pentaquarks are studied in this approach. Spins are introduced as distribution functions over the string, and a second quantization method of string theory is examined and used to estimate the mass and decay width of various hadrons. This approach provides a way to understand the structure of flavor by a configuration of probe branes.",2007,2008-11-26,https://arxiv.org/abs/0704.1721,1
0709.0746,Geometric Complexity Theory: Introduction,"These are lectures notes for the introductory graduate courses on geometric complexity theory (GCT) in the computer science department, the university of Chicago. Part I consists of the lecture notes for the course given by the first author in the spring quarter, 2007. It gives introduction to the basic structure of GCT. Part II consists of the lecture notes for the course given by the second author in the spring quarter, 2003. It gives introduction to invariant theory with a view towards GCT. No background in algebraic geometry or representation theory is assumed. These lecture notes in conjunction with the article \cite{GCTflip1}, which describes in detail the basic plan of GCT based on the principle called the flip, should provide a high level picture of GCT assuming familiarity with only basic notions of algebra, such as groups, rings, fields etc.",2007,2014-08-02,https://arxiv.org/abs/0709.0746,1
1808.09375,Inference based on Kotlarski's Identity,"Kotlarski's identity has been widely used in applied economic research. However, how to conduct inference based on this popular identification approach has been an open question for two decades. This paper addresses this open problem by constructing a novel confidence band for the density function of a latent variable in repeated measurement error model. The confidence band builds on our finding that we can rewrite Kotlarski's identity as a system of linear moment restrictions. The confidence band controls the asymptotic size uniformly over a class of data generating processes, and it is consistent against all fixed alternatives. Simulation studies support our theoretical results.",2018,2019-09-10,https://arxiv.org/abs/1808.09375,3
0708.0187,Wide therapeutic time window for nimesulide neuroprotection in a model of transient focal cerebral ischemia in the rat,"Results from several studies indicate that cyclooxygenase-2 (COX-2) is involved in ischemic brain injury. The purpose of this study was to evaluate the neuroprotective effects of the selective COX-2 inhibitor nimesulide on cerebral infarction and neurological deficits in a standardized model of transient focal cerebral ischemia in rats. Three doses of nimesulide (3, 6 and 12 mg/kg; i.p.) or vehicle were administered immediately after stroke and additional doses were given at 6, 12, 24, 36 and 48 h after ischemia. In other set of experiments, the effect of nimesulide was studied in a situation in which its first administration was delayed for 3-24 h after ischemia. Total, cortical and subcortical infarct volumes and functional outcome (assessed by neurological deficit score and rotarod performance) were determined 3 days after ischemia. The effect of nimesulide on prostaglandin E(2) (PGE(2)) levels in the injured brain was also investigated. Nimesulide dose-dependently reduced infarct volume and improved functional recovery when compared to vehicle. Of interest is the finding that neuroprotection conferred by nimesulide (reduction of infarct size and neurological deficits and improvement of rotarod performance) was also observed when treatment was delayed until 24 h after ischemia. Further, administration of nimesulide in a delayed treatment paradigm completely abolished PGE(2) accumulation in the postischemic brain, suggesting that COX-2 inhibition is a promising therapeutic strategy for cerebral ischemia to target the late-occurring inflammatory events which amplify initial damage.",2007,2007-08-02,https://arxiv.org/abs/0708.0187,1
1812.07495,GPR-based Detection of Voids and Evaluation of Grouting Under Semi-rigid Basement,"The void underneath semi-rigid base is a common defect in roads. There are some difficulties in the detection and repair for this kind of hidden damage, as well as in the evaluation of the effects of grouting treatment. For the detection and maintenance of roads, it is essential to study the detection and judging for voids underneath base and the evaluation of the spread of grout. Through theoretical analysis, numerical simulation and analysis of real data, this research generated the characteristics of under-base voids of different types and dimensions on GPR images, proposed the detecting and dimension-measuring methods for under-base voids, and studied the process and effects of data analysis techniques. (1) The characteristics of under-base voids of different types (air-filled, water-filled or grout-treated) and dimensions (height and horizontal dimensions), on A-scan and B-scan GPR image respectively, were analyzed theoretically. (2) Approaches for detecting voids and for estimating its height were studied, focusing on voids with a height ranging from 0.01m to 0.3m. (3) The approach for estimating the horizontal dimension of voids was studied, focusing on voids with a length ranging from 0.04m to 0.52m. (4) The data processing process was discussed. Also, the effects of different data processing techniques were studied in terms of noise filtering and attenuation compensation, and their influence on the image characteristics was also discussed.",2018,2018-12-19,https://arxiv.org/abs/1812.07495,1
1809.03583,Benefits of Positioning-Aided Communication Technology in High-Frequency Industrial IoT,"The future of industrial applications is shaped by intelligent moving IoT devices, such as flying drones, advanced factory robots, and connected vehicles, which may operate (semi-)autonomously. In these challenging scenarios, dynamic radio connectivity at high frequencies -- augmented with timely positioning-related information -- becomes instrumental to improve communication performance and facilitate efficient computation offloading. Our work reviews the main research challenges and reveals open implementation gaps in Industrial IoT (IIoT) applications that rely on location awareness and multi-connectivity in super high and extremely high frequency bands. It further conducts a rigorous numerical investigation to confirm the potential of precise device localization in the emerging IIoT systems. We focus on positioning-aided benefits made available to multi-connectivity IIoT device operation at 28 GHz, which notably improve data transfer rates, communication latency, and extent of control overhead.",2018,2018-09-12,https://arxiv.org/abs/1809.03583,1
0704.0622,On the number of moduli of plane sextics with six cusps,"Let S be the variety of irreducible sextics with six cusps as singularities. Let W be one of irreducible components of W. Denoting by M_4 the space of moduli of smooth curves of genus 4, the moduli map of W is the rational map from W to M_4 sending the general point of W, corresponding to a plane curve D, to the point of M_4 parametrizing the normalization curve of D. The number of moduli of W is, by definition the dimension of the image of W with respect to the moduli map. We know that this number is at most equal to seven. In this paper we prove that both irreducible components of S have number of moduli exactly equal to seven.",2007,2007-05-23,https://arxiv.org/abs/0704.0622,1
1004.5060,STDP-driven networks and the \emph{C. elegans} neuronal network,"We study the dynamics of the structure of a formal neural network wherein the strengths of the synapses are governed by spike-timing-dependent plasticity (STDP). For properly chosen input signals, there exists a steady state with a residual network. We compare the motif profile of such a network with that of a real neural network of \emph{C. elegans} and identify robust qualitative similarities. In particular, our extensive numerical simulations show that this STDP-driven resulting network is robust under variations of the model parameters.",2010,2015-05-18,https://arxiv.org/abs/1004.5060,1
0709.4063,The Importance and Criticality of Spreadsheets in the City of London,"Spreadsheets have been with us in their present form for over a quarter of a century. We have become so used to them that we forget that we are using them at all. It may serve us well to stand back for a moment to review where, when and how we use spreadsheets in the financial markets and elsewhere in order to inform research that may guide their future development. In this article I bring together the experiences of a number of senior practitioners who have spent much of their careers working with large spreadsheets that have been and continue to be used to support major financial transactions and manage large institutions in the City of London. The author suggests that the City of London is presently exposed to significant reputational risk through the continued uncontrolled use of critical spreadsheets in the financial markets and elsewhere.",2007,2008-03-10,https://arxiv.org/abs/0709.4063,2
1607.00698,The Econometrics of Randomized Experiments,"In this review, we present econometric and statistical methods for analyzing randomized experiments. For basic experiments we stress randomization-based inference as opposed to sampling-based inference. In randomization-based inference, uncertainty in estimates arises naturally from the random assignment of the treatments, rather than from hypothesized sampling from a large population. We show how this perspective relates to regression analyses for randomized experiments. We discuss the analyses of stratified, paired, and clustered randomized experiments, and we stress the general efficiency gains from stratification. We also discuss complications in randomized experiments such as non-compliance. In the presence of non-compliance we contrast intention-to-treat analyses with instrumental variables analyses allowing for general treatment effect heterogeneity. We consider in detail estimation and inference for heterogeneous treatment effects in settings with (possibly many) covariates. These methods allow researchers to explore heterogeneity by identifying subpopulations with different treatment effects while maintaining the ability to construct valid confidence intervals. We also discuss optimal assignment to treatment based on covariates in such settings. Finally, we discuss estimation and inference in experiments in settings with interactions between units, both in general network settings and in settings where the population is partitioned into groups with all interactions contained within these groups.",2016,2017-10-26,https://arxiv.org/abs/1607.00698,1
0704.0155,A computer program for fast non-LTE analysis of interstellar line spectra,"The large quantity and high quality of modern radio and infrared line observations require efficient modeling techniques to infer physical and chemical parameters such as temperature, density, and molecular abundances. We present a computer program to calculate the intensities of atomic and molecular lines produced in a uniform medium, based on statistical equilibrium calculations involving collisional and radiative processes and including radiation from background sources. Optical depth effects are treated with an escape probability method. The program is available on the World Wide Web at http://www.sron.rug.nl/~vdtak/radex/index.shtml . The program makes use of molecular data files maintained in the Leiden Atomic and Molecular Database (LAMDA), which will continue to be improved and expanded. The performance of the program is compared with more approximate and with more sophisticated methods. An Appendix provides diagnostic plots to estimate physical parameters from line intensity ratios of commonly observed molecules. This program should form an important tool in analyzing observations from current and future radio and infrared telescopes.",2007,2015-05-13,https://arxiv.org/abs/0704.0155,1
0710.1676,Electric Transport Properties of the p53 Gene and the Effects of Point Mutations,"In this work, charge transport (CT) properties of the p53 gene are numerically studied by the transfer matrix method, and using either single or double strand effective tight-binding models. A statistical analysis of the consequences of known p53 point mutations on CT features is performed. It is found that in contrast to other kind of mutation defects, cancerous mutations result in much weaker changes of CT efficiency. Given the envisioned role played by CT in the DNA-repairing mechanism, our theoretical results suggest an underlying physical explanation at the origin of carcinogenesis.",2007,2007-10-10,https://arxiv.org/abs/0710.1676,1
0710.3170,Fast Intrinsic Mode Decomposition of Time Series Data with Sawtooth Transform,"An efficient method is introduced in this paper to find the intrinsic mode function (IMF) components of time series data. This method is faster and more predictable than the Empirical Mode Decomposition (EMD) method devised by the author of Hilbert Huang Transform (HHT). The approach is to transforms the original data function into a piecewise linear sawtooth function (or triangle wave function), then directly constructs the upper envelope by connecting the maxima and construct lower envelope by connecting minima with straight line segments in the sawtooth space, the IMF is calculated as the difference between the sawtooth function and the mean of the upper and lower envelopes. The results found in the sawtooth space are reversely transformed into the original data space as the required IMF and envelopes mean. This decomposition method process the data in one pass to obtain a unique IMF component without the time consuming repetitive sifting process of EMD method. An alternative decomposition method with sawtooth function expansion is also presented.",2007,2007-11-14,https://arxiv.org/abs/0710.3170,2
0901.3067,Enumeration and Online Library of Mass-Action Reaction Networks,"The aim of this work is to make available to the community a large collection of mass-action reaction networks of a given size for further research. The set is limited to what can be computed on a modern multi-core desktop in reasonable time (< 20 days). We have currently generated over 47 million unique reaction networks. All currently generated sets of networks are available and as new sets are completed they will also be made available. Also provided are programs for translating them into different formats, along with documentation and examples. Source code and binaries for all the programs are included. These can be downloaded from (http://www.sys-bio.org/networkenumeration). This library of networks will allow for thorough studies of the reaction network space. Additionally, these methods serve as an example for future work on enumerating other types of biological networks, such as genetic regulatory networks and mass-action networks that include regulation.",2009,2009-01-21,https://arxiv.org/abs/0901.3067,1
0704.1903,Growth window and possible mechanism of millimeter-thick single-walled carbon nanotube forests,"Our group recently reproduced the water-assisted growth method, so-called ""super growth"", of millimeter-thick single-walled carbon nanotube (SWNT) forests by using C2H4/ H2/ H2O/ Ar reactant gas and Fe/ Al2O3 catalyst. In this current work, a parametric study was carried out on both reaction and catalyst conditions. Results revealed that a thin Fe catalyst layer (about 0.5 nm) yielded rapid growth of SWNTs only when supported on Al2O3, and that Al2O3 support enhanced the activity of Fe, Co, and Ni catalysts. The growth window for the rapid SWNT growth was narrow, however. Optimum amount of added H2O increased the SWNT growth rate but further addition of H2O degraded both the SWNT growth rate and quality. Addition of H2 was also essential for rapid SWNT growth, but again, further addition decreased both the SWNT growth rate and quality. Because Al2O3 catalyzes hydrocarbon reforming, Al2O3 support possibly enhances the SWNT growth rate by supplying the carbon source to the catalyst nanoparticles. The origin of the narrow window for rapid SWNT growth will also be discussed.",2007,2007-05-23,https://arxiv.org/abs/0704.1903,1
0704.0467,Modeling Accretion Disk X-ray Continuum of Black Hole Candidates,"We critically examine issues associated with determining the fundamental properties of the black hole and the surrounding accretion disk in an X-ray binary based on modeling the disk X-ray continuum of the source. We base our work mainly on two XMM-Newton observations of GX 339-4, because they provided high-quality data at low energies (below 1 keV) which are critical for reliably modeling the spectrum of the accretion disk. A key issue examined is the determination of the so-called ""color correction factor"", which is often empirically introduced to account for the deviation of the local disk spectrum from a blackbody (due to electron scattering). This factor cannot be pre-determined theoretically because it may vary with, e.g., mass accretion rate, among a number of important factors. We follow up on an earlier suggestion to estimate the color correction observationally by modeling the disk spectrum with saturated Compton scattering. We show that the spectra can be fitted well and the approach yields reasonable values for the color correction factor. For comparison, we have also attempted to fit the spectra with other models. We show that even the high-soft-state continuum (which is dominated by the disk emission) cannot be satisfactorily fitted by state-of-the-art disk models. We discuss the implication of the results.",2007,2009-09-29,https://arxiv.org/abs/0704.0467,3
0704.1619,Proper Motion Dispersions of Red Clump Giants in the Galactic Bulge: Observations and Model Comparisons,"Red clump giants in the Galactic bulge are approximate standard candles and hence they can be used as distance indicators. We compute the proper motion dispersions of RCG stars in the Galactic bulge using the proper motion catalogue from the second phase of the Optical Gravitational Microlensing Experiment (OGLE-II, Sumi et al. 2004) for 45 fields. The proper motion dispersions are measured to a few per cent accuracy due to the large number of stars in the fields. The observational sample is comprised of 577736 stars. These observed data are compared to a state-of-the-art particle simulation of the Galactic bulge region. The predictions are in rough agreement with observations, but appear to be too anisotropic in the velocity ellipsoid. We note that there is significant field-to-field variation in the observed proper motion dispersions. This could either be a real feature, or due to some unknown systematic effect.",2007,2009-06-23,https://arxiv.org/abs/0704.1619,1
0909.5125,Resonant activation: a strategy against bacterial persistence,"A bacterial colony may develop a small number of cells genetically identical to, but phenotypically different from other normally growing bacteria. These so-called persister cells keep themselves in a dormant state and thus are insensitive to antibiotic treatment, resulting in serious problems of drug resistance. In this paper, we proposed a novel strategy to ""kill"" persister cells by triggering them to switch, in a fast and synchronized way, into normally growing cells that are susceptible to antibiotics. The strategy is based on resonant activation (RA), a well-studied phenomenon in physics where the internal noise of a system can constructively facilitate fast and synchronized barrier crossings. Through stochastic Gilliespie simulation with a generic toggle switch model, we demonstrated that RA exists in the phenotypic switching of a single bacterium. Further, by coupling single cell level and population level simulations, we showed that with RA, one can greatly reduce the time and total amount of antibiotics needed to sterilize a bacterial population. We suggest that resonant activation is a general phenomenon in phenotypic transition, and can find other applications such as cancer therapy.",2009,2010-02-11,https://arxiv.org/abs/0909.5125,2
1005.0892,Extracting abundance indices from longline surveys : method to account for hook competition and unbaited hooks,"The most commonly used relative abundance index in stock assessments of longline fisheries is catch per unit effort (CPUE), here defined as the number of fish of the targeted species caught per hook and minute of soak time. Longline CPUE can be affected by interspecific competition and the retrieval of unbaited or empty hooks, and interannual variation in these can lead to biases in the apparent abundance trends in the CPUE. Interspecific competition on longlines has been previously studied but the return of empty hooks is ignored in all current treatments of longline CPUE. In this work we propose some different methods to build indices to address the interspecific competition that relates to empty hooks. We show that in the absence of information about empty hooks, the relative abundance estimates have constant biases with respect to fish density and this is typically not problematic for stock assessment. The simple CPUE index behaves poorly in every scenario. Understanding the reasons for empty hooks allows selection of the appropriate index. A scientific longline survey is conducted every two years in the Strait of Georgia, British Columbia by Fisheries and Oceans Canada. The above methods are applied to build the time-series of indices from 2003 to 2009 for quillback rockfish (Sebastes maliger). Due to variation in the incidence of non-target species, the index trend obtained is moderately sensitive to the choice of the estimator.",2010,2015-03-17,https://arxiv.org/abs/1005.0892,3
0704.0391,Exactly solvable spin dynamics of an electron coupled to large number of nuclei and the electron-nuclear spin echo in a quantum dot,"The model considered in the paper is used nowadays to describe spin dynamics of quantum dots after optical excitation. Based on the exact diagonalization of a model Hamiltonian, we solve the problems of the electron spin polarization decay and magnetic field dependence of the steady state polarization. The important role of the nuclear state is shown and methods of its calculation for different regimes of optical excitation are proposed. The effect of spin echo observed after application of the magnetic field $\pi$-pulse is predicted.",2007,2009-11-13,https://arxiv.org/abs/0704.0391,3
1001.2136,An alternative marginal likelihood estimator for phylogenetic models,"Bayesian phylogenetic methods are generating noticeable enthusiasm in the field of molecular systematics. Many phylogenetic models are often at stake and different approaches are used to compare them within a Bayesian framework. The Bayes factor, defined as the ratio of the marginal likelihoods of two competing models, plays a key role in Bayesian model selection. We focus on an alternative estimator of the marginal likelihood whose computation is still a challenging problem. Several computational solutions have been proposed none of which can be considered outperforming the others simultaneously in terms of simplicity of implementation, computational burden and precision of the estimates. Practitioners and researchers, often led by available software, have privileged so far the simplicity of the harmonic mean estimator (HM) and the arithmetic mean estimator (AM). However it is known that the resulting estimates of the Bayesian evidence in favor of one model are biased and often inaccurate up to having an infinite variance so that the reliability of the corresponding conclusions is doubtful. Our new implementation of the generalized harmonic mean (GHM) idea recycles MCMC simulations from the posterior, shares the computational simplicity of the original HM estimator, but, unlike it, overcomes the infinite variance issue. The alternative estimator is applied to simulated phylogenetic data and produces fully satisfactory results outperforming those simple estimators currently provided by most of the publicly available software.",2010,2010-06-22,https://arxiv.org/abs/1001.2136,2
0704.0196,Remarks on N_c dependence of decays of exotic baryons,We calculate the N_c dependence of the decay widths of exotic eikosiheptaplet within the framework of Chral Quark Soliton Model. We also discuss generalizations of regular baryon representations for arbitrary N_c.,2007,2008-11-26,https://arxiv.org/abs/0704.0196,2
0704.3931,The Complexity of Model Checking Higher-Order Fixpoint Logic,"Higher-Order Fixpoint Logic (HFL) is a hybrid of the simply typed \lambda-calculus and the modal \lambda-calculus. This makes it a highly expressive temporal logic that is capable of expressing various interesting correctness properties of programs that are not expressible in the modal \lambda-calculus. This paper provides complexity results for its model checking problem. In particular we consider those fragments of HFL built by using only types of bounded order k and arity m. We establish k-fold exponential time completeness for model checking each such fragment. For the upper bound we use fixpoint elimination to obtain reachability games that are singly-exponential in the size of the formula and k-fold exponential in the size of the underlying transition system. These games can be solved in deterministic linear time. As a simple consequence, we obtain an exponential time upper bound on the expression complexity of each such fragment. The lower bound is established by a reduction from the word problem for alternating (k-1)-fold exponential space bounded Turing Machines. Since there are fixed machines of that type whose word problems are already hard with respect to k-fold exponential time, we obtain, as a corollary, k-fold exponential time completeness for the data complexity of our fragments of HFL, provided m exceeds 3. This also yields a hierarchy result in expressive power.",2007,2015-07-01,https://arxiv.org/abs/0704.3931,2
1104.2210,From EM to Data Augmentation: The Emergence of MCMC Bayesian Computation in the 1980s,"It was known from Metropolis et al. [J. Chem. Phys. 21 (1953) 1087--1092] that one can sample from a distribution by performing Monte Carlo simulation from a Markov chain whose equilibrium distribution is equal to the target distribution. However, it took several decades before the statistical community embraced Markov chain Monte Carlo (MCMC) as a general computational tool in Bayesian inference. The usual reasons that are advanced to explain why statisticians were slow to catch on to the method include lack of computing power and unfamiliarity with the early dynamic Monte Carlo papers in the statistical physics literature. We argue that there was a deeper reason, namely, that the structure of problems in the statistical mechanics and those in the standard statistical literature are different. To make the methods usable in standard Bayesian problems, one had to exploit the power that comes from the introduction of judiciously chosen auxiliary variables and collective moves. This paper examines the development in the critical period 1980--1990, when the ideas of Markov chain simulation from the statistical physics literature and the latent variable formulation in maximum likelihood computation (i.e., EM algorithm) came together to spark the widespread application of MCMC methods in Bayesian computation.",2011,2011-04-13,https://arxiv.org/abs/1104.2210,1
0706.3985,Distributions associated with general runs and patterns in hidden Markov models,"This paper gives a method for computing distributions associated with patterns in the state sequence of a hidden Markov model, conditional on observing all or part of the observation sequence. Probabilities are computed for very general classes of patterns (competing patterns and generalized later patterns), and thus, the theory includes as special cases results for a large class of problems that have wide application. The unobserved state sequence is assumed to be Markovian with a general order of dependence. An auxiliary Markov chain is associated with the state sequence and is used to simplify the computations. Two examples are given to illustrate the use of the methodology. Whereas the first application is more to illustrate the basic steps in applying the theory, the second is a more detailed application to DNA sequences, and shows that the methods can be adapted to include restrictions related to biological knowledge.",2007,2007-12-18,https://arxiv.org/abs/0706.3985,2
1808.10743,Full-Duplex Energy-Harvesting Enabled Relay Networks in Generalized Fading Channels,"This paper analyzes the performance of a full-duplex decode-and-forward relaying network over the generalized \kappa-\mu fading channel. The relay is energy-constrained and relies entirely on harvesting the power signal transmitted by the source based on the time-switching relaying protocol. A unified analytical expression for the ergodic outage probability is derived for the system under consideration. This is then used to derive closed-form analytical expressions for three special cases of the \kappa-\mu fading model, namely, Nakagami-m, Rice and Rayleigh. Monte Carlo simulations are provided throughout to verify the correctness of our analysis.",2018,2018-09-03,https://arxiv.org/abs/1808.10743,1
1708.06443,Bias Reduction in Instrumental Variable Estimation through First-Stage Shrinkage,"The two-stage least-squares (2SLS) estimator is known to be biased when its first-stage fit is poor. I show that better first-stage prediction can alleviate this bias. In a two-stage linear regression model with Normal noise, I consider shrinkage in the estimation of the first-stage instrumental variable coefficients. For at least four instrumental variables and a single endogenous regressor, I establish that the standard 2SLS estimator is dominated with respect to bias. The dominating IV estimator applies James-Stein type shrinkage in a first-stage high-dimensional Normal-means problem followed by a control-function approach in the second stage. It preserves invariances of the structural instrumental variable equations.",2017,2017-11-01,https://arxiv.org/abs/1708.06443,2
0705.2309,Stability of associated primes of monomial ideals,Let $I$ be a monomial ideal of a polynomial ring $R$. In this paper we determine a number $B$ such that $\Ass (I^n/I^{n+1}) = \Ass (I^{B}/I^{B+1})$ for all $n\geq B$.,2007,2007-05-23,https://arxiv.org/abs/0705.2309,1
0711.1273,Practical Resource Allocation Algorithms for QoS in OFDMA-based Wireless Systems,"In this work we propose an efficient resource allocation algorithm for OFDMA based wireless systems supporting heterogeneous traffic. The proposed algorithm provides proportionally fairness to data users and short term rate guarantees to real-time users. Based on the QoS requirements, buffer occupancy and channel conditions, we propose a scheme for rate requirement determination for delay constrained sessions. Then we formulate and solve the proportional fair rate allocation problem subject to those rate requirements and power/bandwidth constraints. Simulations results show that the proposed algorithm provides significant improvement with respect to the benchmark algorithm.",2007,2016-11-18,https://arxiv.org/abs/0711.1273,1
0801.1560,On differences between fractional and integer order differential equations for dynamical games,We argue that fractional order (FO) differential equations are more suitable to model complex adaptive systems (CAS). Hence they are applied in replicator equations for non-cooperative game. Rock-Scissors-Paper game is discussed. It is known that its integer order model does not have a stable equilibrium. Its fractional order model is shown to have a locally asymptotically stable internal solution. A FO asymmetric game is shown to have a locally asymptotically stable internal solution. This is not the case for its integer order counterpart.,2008,2015-05-13,https://arxiv.org/abs/0801.1560,1
1901.05645,Relational Communication,"We study a communication game between an informed sender and an uninformed receiver with repeated interactions and voluntary transfers. Transfers motivate the receiver's decision-making and signal the sender's information. Although full separation can always be supported in equilibrium, partial or complete pooling is optimal if the receiver's decision-making is highly responsive to information. In this case, the receiver's decision-making is disciplined by pooling extreme states where she is most tempted to defect.",2019,2020-12-11,https://arxiv.org/abs/1901.05645,3
0808.0593,"Comment: Microarrays, Empirical Bayes and the Two-Group Model","Comment on ``Microarrays, Empirical Bayes and the Two-Group Model'' [arXiv:0808.0572]",2008,2008-08-06,https://arxiv.org/abs/0808.0593,1
0704.1672,Two center multipole expansion method: application to macromolecular systems,"We propose a new theoretical method for the calculation of the interaction energy between macromolecular systems at large distances. The method provides a linear scaling of the computing time with the system size and is considered as an alternative to the well known fast multipole method. Its efficiency, accuracy and applicability to macromolecular systems is analyzed and discussed in detail.",2007,2009-11-13,https://arxiv.org/abs/0704.1672,1
0704.3640,Linked by Loops: Network Structure and Switch Integration in Complex Dynamical Systems,"Simple nonlinear dynamical systems with multiple stable stationary states are often taken as models for switchlike biological systems. This paper considers the interaction of multiple such simple multistable systems when they are embedded together into a larger dynamical ""supersystem."" Attention is focused on the network structure of the resulting set of coupled differential equations, and the consequences of this structure on the propensity of the embedded switches to act independently versus cooperatively. Specifically, it is argued that both larger average and larger variance of the node degree distribution lead to increased switch independence. Given the frequency of empirical observations of high variance degree distributions (e.g., power-law) in biological networks, it is suggested that the results presented here may aid in identifying switch-integrating subnetworks as comparatively homogenous, low-degree, substructures. Potential applications to ecological problems such as the relationship of stability and complexity are also briefly discussed.",2007,2008-04-10,https://arxiv.org/abs/0704.3640,4
0704.1234,Generalization of Einstein-Lovelock theory to higher order dilaton gravity,"A higher order theory of dilaton gravity is constructed as a generalization of the Einstein-Lovelock theory of pure gravity. Its Lagrangian contains terms with higher powers of the Riemann tensor and of the first two derivatives of the dilaton. Nevertheless, the resulting equations of motion are quasi-linear in the second derivatives of the metric and of the dilaton. This property is crucial for the existence of brane solutions in the thin wall limit. At each order in derivatives the contribution to the Lagrangian is unique up to an overall normalization. Relations between symmetries of this theory and the O(d,d) symmetry of the string-inspired models are discussed.",2007,2008-11-26,https://arxiv.org/abs/0704.1234,2
1810.04123,Computer-Aided Arrhythmia Diagnosis by Learning ECG Signal,"Electrocardiogram (ECG) is one of the non-invasive and low-risk methods to monitor the condition of the human heart. Any abnormal pattern(s) in the ECG signal is an indicative measure of malfunctioning of the heart, termed as arrhythmia. Due to the lack of human expertise and high probability to misdiagnose, computer-aided diagnosis and analysis are preferred. In this work, we perform arrhythmia detection with an optimized neural network having piecewise linear approximation based activation function to alleviate the complex computations in the traditional activation functions. Further, we propose a self-learning method for arrhythmia detection by learning and analyzing the characteristics (period) of the ECG signal. Self-learning based approach achieves 97.28% of arrhythmia detection accuracy, and neural network with optimized activation functions achieve an arrhythmia detection accuracy of 99.56%.",2018,2018-10-10,https://arxiv.org/abs/1810.04123,1
0806.2099,Metabolic scaling law for fetus and placenta,We present a version of Kleiber's scaling law for fetus and placenta.,2008,2008-06-13,https://arxiv.org/abs/0806.2099,1
1105.2150,Matrix Variate Logistic Regression Model with Application to EEG Data,"Logistic regression has been widely applied in the field of biomedical research for a long time. In some applications, covariates of interest have a natural structure, such as being a matrix, at the time of collection. The rows and columns of the covariate matrix then have certain physical meanings, and they must contain useful information regarding the response. If we simply stack the covariate matrix as a vector and fit the conventional logistic regression model, relevant information can be lost, and the problem of inefficiency will arise. Motivated from these reasons, we propose in this paper the matrix variate logistic (MV-logistic) regression model. Advantages of MV-logistic regression model include the preservation of the inherent matrix structure of covariates and the parsimony of parameters needed. In the EEG Database Data Set, we successfully extract the structural effects of covariate matrix, and a high classification accuracy is achieved.",2011,2011-12-02,https://arxiv.org/abs/1105.2150,2
1802.06250,First-order bifurcation detection for dynamic complex networks,"In this paper, we explore how network centrality and network entropy can be used to identify a bifurcation network event. A bifurcation often occurs when a network undergoes a qualitative change in its structure as a response to internal changes or external signals. In this paper, we show that network centrality allows us to capture important topological properties of dynamic networks. By extracting multiple centrality features from a network for dimensionality reduction, we are able to track the network dynamics underlying an intrinsic low-dimensional manifold. Moreover, we employ von Neumann graph entropy (VNGE) to measure the information divergence between networks over time. In particular, we propose an asymptotically consistent estimator of VNGE so that the cubic complexity of VNGE is reduced to quadratic complexity that scales more gracefully with network size. Finally, the effectiveness of our approaches is demonstrated through a real-life application of cyber intrusion detection.",2018,2018-02-20,https://arxiv.org/abs/1802.06250,1
1205.1861,Carbon-dioxide emissions trading and hierarchical structure in worldwide finance and commodities markets,"In a highly interdependent economic world, the nature of relationships between financial entities is becoming an increasingly important area of study. Recently, many studies have shown the usefulness of minimal spanning trees (MST) in extracting interactions between financial entities. Here, we propose a modified MST network whose metric distance is defined in terms of cross-correlation coefficient absolute values, enabling the connections between anticorrelated entities to manifest properly. We investigate 69 daily time series, comprising three types of financial assets: 28 stock market indicators, 21 currency futures, and 20 commodity futures. We show that though the resulting MST network evolves over time, the financial assets of similar type tend to have connections which are stable over time. In addition, we find a characteristic time lag between the volatility time series of the stock market indicators and those of the EU CO2 emission allowance (EUA) and crude oil futures (WTI). This time lag is given by the peak of the cross-correlation function of the volatility time series EUA (or WTI) with that of the stock market indicators, and is markedly different (>20 days) from 0, showing that the volatility of stock market indicators today can predict the volatility of EU emissions allowances and of crude oil in the near future.",2012,2013-08-19,https://arxiv.org/abs/1205.1861,2
0806.1267,Rapid divergence of the ecdysone receptor in Diptera and Lepidoptera suggests coevolution between ECR and USP-RXR,"Ecdysteroid hormones are major regulators in reproduction and development of insects, including larval molts and metamorphosis. The functional ecdysone receptor is a heterodimer of ECR (NR1H1) and USP-RXR (NR2B4), which is the orthologue of vertebrate retinoid X receptors (RXR alpha, beta, gamma). Both proteins belong to the superfamily of nuclear hormone receptors, ligand-dependent transcription factors that share two conserved domains: the DNA-binding domain (DBD) and the ligand-binding domain (LBD). In order to gain further insight into the evolution of metamorphosis and gene regulation by ecdysone in arthropods, we performed a phylogenetic analysis of both partners of the heterodimer ECR/USP-RXR. Overall, 38 USP-RXR and 19 ECR protein sequences, from 33 species, have been used for this analysis. Interestingly, sequence alignments and structural comparisons reveal high divergence rates, for both ECR and USP-RXR, specifically among Diptera and Lepidoptera. The most impressive differences affect the ligand-binding domain of USP-RXR. In addition, ECR sequences show variability in other domains, namely the DNA-binding and the carboxy-terminal F domains. Our data provide the first evidence that ECR and USP-RXR may have coevolved during holometabolous insect diversification, leading to a functional divergence of the ecdysone receptor. These results have general implications on fundamental aspects of insect development, evolution of nuclear receptors, and the design of specific insecticides.",2008,2008-12-18,https://arxiv.org/abs/0806.1267,1
0705.0207,Chiral Equivariant Cohomology III,"This is the third of a series of papers on a new equivariant cohomology that takes values in a vertex algebra, and contains and generalizes the classical equivariant cohomology of a manifold with a Lie group action a la H. Cartan. In this paper, we compute this cohomology for spheres and show that for any simple connected group G, there is a sphere with infinitely many actions of G which have distinct chiral equivariant cohomology, but identical classical equivariant cohomology. Unlike the classical case, the description of the chiral equivariant cohomology of spheres requires a substantial amount of new structural theory, which we fully develop in this paper. This includes a quasi-conformal structure, equivariant homotopy invariance, and the values of this cohomology on homogeneous spaces. These results rely on crucial features of the underlying vertex algebra valued complex that have no classical analogues.",2007,2021-05-21,https://arxiv.org/abs/0705.0207,4
0704.1558,Chromospheric Cloud-Model Inversion Techniques,"Spectral inversion techniques based on the cloud model are extremely useful for the study of properties and dynamics of various chromospheric cloud-like structures. Several inversion techniques are reviewed based on simple (constant source function) and more elaborated cloud models, as well as on grids of synthetic line profiles produced for a wide range of physical parameters by different NLTE codes. Several examples are shown of how such techniques can be used in different chromospheric lines, for the study of structures of the quiet chromosphere, such as mottles/spicules, as well as for active region structures such as fibrils, arch filament systems (AFS), filaments and flares.",2007,2007-05-23,https://arxiv.org/abs/0704.1558,1
0708.0607,Real-time control and monitoring system for LIPI's Public Cluster,"We have developed a monitoring and control system for LIPI's Public Cluster. The system consists of microcontrollers and full web-based user interfaces for daily operation. It is argued that, due to its special natures, the cluster requires fully dedicated and self developed control and monitoring system. We discuss the implementation of using parallel port and dedicated micro-controller for this purpose. We also show that integrating such systems enables an autonomous control system based on the real time monitoring, for instance an autonomous power supply control based on the actual temperature, etc.",2007,2007-08-07,https://arxiv.org/abs/0708.0607,1
0711.1141,Transcriptional pulsing and consequent stochasticity in gene expression,"Transcriptional pulsing has been observed in both prokaryotes and eukaryotes and plays a crucial role in cell to cell variability of protein and mRNA numbers. The issue is how the time constants associated with episodes of transcriptional bursting impact cellular mRNA and protein distributions and reciprocally, to what extent experimentally observed distributions can be attributed to transcriptional pulsing. We address these questions by investigating the exact time-dependent solution of the Master equation for a transcriptional pulsing model of mRNA distributions. We find a plethora of results: we show that, among others, bimodal and long-tailed (power law) distributions occur in the steady state as the rate constants are varied over biologically significant time scales. Since steady state distributions may not be reached experimentally we present results for the time evolution of the distributions. Because cellular behavior is essentially determined by proteins, we investigate the effect of the different mRNA distributions on the corresponding protein distributions. We delineate the regimes of rate constants for which the protein distribution mimics the mRNA distribution and those for which the protein distribution deviates significantly from the mRNA distribution.",2007,2009-09-29,https://arxiv.org/abs/0711.1141,1
0710.1208,Diagrammatic Inference,"Diagrammatic logics were introduced in 2002, with emphasis on the notions of specifications and models. In this paper we improve the description of the inference process, which is seen as a Yoneda functor on a bicategory of fractions. A diagrammatic logic is defined from a morphism of limit sketches (called a propagator) which gives rise to an adjunction, which in turn determines a bicategory of fractions. The propagator, the adjunction and the bicategory provide respectively the syntax, the models and the inference process for the logic. Then diagrammatic logics and their morphisms are applied to the semantics of side effects in computer languages.",2007,2009-11-20,https://arxiv.org/abs/0710.1208,2
1903.01690,ppmlhdfe: Fast Poisson Estimation with High-Dimensional Fixed Effects,"In this paper we present ppmlhdfe, a new Stata command for estimation of (pseudo) Poisson regression models with multiple high-dimensional fixed effects (HDFE). Estimation is implemented using a modified version of the iteratively reweighted least-squares (IRLS) algorithm that allows for fast estimation in the presence of HDFE. Because the code is built around the reghdfe package, it has similar syntax, supports many of the same functionalities, and benefits from reghdfe's fast convergence properties for computing high-dimensional least squares problems. Performance is further enhanced by some new techniques we introduce for accelerating HDFE-IRLS estimation specifically. ppmlhdfe also implements a novel and more robust approach to check for the existence of (pseudo) maximum likelihood estimates.",2019,2022-07-26,https://arxiv.org/abs/1903.01690,3
0704.0839,Moduli spaces of rational tropical curves,This note is devoted to the definition of moduli spaces of rational tropical curves with n marked points. We show that this space has a structure of a smooth tropical variety of dimension n-3. We define the Deligne-Mumford compactification of this space and tropical $\psi$-class divisors.,2007,2007-05-23,https://arxiv.org/abs/0704.0839,1
0707.0498,The Role of Time in the Creation of Knowledge,"This paper I assume that in humans the creation of knowledge depends on a discrete time, or stage, sequential decision-making process subjected to a stochastic, information transmitting environment. For each time-stage, this environment randomly transmits Shannon type information-packets to the decision-maker, who examines each of them for relevancy and then determines his optimal choices. Using this set of relevant information-packets, the decision-maker adapts, over time, to the stochastic nature of his environment, and optimizes the subjective expected rate-of-growth of knowledge. The decision-maker's optimal actions, lead to a decision function that involves, over time, his view of the subjective entropy of the environmental process and other important parameters at each time-stage of the process. Using this model of human behavior, one could create psychometric experiments using computer simulation and real decision-makers, to play programmed games to measure the resulting human performance.",2007,2007-07-13,https://arxiv.org/abs/0707.0498,1
1904.08136,"A Pyramid Scheme Model Based on ""Consumption Rebate"" Frauds","There are various types of pyramid schemes which have inflicted or are inflicting losses on many people in the world. We propose a pyramid scheme model which has the principal characters of many pyramid schemes appeared in recent years: promising high returns, rewarding the participants recruiting the next generation of participants, and the organizer will take all the money away when he finds the money from the new participants is not enough to pay the previous participants interest and rewards. We assume the pyramid scheme carries on in the tree network, ER random network, SW small-world network or BA scale-free network respectively, then give the analytical results of how many generations the pyramid scheme can last in these cases. We also use our model to analyse a pyramid scheme in the real world and we find the connections between participants in the pyramid scheme may constitute a SW small-world network.",2019,2021-02-05,https://arxiv.org/abs/1904.08136,3
1204.6590,The monetary growth order,"Growth of monetary assets and debts is commonly described by the formula of compound interest which for the case of continuous compounding is the exponential growth law. Its differential form is dc/dt = i c where dc/dt describes the rate of monetary growth, i the compounded interest rate and c the actual principal. Exponential growth of this type is fixed to be neither resource-limited nor self-limiting which is in contrast to real economic growth (such as the GDP) which may have exponential, but also subexponential, linear, saturation, and even decline phases. As a result assets and debts commonly outgrow their economic fundament giving rise to the financial equivalent of Malthusian catastrophes after a certain interval of time. We here introduce an alternative for exponential compounding and propose to replace dc/dt = i c by dc/dt = i c^p where the exponent p (called reaction order in chemistry) is a quantity which will be termed monetary growth order. The monetary growth order p is seen as a tuning handle which enables to adjust gross monetary growth to real economic growth. It is suggested that the central banks take a serious look to this control instrument which allows tuning in crisis situations and immediate return to the exponential norm if needed.",2012,2012-05-01,https://arxiv.org/abs/1204.6590,1
0704.3696,A weighted graph problem from commutative algebra,"We give an especially simple proof of a theorem in graph theory that forms the key part of the solution to a problem in commutative algebra, on how to characterize the integral closure of a polynomial ring generated by quadratic monomials.",2007,2011-06-09,https://arxiv.org/abs/0704.3696,3
0704.1395,Higgs and Z' Phenomenology in B-L extension of the Standard Model at LHC,"The phenomenology of the low scale U(1)_{B-L} extension of the standard model and its implications at LHC is presented. In this model, an extra gauge boson corresponding to B-L gauge symmetry and an extra SM singlet scalar (heavy Higgs) are predicted. We show a detailed analysis of both heavy and light Higgses decay and production in addition to the possible decay channels of the new gauge boson. We find that the cross sections of the SM-like Higgs production are reduced by ~ 20%-30%, while its decay branching ratios remain intact. The extra Higgs has relatively small cross sections and the branching ratios of Z'-> l^+ l^- are of order ~20% compared to ~ 3% of the SM resuls. Hence, the search for Z' is accessible via a clean dilepton signal at LHC.",2007,2010-11-03,https://arxiv.org/abs/0704.1395,1
1811.07637,Mismatch error correction for time interleaved analog-to-digital converter over a wide frequency range,"High-speed high-resolution Analog-to-Digital Conversion is the key part for waveform digitization in physics experiments and many other domains. This paper presents a new fully digital correction of mismatch errors among the channels in Time Interleaved Analog-to-Digital Converter (TIADC) systems. We focus on correction with wide-band input signal, which means that we can correct the mismatch errors for any frequency point in a broad band with only one set of filter coefficients. Studies were also made to show how to apply the correction algorithm beyond the base band, i.e. other Nyquist zones in the under-sampling situation. Structure of the correction algorithm is presented in this paper, as well as simulation results. To evaluate the correction performance, we actually conducted a series of tests with two TIADC systems. The results indicate that the performance of both two TIADC systems can be greatly improved by correction, and the Effective Number Of Bits (ENOB) is successfully improved to be better than 9.5 bits and 5.5 bits for an input signal up to the bandwidth (-3dB) range in the 1.6-Gsps 14-bit and the 10-Gsps 8-bit TIADC systems, respectively. Tests were also conducted for input signal frequencies in the second Nyquist zone, which shows that the correction algorithms also work well as expected.",2018,2018-11-20,https://arxiv.org/abs/1811.07637,1
0704.2370,Effect of flux-dependent Friedel oscillations upon the effective transmission of an interacting nano-system,"We consider a nano-system connected to measurement probes via non interacting leads. When the electrons interact inside the nano-system, the coefficient |ts(E_F)|^2 describing its effective transmission at the Fermi energy E_F ceases to be local. This effect of electron-electron interactions upon |ts(E_F)|^2 is studied using a one dimensional model of spinless fermions and the Hartree-Fock approximation. The non locality of |ts(E_F)|^2 is due to the coupling between the Hartree and Fock corrections inside the nano-system and the scatterers outside the nano-system via long range Friedel oscillations. Using this phenomenon, one can vary |ts(E_F)|^2 by an Aharonov-Bohm flux threading a ring which is attached to one lead at a distance Lc from the nano-system. For small distances Lc, the variation of the quantum conductance induced by this non local effect can exceed 0.1 (e^2/h).",2007,2011-11-09,https://arxiv.org/abs/0704.2370,1
1905.09552,Technological Learning and Innovation Gestation Lags at the Frontier of Science: from CERN Procurement to Patent,"This paper contributes to the literature on the impact of Big Science Centres on technological innovation. We exploit a unique dataset with information on CERN's procurement orders to study the collaborative innovation process between CERN and its industrial partners. After a qualitative discussion of case studies, survival and count data models are estimated; the impact of CERN procurement on suppliers' innovation is captured by the number of patent applications. The fact that firms in our sample received their first order over a long time span (1995-2008) delivers a natural partition of industrial partners into ""suppliers"" and ""not yet suppliers"". This allows estimating the impact of CERN on the hazard to file a patent for the first time and on the number of patent applications, as well as the time needed for these effects to show up. We find that a ""CERN effect"" does exist: being an industrial partner of CERN is associated with an increase in the hazard to file a patent for the first time and in the number of patent applications. These effects require a significant ""gestation lag"" in the range of five to eight years, pointing to a relatively slow process of absorption of new ideas.",2019,2019-05-24,https://arxiv.org/abs/1905.09552,1
1806.06358,Effect of Climate and Geography on worldwide fine resolution economic activity,"Geography, including climatic factors, have long been considered potentially important elements in shaping socio-economic activities, alongside other determinants, such as institutions. Here we demonstrate that geography and climate satisfactorily explain worldwide economic activity as measured by the per capita Gross Cell Product (GCP-PC) at a fine geographical resolution, typically much higher than country average. A 1{\deg} by 1{\deg} GCP-PC dataset has been key for establishing and testing a direct relationship between 'local' geography/climate and GCP-PC. Not only have we tested the geography/climate hypothesis using many possible explanatory variables, importantly we have also predicted and reconstructed GCP-PC worldwide by retaining the most significant predictors. While this study confirms that latitude is the most important predictor for GCP-PC when taken in isolation, the accuracy of the GCP-PC prediction is greatly improved when other factors mainly related to variations in climatic variables, such as the variability in air pressure, rather than average climatic conditions as typically used, are considered. Implications of these findings include an improved understanding of why economically better-off societies are geographically placed where they are",2018,2019-01-04,https://arxiv.org/abs/1806.06358,2
1307.2180,Explaining Cost Overruns of Large-Scale Transportation Infrastructure Projects using a Signalling Game,"Strategic behaviour is one of the main explanations for cost overruns. It can theoretically be supported by agency theory, in which strategic behaviour is the result of asymmetric information between the principal and agent. This paper gives a formal account of this relation by a signalling game. This is a game with incomplete information which considers the way in which parties anticipate upon other parties' behaviour in choosing a course of action. The game shows how cost overruns are the result of an inappropriate signal. This makes it impossible for the principal to distinguish between the types of agents, and hence, allows for strategic behaviour. It is illustrated how cost overruns can be avoided by means of two policy measures, e.g. an accountability structure and benchmarking.",2013,2013-07-09,https://arxiv.org/abs/1307.2180,1
0712.2188,Capillarity-like growth of protein folding nuclei,"We analyzed folding routes predicted by a variational model in terms of a generalized formalism of the capillarity scaling theory for 28 two-state proteins. The scaling exponent ranged from 0.2 to 0.45 with an average of 0.33. This average value corresponds to packing of rigid objects.That is, on average the folded core of the nucleus is found to be relatively diffuse. We also studied the growth of the folding nucleus and interface along the folding route in terms of the density or packing fraction. The evolution of the folded core and interface regions can be classified into three patterns of growth depending on how the growth of the folded core is balanced by changes in density of the interface. Finally, we quantified the diffuse versus polarized structure of the critical nucleus through direct calculation of the packing fraction of the folded core and interface regions. Our results support the general picture of describing protein folding as the capillarity-like growth of folding nuclei.",2007,2007-12-14,https://arxiv.org/abs/0712.2188,1
0704.0484,Search for Chaotic Behavior in a Flapping Flag,We measured the correlation of the times between successive flaps of a flag for a variety of wind speeds and found no evidence of low dimensional chaotic behavior in the return maps of these times. We instead observed what is best modeled as random times determined by an exponential distribution. This study was done as an undergraduate experiment and illustrates the differences between low dimensional chaotic and possibly higher dimensional chaotic systems.,2007,2007-08-29,https://arxiv.org/abs/0704.0484,4
0909.2785,On Goodness of Fit Tests For Models of Neuronal Spike Trains Considered as Counting Processes,"After an elementary derivation of the ""time transformation"", mapping a counting process onto a homogeneous Poisson process with rate one, a brief review of Ogata's goodness of fit tests is presented and a new test, the ""Wiener process test"", is proposed. This test is based on a straightforward application of Donsker's Theorem to the intervals of time transformed counting processes. The finite sample properties of the test are studied by Monte Carlo simulations. Performances on simulated as well as on real data are presented. It is argued that due to its good finite sample properties, the new test is both a simple and a useful complement to Ogata's tests. Warnings are moreover given against the use of a single goodness of fit test.",2009,2009-09-16,https://arxiv.org/abs/0909.2785,1
1011.2895,Detection of radioactive material entering national ports: A Bayesian approach to radiation portal data,"Given the potential for illicit nuclear material being used for terrorism, most ports now inspect a large number of goods entering national borders for radioactive cargo. The U.S. Department of Homeland Security is moving toward one hundred percent inspection of all containers entering the U.S. at various ports of entry for nuclear material. We propose a Bayesian classification approach for the real-time data collected by the inline Polyvinyl Toluene radiation portal monitors. We study the computational and asymptotic properties of the proposed method and demonstrate its efficacy in simulations. Given data available to the authorities, it should be feasible to implement this approach in practice.",2010,2010-11-15,https://arxiv.org/abs/1011.2895,1
1304.3969,Post-Selection Inference for Generalized Linear Models with Many Controls,"This paper considers generalized linear models in the presence of many controls. We lay out a general methodology to estimate an effect of interest based on the construction of an instrument that immunize against model selection mistakes and apply it to the case of logistic binary choice model. More specifically we propose new methods for estimating and constructing confidence regions for a regression parameter of primary interest $\alpha_0$, a parameter in front of the regressor of interest, such as the treatment variable or a policy variable. These methods allow to estimate $\alpha_0$ at the root-$n$ rate when the total number $p$ of other regressors, called controls, potentially exceed the sample size $n$ using sparsity assumptions. The sparsity assumption means that there is a subset of $s<n$ controls which suffices to accurately approximate the nuisance part of the regression function. Importantly, the estimators and these resulting confidence regions are valid uniformly over $s$-sparse models satisfying $s^2\log^2 p = o(n)$ and other technical conditions. These procedures do not rely on traditional consistent model selection arguments for their validity. In fact, they are robust with respect to moderate model selection mistakes in variable selection. Under suitable conditions, the estimators are semi-parametrically efficient in the sense of attaining the semi-parametric efficiency bounds for the class of models in this paper.",2013,2017-10-05,https://arxiv.org/abs/1304.3969,3
1407.3154,Portfolio optimization in the case of an asset with a given liquidation time distribution,"Management of the portfolios containing low liquidity assets is a tedious problem. The buyer proposes the price that can differ greatly from the paper value estimated by the seller, the seller, on the other hand, can not liquidate his portfolio instantly and waits for a more favorable offer. To minimize losses in this case we need to develop new methods. One of the steps moving the theory towards practical needs is to take into account the time lag of the liquidation of an illiquid asset. This task became especially significant for the practitioners in the time of the global financial crises. Working in the Merton's optimal consumption framework with continuous time we consider an optimization problem for a portfolio with an illiquid, a risky and a risk-free asset. While a standard Black-Scholes market describes the liquid part of the investment the illiquid asset is sold at a random moment with prescribed liquidation time distribution. In the moment of liquidation it generates additional liquid wealth dependent on illiquid assets paper value. The investor has the logarithmic utility function as a limit case of a HARA-type utility. Different distributions of the liquidation time of the illiquid asset are under consideration - a classical exponential distribution and Weibull distribution that is more practically relevant. Under certain conditions we show the existence of the viscosity solution in both cases. Applying numerical methods we compare classical Merton's strategies and the optimal consumption-allocation strategies for portfolios with different liquidation-time distributions of an illiquid asset.",2014,2020-09-28,https://arxiv.org/abs/1407.3154,1
0705.0734,Soft constraint abstraction based on semiring homomorphism,"The semiring-based constraint satisfaction problems (semiring CSPs), proposed by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of soft constraints. In this paper we propose an abstraction scheme for soft constraints that uses semiring homomorphism. To find optimal solutions of the concrete problem, the idea is, first working in the abstract problem and finding its optimal solutions, then using them to solve the concrete problem. In particular, we show that a mapping preserves optimal solutions if and only if it is an order-reflecting semiring homomorphism. Moreover, for a semiring homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in $\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that $\bar{t}$ has the same value as $t$ in $\alpha(P)$.",2007,2010-07-01,https://arxiv.org/abs/0705.0734,1
0705.3496,The two-parameter Poisson--Dirichlet point process,"The two-parameter Poisson--Dirichlet distribution is a probability distribution on the totality of positive decreasing sequences with sum 1 and hence considered to govern masses of a random discrete distribution. A characterization of the associated point process (that is, the random point process obtained by regarding the masses as points in the positive real line) is given in terms of the correlation functions. Using this, we apply the theory of point processes to reveal the mathematical structure of the two-parameter Poisson--Dirichlet distribution. Also, developing the Laplace transform approach due to Pitman and Yor, we are able to extend several results previously known for the one-parameter case. The Markov--Krein identity for the generalized Dirichlet process is discussed from the point of view of functional analysis based on the two-parameter Poisson--Dirichlet distribution.",2007,2010-01-12,https://arxiv.org/abs/0705.3496,3
0708.1136,Spatial effects on the speed and reliability of protein-DNA search,"Strong experimental and theoretical evidence shows that transcription factors and other specific DNA-binding proteins find their sites using a two-mode search: alternating between 3D diffusion through the cell and 1D sliding along the DNA. We consider the role spatial effects in the mechanism on two different scales. First, we reconcile recent experimental findings by showing that the 3D diffusion of the transcription factor is often local, i.e. the transcription factor lands quite near its dissociation site. Second, we discriminate between two types of searches: global searches and local searches. We show that these searches differ significantly in average search time and the variability of search time. Using experimentally measured parameter values, we also show that 1D and 3D search is not optimally balanced, leading to much larger estimates of search time. Together, these results lead to a number of biological implications including suggestions of how prokaryotes and eukaryotes achieve rapid gene regulation and the relationship between the search mechanism and noise in gene expression.",2007,2008-06-11,https://arxiv.org/abs/0708.1136,3
0705.1076,Noncommutative tori and the Riemann-Hilbert correspondence,"We study the interplay between noncommutative tori and noncommutative elliptic curves through a category of equivariant differential modules on $\mathbb{C}^*$. We functorially relate this category to the category of holomorphic vector bundles on noncommutative tori as introduced by Polishchuk and Schwarz and study the induced map between the corresponding K-theories. In addition, there is a forgetful functor to the category of noncommutative elliptic curves of Soibelman and Vologodsky, as well as a forgetful functor to the category of vector bundles on $\mathbb{C}^*$ with regular singular connections. The category that we consider has the nice property of being a Tannakian category, hence it is equivalent to the category of representations of an affine group scheme. Via an equivariant version of the Riemann-Hilbert correspondence we determine this group scheme to be (the algebraic hull of) $\mathbb{Z}^2$. We also obtain a full subcategory of the category of holomorphic bundles of the noncommutative torus, which is equivalent to the category of representations of $\mathbb{Z}$. This group is the proposed topological fundamental group of the noncommutative torus (understood as a degenerate elliptic curve) and we study Nori's notion of \'etale fundamental group in this context.",2007,2015-08-26,https://arxiv.org/abs/0705.1076,2
0704.2278,Inference on Eigenvalues of Wishart Distribution Using Asymptotics with respect to the Dispersion of Population Eigenvalues,"In this paper we derive some new and practical results on testing and interval estimation problems for the population eigenvalues of a Wishart matrix based on the asymptotic theory for block-wise infinite dispersion of the population eigenvalues. This new type of asymptotic theory has been developed by the present authors in Takemura and Sheena (2005) and Sheena and Takemura (2007a,b) and in these papers it was applied to point estimation problem of population covariance matrix in a decision theoretic framework. In this paper we apply it to some testing and interval estimation problems. We show that the approximation based on this type of asymptotics is generally much better than the traditional large-sample asymptotics for the problems.",2007,2009-01-27,https://arxiv.org/abs/0704.2278,1
0704.0296,"Generalized Twistor Transform And Dualities, With A New Description of Particles With Spin, Beyond Free and Massless","A generalized twistor transform for spinning particles in 3+1 dimensions is constructed that beautifully unifies many types of spinning systems by mapping them to the same twistor, thus predicting an infinite set of duality relations among spinning systems with different Hamiltonians. Usual 1T-physics is not equipped to explain the duality relationships and unification between these systems. We use 2T-physics in 4+2 dimensions to uncover new properties of twistors, and expect that our approach will prove to be useful for practical applications as well as for a deeper understanding of fundamental physics. Unexpected structures for a new description of spinning particles emerge. A unifying symmetry SU(2,3) that includes conformal symmetry SU(2,2)=SO(4,2) in the massless case, turns out to be a fundamental property underlying the dualities of a large set of spinning systems, including those that occur in high spin theories. This may lead to new forms of string theory backgrounds as well as to new methods for studying various corners of M theory. In this paper we present the main concepts, and in a companion paper we give other details.",2007,2008-11-26,https://arxiv.org/abs/0704.0296,2
0711.2618,"A System for Distributed Mechanisms: Design, Implementation and Applications","We describe here a structured system for distributed mechanism design appropriate for both Intranet and Internet applications. In our approach the players dynamically form a network in which they know neither their neighbours nor the size of the network and interact to jointly take decisions. The only assumption concerning the underlying communication layer is that for each pair of processes there is a path of neighbours connecting them. This allows us to deal with arbitrary network topologies. We also discuss the implementation of this system which consists of a sequence of layers. The lower layers deal with the operations that implement the basic primitives of distributed computing, namely low level communication and distributed termination, while the upper layers use these primitives to implement high level communication among players, including broadcasting and multicasting, and distributed decision making. This yields a highly flexible distributed system whose specific applications are realized as instances of its top layer. This design is implemented in Java. The system supports at various levels fault-tolerance and includes a provision for distributed policing the purpose of which is to exclude `dishonest' players. Also, it can be used for repeated creation of dynamically formed networks of players interested in a joint decision making implemented by means of a tax-based mechanism. We illustrate its flexibility by discussing a number of implemented examples.",2007,2011-09-21,https://arxiv.org/abs/0711.2618,4
1801.01811,SABCEMM-A Simulator for Agent-Based Computational Economic Market Models,"We introduce the simulation tool SABCEMM (Simulator for Agent-Based Computational Economic Market Models) for agent-based computational economic market (ABCEM) models. Our simulation tool is implemented in C++ and we can easily run ABCEM models with several million agents. The object-oriented software design enables the isolated implementation of building blocks for ABCEM models, such as agent types and market mechanisms. The user can design and compare ABCEM models in a unified environment by recombining existing building blocks using the XML-based SABCEMM configuration file. We introduce an abstract ABCEM model class which our simulation tool is built upon. Furthermore, we present the software architecture as well as computational aspects of SABCEMM. Here, we focus on the efficiency of SABCEMM with respect to the run time of our simulations. We show the great impact of different random number generators on the run time of ABCEM models. The code and documentation is published on GitHub at https://github.com/SABCEMM/SABCEMM, such that all results can be reproduced by the reader.",2018,2018-10-11,https://arxiv.org/abs/1801.01811,2
1710.01604,Cell Detection by Functional Inverse Diffusion and Non-negative Group Sparsity$-$Part I: Modeling and Inverse Problems,"In this two-part paper, we present a novel framework and methodology to analyze data from certain image-based biochemical assays, e.g., ELISPOT and Fluorospot assays. In this first part, we start by presenting a physical partial differential equations (PDE) model up to image acquisition for these biochemical assays. Then, we use the PDEs' Green function to derive a novel parametrization of the acquired images. This parametrization allows us to propose a functional optimization problem to address inverse diffusion. In particular, we propose a non-negative group-sparsity regularized optimization problem with the goal of localizing and characterizing the biological cells involved in the said assays. We continue by proposing a suitable discretization scheme that enables both the generation of synthetic data and implementable algorithms to address inverse diffusion. We end Part I by providing a preliminary comparison between the results of our methodology and an expert human labeler on real data. Part II is devoted to providing an accelerated proximal gradient algorithm to solve the proposed problem and to the empirical validation of our methodology.",2017,2018-10-19,https://arxiv.org/abs/1710.01604,5
1801.01979,On the efficiency of computational imaging with structured illumination,"A generic computational imaging setup is considered which assumes sequential illumination of a semi-transparent object by an arbitrary set of structured illumination patterns. For each incident illumination pattern, all transmitted light is collected by a photon-counting bucket (single-pixel) detector. The transmission coefficients measured in this way are then used to reconstruct the spatial distribution of the object's projected transmission. It is demonstrated that the squared spatial resolution of such a setup is usually equal to the ratio of the image area to the number of linearly independent illumination patterns. If the noise in the measured transmission coefficients is dominated by photon shot noise, then the ratio of the spatially-averaged squared mean signal to the spatially-averaged noise variance in the ""flat"" distribution reconstructed in the absence of the object, is equal to the average number of registered photons when the illumination patterns are orthogonal. The signal-to-noise ratio in a reconstructed transmission distribution is always lower in the case of non-orthogonal illumination patterns due to spatial correlations in the measured data. Examples of imaging methods relevant to the presented analysis include conventional imaging with a pixelated detector, computational ghost imaging, compressive sensing, super-resolution imaging and computed tomography.",2018,2018-05-23,https://arxiv.org/abs/1801.01979,2
1904.05952,Identification of Noncausal Models by Quantile Autoregressions,"We propose a model selection criterion to detect purely causal from purely noncausal models in the framework of quantile autoregressions (QAR). We also present asymptotics for the i.i.d. case with regularly varying distributed innovations in QAR. This new modelling perspective is appealing for investigating the presence of bubbles in economic and financial time series, and is an alternative to approximate maximum likelihood methods. We illustrate our analysis using hyperinflation episodes in Latin American countries.",2019,2019-04-15,https://arxiv.org/abs/1904.05952,1
0904.1157,Addressing the bias in Monte Carlo pricing of multi-asset options with multiple barriers through discrete sampling,"An efficient conditioning technique, the so-called Brownian Bridge simulation, has previously been applied to eliminate pricing bias that arises in applications of the standard discrete-time Monte Carlo method to evaluate options written on the continuous-time extrema of an underlying asset. It is based on the simple and easy to implement analytic formulas for the distribution of one-dimensional Brownian Bridge extremes. This paper extends the technique to the valuation of multi-asset options with knock-out barriers imposed for all or some of the underlying assets. We derive formula for the unbiased option price estimator based on the joint distribution of the multi-dimensional Brownian Bridge dependent extrema. As analytic formulas are not available for the joint distribution in general, we develop upper and lower biased option price estimators based on the distribution of independent extrema and the Fr\'echet lower and upper bounds for the unknown distribution. All estimators are simple and easy to implement. They can always be used to bind the true value by a confidence interval. Numerical tests indicate that our biased estimators converge rapidly to the true option value as the number of time steps for the asset path simulation increases in comparison to the estimator based on the standard discrete-time method. The convergence rate depends on the correlation and barrier structures of the underlying assets.",2009,2009-04-08,https://arxiv.org/abs/0904.1157,1
1903.07997,"Variety, Complexity and Economic Development","We propose a combinatorial model of economic development. An economy develops by acquiring new capabilities allowing for the production of an ever greater variety of products of increasingly complex products. Taking into account that economies abandon the least complex products as they develop over time, we show that variety first increases and then decreases in the course of economic development. This is consistent with the empirical pattern known as 'the hump'. Our results question the common association of variety with complexity. We further discuss the implications of our model for future research.",2019,2019-03-20,https://arxiv.org/abs/1903.07997,1
1902.05938,A Comparison of Economic Agent-Based Model Calibration Methods,"Interest in agent-based models of financial markets and the wider economy has increased consistently over the last few decades, in no small part due to their ability to reproduce a number of empirically-observed stylised facts that are not easily recovered by more traditional modelling approaches. Nevertheless, the agent-based modelling paradigm faces mounting criticism, focused particularly on the rigour of current validation and calibration practices, most of which remain qualitative and stylised fact-driven. While the literature on quantitative and data-driven approaches has seen significant expansion in recent years, most studies have focused on the introduction of new calibration methods that are neither benchmarked against existing alternatives nor rigorously tested in terms of the quality of the estimates they produce. We therefore compare a number of prominent ABM calibration methods, both established and novel, through a series of computational experiments in an attempt to determine the respective strengths and weaknesses of each approach and the overall quality of the resultant parameter estimates. We find that Bayesian estimation, though less popular in the literature, consistently outperforms frequentist, objective function-based approaches and results in reasonable parameter estimates in many contexts. Despite this, we also find that agent-based model calibration techniques require further development in order to definitively calibrate large-scale models. We therefore make suggestions for future research.",2019,2019-02-18,https://arxiv.org/abs/1902.05938,1
1305.6988,Integrals of Higher Binary Options and Defaultable Bond with Discrete Default Information,"In this article, we study the problem of pricing defaultable bond with discrete default intensity and barrier under constant risk free short rate using higher order binary options and their integrals. In our credit risk model, the risk free short rate is a constant and the default event occurs in an expected manner when the firm value reaches a given default barrier at predetermined discrete announcing dates or in an unexpected manner at the first jump time of a Poisson process with given default intensity given by a step function of time variable, respectively. We consider both endogenous and exogenous default recovery. Our pricing problem is derived to a solving problem of inhomogeneous or homogeneous Black-Scholes PDEs with different coefficients and terminal value of binary type in every subinterval between the two adjacent announcing dates. In order to deal with the difference of coefficients in subintervals we use a relation between prices of higher order binaries with different coefficients. In our model, due to the inhomogenous term related to endogenous recovery, our pricing formulae are represented by not only the prices of higher binary options but also the integrals of them. So we consider a special binary option called integral of i-th binary or nothing and then we obtain the pricing formulae of our defaultable corporate bond by using the pricing formulae of higher binary options and integrals of them.",2013,2013-10-23,https://arxiv.org/abs/1305.6988,5
1811.06950,Deep learning approach to coherent noise reduction in optical diffraction tomography,"We present a deep neural network to reduce coherent noise in three-dimensional quantitative phase imaging. Inspired by the cycle generative adversarial network, the denoising network was trained to learn a transform between two image domains: clean and noisy refractive index tomograms. The unique feature of this network, distinct from previous machine learning approaches employed in the optical imaging problem, is that it uses unpaired images. The learned network quantitatively demonstrated its performance and generalization capability through denoising experiments of various samples. We concluded by applying our technique to reduce the temporally changing noise emerging from focal drift in time-lapse imaging of biological cells. This reduction cannot be performed using other optical methods for denoising.",2018,2019-03-27,https://arxiv.org/abs/1811.06950,1
0704.3916,The Gehring Lemma in Metric Spaces,We present a proof for the Gehring lemma in a metric measure space endowed with a doubling measure. As an application we show the self improving property of Muckenhoupt weights.,2007,2008-01-15,https://arxiv.org/abs/0704.3916,3
1303.3391,US Corporate Bond Yield Spread : A default risk debate,"According to theoretical models of valuing risky corporate securities, risk of default is primary component in overall yield spread. However, sizable empirical literature considers it otherwise by giving more importance to non-default risk factors. Current study empirically attempts to provide relative solution to this conundrum by presuming that problem lies in the subjective empirical treatment of default risk. By using post-hoc estimator approach of Lubotsky & Wittenberg (2006), we construct an efficient indicator for risk of default, by using sample of 252 US non-financial corporate data (2000-2010). On average, our results validate that almost 48% of change in yield spread is explained by default risk especially in recent financial crisis period (2007-2009). Hence, our results relatively suggest that potential problem lies in the ad-hoc measurement methods used in existing empirical literature.",2013,2013-03-15,https://arxiv.org/abs/1303.3391,1
1905.04419,The role of pawnshops in risk coping in early twentieth-century Japan,"This study examines the role of pawnshops as a risk-coping device in prewar Japan. Using data on pawnshop loans for more than 250 municipalities and exploiting the 1918-1920 influenza pandemic as a natural experiment, we find that the adverse health shock increased the total amount of loans from pawnshops. This is because those who regularly relied on pawnshops borrowed more money from them than usual to cope with the adverse health shock, and not because the number of people who used pawnshops increased.",2019,2023-06-22,https://arxiv.org/abs/1905.04419,2
1005.1214,Parametric inference in a perturbed gamma degradation process,"We consider the gamma process perturbed by a Brownian motion (independent of the gamma process) as a degradation model. Parameters estimation is studied here. We assume that $n$ independent items are observed at irregular instants. From these observations, we estimate the parameters using the moments method. Then, we study the asymptotic properties of the estimators. Furthermore we derive some particular cases of items observed at regular or non-regular instants. Finally, some numerical simulations and two real data applications are provided to illustrate our method.",2010,2010-06-16,https://arxiv.org/abs/1005.1214,1
0704.1932,A discriminating probe of gravity at cosmological scales,"The standard cosmological model is based on general relativity and includes dark matter and dark energy. An important prediction of this model is a fixed relationship between the gravitational potentials responsible for gravitational lensing and the matter overdensity. Alternative theories of gravity often make different predictions for this relationship. We propose a set of measurements which can test the lensing/matter relationship, thereby distinguishing between dark energy/matter models and models in which gravity differs from general relativity. Planned optical, infrared and radio galaxy and lensing surveys will be able to measure $E_G$, an observational quantity whose expectation value is equal to the ratio of the Laplacian of the Newtonian potentials to the peculiar velocity divergence, to percent accuracy. We show that this will easily separate alternatives such as $\Lambda$CDM, DGP, TeVeS and $f(R)$ gravity.",2007,2008-11-26,https://arxiv.org/abs/0704.1932,3
1010.0807,Discussion of Likelihood Inference for Models with Unobservables: Another View,"Discussion of ""Likelihood Inference for Models with Unobservables: Another View"" by Youngjo Lee and John A. Nelder [arXiv:1010.0303]",2010,2010-10-06,https://arxiv.org/abs/1010.0807,1
1210.0057,Consumer finance data generator - a new approach to Credit Scoring technique comparison,"This paper aims to present a general idea of method comparison of Credit Scoring techniques. Any scorecard can be made in various methods based on variable transformations in the logistic regression model. To make a comparison and come up with the proof that one technique is better than another is a big challenge due to the limited availability of data. The same conclusion cannot be guaranteed when using other data from another source. The following research challenge can therefore be formulated: how should the comparison be managed in order to get general results that are not biased by particular data? The solution may be in the use of various random data generators. The data generator uses two approaches: transition matrix and scorings. Here are presented both: results of comparison methods and the methodology of these comparison techniques creating. Before building a new model the modeler can undertake a comparison exercise that aims at identifying the best method in the case of the particular data. Here are presented various measures of predictive model like: Gini, Delta Gini, VIF and Max p-value, emphasizing the multi-criteria problem of a ""Good model"". The idea that is being suggested is of particular use in the model building process where there are defined complex criteria trying to cover the important problems of model stability over a period of time, in order to avoid a crisis. Some arguments for choosing Logit or WOE approach as the best scorecard technique are presented.",2012,2012-10-02,https://arxiv.org/abs/1210.0057,1
0812.4156,Arbitrage-free Pricing of Credit Index Options: The no-armageddon pricing measure and the role of correlation after the subprime crisis,"In this work we consider three problems of the standard market approach to pricing of credit index options: the definition of the index spread is not valid in general, the usually considered payoff leads to a pricing which is not always defined, and the candidate numeraire one would use to define a pricing measure is not strictly positive, which would lead to a non-equivalent pricing measure. We give a general mathematical solution to the three problems, based on a novel way of modeling the flow of information through the definition of a new subfiltration. Using this subfiltration, we take into account consistently the possibility of default of all names in the portfolio, that is neglected in the standard market approach. We show that, while the related mispricing can be negligible for standard options in normal market conditions, it can become highly relevant for different options or in stressed market conditions. In particular, we show on 2007 market data that after the subprime credit crisis the mispricing of the market formula compared to the no arbitrage formula we propose has become financially relevant even for the liquid Crossover Index Options.",2008,2008-12-23,https://arxiv.org/abs/0812.4156,1
1811.03329,Nonparametric maximum likelihood methods for binary response models with random coefficients,"Single index linear models for binary response with random coefficients have been extensively employed in many econometric settings under various parametric specifications of the distribution of the random coefficients. Nonparametric maximum likelihood estimation (NPMLE) as proposed by Cosslett (1983) and Ichimura and Thompson (1998), in contrast, has received less attention in applied work due primarily to computational difficulties. We propose a new approach to computation of NPMLEs for binary response models that significantly increase their computational tractability thereby facilitating greater flexibility in applications. Our approach, which relies on recent developments involving the geometry of hyperplane arrangements, is contrasted with the recently proposed deconvolution method of Gautier and Kitamura (2013). An application to modal choice for the journey to work in the Washington DC area illustrates the methods.",2018,2020-01-15,https://arxiv.org/abs/1811.03329,3
0704.1653,"Scaling cosmologies, geodesic motion and pseudo-susy","One-parameter solutions in supergravity carried by scalars and a metric trace out curves on the scalar manifold. In ungauged supergravity these curves describe a geodesic motion. It is known that a geodesic motion sometimes occurs in the presence of a scalar potential and for time-dependent solutions this can happen for scaling cosmologies. This note contains a further study of such solutions in the context of pseudo-supersymmetry for multi-field systems whose first-order equations we derive using a Bogomol'nyi-like method. In particular we show that scaling solutions that are pseudo-BPS must describe geodesic curves. Furthermore, we clarify how to solve the geodesic equations of motion when the scalar manifold is a maximally non-compact coset such as occurs in maximal supergravity. This relies upon a parametrization of the coset in the Borel gauge. We then illustrate this with the cosmological solutions of higher-dimensional gravity compactified on a $n$-torus.",2007,2008-11-26,https://arxiv.org/abs/0704.1653,3
0904.0830,Computing Tails of Compound Distributions Using Direct Numerical Integration,"An efficient adaptive direct numerical integration (DNI) algorithm is developed for computing high quantiles and conditional Value at Risk (CVaR) of compound distributions using characteristic functions. A key innovation of the numerical scheme is an effective tail integration approximation that reduces the truncation errors significantly with little extra effort. High precision results of the 0.999 quantile and CVaR were obtained for compound losses with heavy tails and a very wide range of loss frequencies using the DNI, Fast Fourier Transform (FFT) and Monte Carlo (MC) methods. These results, particularly relevant to operational risk modelling, can serve as benchmarks for comparing different numerical methods. We found that the adaptive DNI can achieve high accuracy with relatively coarse grids. It is much faster than MC and competitive with FFT in computing high quantiles and CVaR of compound distributions in the case of moderate to high frequencies and heavy tails.",2009,2010-02-04,https://arxiv.org/abs/0904.0830,3
0903.2960,Evolution of genomes in the hybridogenetic populations modelled by the Penna model,"Background: Hybridogenesis is a very interesting example of reproduction which seems to integrate the sexual and clonal processes in one system. In a case of frogs, described in the paper, two parental species - Rana lessonae and Rana ridibunda can form fertile hybrid individuals - Rana esculenta. Hybrid individuals eliminate one parental haplotype from their germ line cells before meiosis (end before recombination) which implicates clonal reproduction of the haplotype transferred to the gametes. All three ""species"" are called ""complex species"". To study the evolution of genomes in the hybridogenetic fraction of this complex species we have used the Monte Carlo based model rendering the age structured populations. The model enables the analysis of distribution of defective alleles in the individual genomes as well as in the genetic pool of the whole populations. Results: We have shown that longer isolation of hybrids' populations leads to the speciation through emerging the specific sets of complementing haplotypes in their genetic pool. The fraction of defective alleles increases but the defects are complemented in the heterozygous loci. Nevertheless, even small supply of new hybrids generated by the two parental species or crossbreeding between hybrids and one of the parental species prevents the speciation and changes the strategy of the genome evolution from the complementing to the purifying Darwinian selection.",2009,2009-03-18,https://arxiv.org/abs/0903.2960,1
0901.1572,Why do dolphins form mixed-species associations in the Azores ?,"Mixed-species associations are temporary associations between individuals of different species that are often observed in birds, primates and cetaceans. They have been interpreted as a strategy to reduce predation risk, enhance foraging success and/or provide a social advantage. In the archipelago of the Azores, four species of dolphins are commonly involved in mixed-species associations: the common dolphin, Delphinus delphis, the bottlenose dolphin, Tursiops truncatus, the striped dolphin, Stenella coeruleoalba, and the spotted dolphin, Stenella frontalis. In order to understand the reasons why dolphins associate, we analysed field data collected since 1999 by research scientists and trained observers placed onboard fishing vessels. In total, 113 mixed-species groups were observed out of 5720 sightings. The temporal distribution, habitat (water depth, distance to the coast), behaviour (i.e. feeding, travelling, socializing), size and composition of mixed-species groups were compared with those of single-species groups. Results did not support the predation avoidance hypothesis and gave little support to the social advantage hypothesis. The foraging advantage hypothesis was the most convincing. However, the benefits of mixed-species associations appeared to depend on the species. Associations were likely to be opportunistic in the larger bottlenose dolphin, while there seemed to be some evolutionary constraints favouring associations in the rarer striped dolphin. Comparison with previous studies suggests that the formation of mixed-species groups depends on several environmental factors, and therefore may constitute an adaptive response.",2009,2010-07-26,https://arxiv.org/abs/0901.1572,2
0707.1025,The star trellis decoding of Reed-Solomon codes,The new method for Reed-Solomon codes decoding is introduced. The method is based on the star trellis decoding of the binary image of Reed-Solomon codes.,2007,2007-07-13,https://arxiv.org/abs/0707.1025,1
1808.03630,A Novel Method for Epileptic Seizure Detection Using Coupled Hidden Markov Models,"We propose a novel Coupled Hidden Markov Model to detect epileptic seizures in multichannel electroencephalography (EEG) data. Our model defines a network of seizure propagation paths to capture both the temporal and spatial evolution of epileptic activity. To address the intractability introduced by the coupled interactions, we derive a variational inference procedure to efficiently infer the seizure evolution from spectral patterns in the EEG data. We validate our model on EEG aquired under clinical conditions in the Epilepsy Monitoring Unit of the Johns Hopkins Hospital. Using 5-fold cross validation, we demonstrate that our model outperforms three baseline approaches which rely on a classical detection framework. Our model also demonstrates the potential to localize seizure onset zones in focal epilepsy.",2018,2018-08-13,https://arxiv.org/abs/1808.03630,1
1906.11208,Proxy expenditure weights for Consumer Price Index: Audit sampling inference for big data statistics,"Purchase data from retail chains provide proxy measures of private household expenditure on items that are the most troublesome to collect in the traditional expenditure survey. Due to the sheer amount of proxy data, the bias due to coverage and selection errors completely dominates the variance. We develop tests for bias based on audit sampling, which makes use of available survey data that cannot be linked to the proxy data source at the individual level. However, audit sampling fails to yield a meaningful mean squared error estimate, because the sampling variance is too large compared to the bias of the big data estimate. We propose a novel accuracy measure that is applicable in such situations. This can provide a necessary part of the statistical argument for the uptake of big data source, in replacement of traditional survey sampling. An application to disaggregated food price index is used to demonstrate the proposed approach.",2019,2019-06-27,https://arxiv.org/abs/1906.11208,1
0706.0294,Mixed membership analysis of high-throughput interaction studies: Relational data,"In this paper, we consider the statistical analysis of a protein interaction network. We propose a Bayesian model that uses a hierarchy of probabilistic assumptions about the way proteins interact with one another in order to: (i) identify the number of non-observable functional modules; (ii) estimate the degree of membership of proteins to modules; and (iii) estimate typical interaction patterns among the functional modules themselves. Our model describes large amount of (relational) data using a relatively small set of parameters that we can reliably estimate with an efficient inference algorithm. We apply our methodology to data on protein-to-protein interactions in saccharomyces cerevisiae to reveal proteins' diverse functional roles. The case study provides the basis for an overview of which scientific questions can be addressed using our methods, and for a discussion of technical issues.",2007,2007-11-15,https://arxiv.org/abs/0706.0294,2
1708.06436,Unbiased Shrinkage Estimation,"Shrinkage estimation usually reduces variance at the cost of bias. But when we care only about some parameters of a model, I show that we can reduce variance without incurring bias if we have additional information about the distribution of covariates. In a linear regression model with homoscedastic Normal noise, I consider shrinkage estimation of the nuisance parameters associated with control variables. For at least three control variables and exogenous treatment, I establish that the standard least-squares estimator is dominated with respect to squared-error loss in the treatment effect even among unbiased estimators and even when the target parameter is low-dimensional. I construct the dominating estimator by a variant of James-Stein shrinkage in a high-dimensional Normal-means problem. It can be interpreted as an invariant generalized Bayes estimator with an uninformative (improper) Jeffreys prior in the target parameter.",2017,2017-11-01,https://arxiv.org/abs/1708.06436,2
1108.4113,Probability-free pricing of adjusted American lookbacks,"Consider an American option that pays G(X^*_t) when exercised at time t, where G is a positive increasing function, X^*_t := \sup_{s\le t}X_s, and X_s is the price of the underlying security at time s. Assuming zero interest rates, we show that the seller of this option can hedge his position by trading in the underlying security if he begins with initial capital X_0\int_{X_0}^{\infty}G(x)x^{-2}dx (and this is the smallest initial capital that allows him to hedge his position). This leads to strategies for trading that are always competitive both with a given strategy's current performance and, to a somewhat lesser degree, with its best performance so far. It also leads to methods of statistical testing that avoid sacrificing too much of the maximum statistical significance that they achieve in the course of accumulating data.",2011,2011-08-23,https://arxiv.org/abs/1108.4113,1
0704.1665,Approach to Physical Reality: a note on Poincare Group and the philosophy of Nagarjuna,"We argue about a possible scenario of physical reality based on the parallelism between Poincare group and the sunyata philosophy of Nagarjuna. The notion of ""relational"" is the common denominator of two views. We have approached the relational concept in third-person perspective (ontic level). It is possible to deduce different physical consequence and interpretation through first-person perspective approach. This relational interpretation leave open the questions: i)we must abandon the idea for a physical system the possibility to extract completeness information? ii)we must abandon the idea to infer a possible structure of physical reality?",2007,2007-05-23,https://arxiv.org/abs/0704.1665,1
0804.2752,"Boosting Algorithms: Regularization, Prediction and Model Fitting","We present a statistical perspective on boosting. Special emphasis is given to estimating potentially complex parametric or nonparametric models, including generalized linear and additive models as well as regression models for survival analysis. Concepts of degrees of freedom and corresponding Akaike or Bayesian information criteria, particularly useful for regularization and variable selection in high-dimensional covariate spaces, are discussed as well. The practical aspects of boosting procedures for fitting statistical models are illustrated by means of the dedicated open-source software package mboost. This package implements functions which can be used for model fitting, prediction and variable selection. It is flexible, allowing for the implementation of new boosting algorithms optimizing user-specified loss functions.",2008,2008-12-18,https://arxiv.org/abs/0804.2752,1
0707.3775,Predicting Knot or Catenane Type of Site-Specific Recombination Products,"Site-specific recombination on supercoiled circular DNA yields a variety of knotted or catenated products. We develop a model of this process, and give extensive experimental evidence that the assumptions of our model are reasonable. We then characterize all possible knot or catenane products that arise from the most common substrates. We apply our model to tightly prescribe the knot or catenane type of previously uncharacterized data.",2007,2007-08-07,https://arxiv.org/abs/0707.3775,2
1712.08659,Quickest Attack Detection in Smart Grid Based on Sequential Monte Carlo Filtering,"Quick and accurate detection of cyber attack is key to the normal operation of the smart grid system. In this paper, a joint state estimation and sequential attack detection method for a given bus with grid frequency drift is proposed that utilizes the commonly monitored output voltage. In particular, based on a non-linear state-space model derived from the three-phase sinusoidal voltage equations, we employ the sequential Monte Carlo (SMC) filtering to estimate the system state. The output of the SMC filter is fed into a CUSUM test to detect the attack in a fastest way. Moreover, an adaptive sampling strategy is proposed to reduce the rate of taking measurements and communicating with the controller. Extensive simulation results demonstrate that the proposed method achieves high adaptivity and efficient detection of various types of attacks in power systems.",2017,2017-12-27,https://arxiv.org/abs/1712.08659,1
1207.4309,Vine Constructions of Levy Copulas,"Levy copulas are the most general concept to capture jump dependence in multivariate Levy processes. They translate the intuition and many features of the copula concept into a time series setting. A challenge faced by both, distributional and Levy copulas, is to find flexible but still applicable models for higher dimensions. To overcome this problem, the concept of pair copula constructions has been successfully applied to distributional copulas. In this paper, we develop the pair construction for Levy copulas (PLCC). Similar to pair constructions of distributional copulas, the pair construction of a d-dimensional Levy copula consists of d(d-1)/2 bivariate dependence functions. We show that only d-1 of these bivariate functions are Levy copulas, whereas the remaining functions are distributional copulas. Since there are no restrictions concerning the choice of the copulas, the proposed pair construction adds the desired flexibility to Levy copula models. We discuss estimation and simulation in detail and apply the pair construction in a simulation study.",2012,2012-09-21,https://arxiv.org/abs/1207.4309,2
1003.0168,Order flow dynamics around extreme price changes on an emerging stock market,"We study the dynamics of order flows around large intraday price changes using ultra-high-frequency data from the Shenzhen Stock Exchange. We find a significant reversal of price for both intraday price decreases and increases with a permanent price impact. The volatility, the volume of different types of orders, the bid-ask spread, and the volume imbalance increase before the extreme events and decay slowly as a power law, which forms a well-established peak. The volume of buy market orders increases faster and the corresponding peak appears earlier than for sell market orders around positive events, while the volume peak of sell market orders leads buy market orders in the magnitude and time around negative events. When orders are divided into four groups according to their aggressiveness, we find that the behaviors of order volume and order number are similar, except for buy limit orders and canceled orders that the peak of order number postpones two minutes later after the peak of order volume, implying that investors placing large orders are more informed and play a central role in large price fluctuations. We also study the relative rates of different types of orders and find differences in the dynamics of relative rates between buy orders and sell orders and between individual investors and institutional investors. There is evidence showing that institutions behave very differently from individuals and that they have more aggressive strategies. Combing these findings, we conclude that institutional investors are more informed and play a more influential role in driving large price fluctuations.",2010,2010-08-03,https://arxiv.org/abs/1003.0168,1
1904.07660,Multiple-interaction kinetic modelling of a virtual-item gambling economy,"In recent years, there has been a proliferation of online gambling sites, which made gambling more accessible with a consequent rise in related problems, such as addiction. Hence, the analysis of the gambling behaviour at both the individual and the aggregate levels has become the object of several investigations. In this paper, resorting to classical methods of the kinetic theory, we describe the behaviour of a multi-agent system of gamblers participating in lottery-type games on a virtual-item gambling market. The comparison with previous, often empirical, results highlights the ability of the kinetic approach to explain how the simple microscopic rules of a gambling-type game produce complex collective trends, which might be difficult to interpret precisely by looking only at the available data.",2019,2019-07-24,https://arxiv.org/abs/1904.07660,1
0803.1082,Rank abundance relations in evolutionary dynamics of random replicators,"We present a non-equilibrium statistical mechanics description of rank abundance relations (RAR) in random community models of ecology. Specifically, we study a multi-species replicator system with quenched random interaction matrices. We here consider symmetric interactions as well as asymmetric and anti-symmetric cases. RARs are obtained analytically via a generating functional analysis, describing fixed-point states of the system in terms of a small set of order parameters, and in dependence on the symmetry or otherwise of interactions and on the productivity of the community. Our work is an extension of Tokita [Phys. Rev. Lett. {\bf 93} 178102 (2004)], where the case of symmetric interactions was considered within an equilibrium setup. The species abundance distribution in our model come out as truncated normal distributions or transformations thereof and, in some case, are similar to left-skewed distributions observed in ecology. We also discuss the interaction structure of the resulting food-web of stable species at stationarity, cases of heterogeneous co-operation pressures as well as effects of finite system size and of higher-order interactions.",2008,2009-11-13,https://arxiv.org/abs/0803.1082,2
1810.11609,On the linear static output feedback problem: the annihilating polynomial approach,"One of the fundamental open problems in control theory is that of the stabilization of a linear time invariant dynamical system through static output feedback. We are given a linear dynamical system defined through \begin{align*} \mydot{w} &= Aw + Bu y &= Cw . \end{align*} The problem is to find, if it exists, a feedback $u=Ky$ such that the matrix $A+BKC$ has all its eigenvalues in the complex left half plane and, if such a feedback does not exist, to prove that it does not. Substantial progress has not been made on the computational aspect of the solution to this problem. In this paper we consider instead `which annihilating polynomials can a matrix of the form $A+BKC$ possess?'. We give a simple solution to this problem when the system has either a single input or a single output. For the multi input - multi output case, we use these ideas to characterize the annihilating polynomials when $K$ has rank one, and suggest possible computational solutions for general $K.$ We also present some numerical evidence for the plausibility of this approach for the general case as well as for the problem of shifting the eigenvalues of the system.",2018,2018-10-31,https://arxiv.org/abs/1810.11609,2
0704.1172,Disentanglement in a quantum critical environment,"We study the dynamical process of disentanglement of two qubits and two qutrits coupled to an Ising spin chain in a transverse field, which exhibits a quantum phase transition. We use the concurrence and negativity to quantify entanglement of two qubits and two qutrits, respectively. Explicit connections between the concurrence (negativity) and the decoherence factors are given for two initial states, the pure maximally entangled state and the mixed Werner state. We find that the concurrence and negativity decay exponentially with fourth power of time in the vicinity of critical point of the environmental system.",2007,2009-11-13,https://arxiv.org/abs/0704.1172,1
1711.11407,FPS-SFT: A Multi-dimensional Sparse Fourier Transform Based on the Fourier Projection-slice Theorem,"We propose a multi-dimensional (M-D) sparse Fourier transform inspired by the idea of the Fourier projection-slice theorem, called FPS-SFT. FPS-SFT extracts samples along lines (1-dimensional slices from an M-D data cube), which are parameterized by random slopes and offsets. The discrete Fourier transform (DFT) along those lines represents projections of M-D DFT of the M-D data onto those lines. The M-D sinusoids that are contained in the signal can be reconstructed from the DFT along lines with a low sample and computational complexity provided that the signal is sparse in the frequency domain and the lines are appropriately designed. The performance of FPS-SFT is demonstrated both theoretically and numerically. A sparse image reconstruction application is illustrated, which shows the capability of the FPS-SFT in solving practical problems.",2017,2017-12-01,https://arxiv.org/abs/1711.11407,1
1812.03358,A practical light transport system model for chemiluminescence distribution reconstruction,"Plenoptic cameras and other integral photography instruments capture richer angular information from a scene than traditional 2D cameras. This extra information is used to estimate depth, perform superresolution or reconstruct 3D information from the scene. Many of these applications involve solving a large-scale numerical optimization problem. Most published approaches model the camera(s) using pre-computed matrices that require large amounts of memory and are not well-suited to modern many-core processors. We propose a flexible camera model based on light transport and use it to model plenoptic and traditional cameras. We implement the proposed model on a GPU and use it to reconstruct simulated and real 3D chemiluminescence distributions (flames) from images taken by traditional and plenoptic cameras.",2018,2018-12-11,https://arxiv.org/abs/1812.03358,1
0705.1750,A Tighter Analysis of Setcover Greedy Algorithm for Test Set,"Setcover greedy algorithm is a natural approximation algorithm for test set problem. This paper gives a precise and tighter analysis of performance guarantee of this algorithm. The author improves the performance guarantee $2\ln n$ which derives from set cover problem to $1.1354\ln n$ by applying the potential function technique. In addition, the author gives a nontrivial lower bound $1.0004609\ln n$ of performance guarantee of this algorithm. This lower bound, together with the matching bound of information content heuristic, confirms the fact information content heuristic is slightly better than setcover greedy algorithm in worst case.",2007,2011-03-08,https://arxiv.org/abs/0705.1750,6
0708.2732,Secrecy Capacity Region of Binary and Gaussian Multiple Access Channels,"A generalized multiple access channel (GMAC) with one confidential message set is studied, where two users (users 1 and 2) attempt to transmit common information to a destination, and user 1 also has confidential information intended for the destination. Moreover, user 1 wishes to keep its confidential information as secret as possible from user 2. A deterministic GMAC is first studied, and the capacity-equivocation region and the secrecy capacity region are obtained. Two main classes of the GMAC are then studied: the binary GMAC and the Gaussian GMAC. For both channels, the capacity-equivocation region and the secrecy capacity region are established.",2007,2007-08-22,https://arxiv.org/abs/0708.2732,1
1711.03847,Sparse Bayesian Learning for DOA Estimation in Heteroscedastic Noise,"The paper considers direction of arrival (DOA) estimation from long-term observations in a noisy environment. In such an environment the noise source might evolve, causing the stationary models to fail. Therefore a heteroscedastic Gaussian noise model is introduced where the variance can vary across observations and sensors. The source amplitudes are assumed independent zero-mean complex Gaussian distributed with unknown variances (i.e. the source powers), inspiring stochastic maximum likelihood DOA estimation. The DOAs of plane waves are estimated from multi-snapshot sensor array data using sparse Bayesian learning (SBL) where the noise is estimated across both sensors and snapshots. This SBL approach is more flexible and performs better than high-resolution methods since they cannot estimate the heteroscedastic noise process. An alternative to SBL is simple data normalization, whereby only the phase across the array is utilized. Simulations demonstrate that taking the heteroscedastic noise into account improves DOA estimation.",2017,2017-11-13,https://arxiv.org/abs/1711.03847,1
1011.0096,Concentration inequalities of the cross-validation estimator for Empirical Risk Minimiser,"In this article, we derive concentration inequalities for the cross-validation estimate of the generalization error for empirical risk minimizers. In the general setting, we prove sanity-check bounds in the spirit of \cite{KR99} \textquotedblleft\textit{bounds showing that the worst-case error of this estimate is not much worse that of training error estimate} \textquotedblright . General loss functions and class of predictors with finite VC-dimension are considered. We closely follow the formalism introduced by \cite{DUD03} to cover a large variety of cross-validation procedures including leave-one-out cross-validation, $k$% -fold cross-validation, hold-out cross-validation (or split sample), and the leave-$\upsilon$-out cross-validation. In particular, we focus on proving the consistency of the various cross-validation procedures. We point out the interest of each cross-validation procedure in terms of rate of convergence. An estimation curve with transition phases depending on the cross-validation procedure and not only on the percentage of observations in the test sample gives a simple rule on how to choose the cross-validation. An interesting consequence is that the size of the test sample is not required to grow to infinity for the consistency of the cross-validation procedure.",2010,2010-11-02,https://arxiv.org/abs/1011.0096,1
0710.0445,Evolution of the genetic code from the GC- to the AGUC-alphabet,"A hypothesis of the evolution of the genetic code is proposed, the leading mechanism of which is the nucleotide spontaneous damage leading to AT-enrichment of the genome. The hypothesis accounts for stability of the genetic code towards point mutations, the presence of code dialects, and the symmetry of the genetic code table.",2007,2007-10-03,https://arxiv.org/abs/0710.0445,1
0704.1384,Generalizing circles over algebraic extensions,"This paper deals with a family of spatial rational curves that were introduced by Andradas, Recio and Sendra, under the name of hypercircles, as an algorithmic cornerstone tool in the context of improving the rational parametrization (simplifying the coefficients of the rational functions, when possible) of algebraic varieties. A real circle can be defined as the image of the real axis under a Moebius transformation in the complex field. Likewise, and roughly speaking, a hypercircle can be defined as the image of a line (""the ${\mathbb{K}}$-axis"") in a $n$-degree finite algebraic extension $\mathbb{K}(\alpha)\thickapprox\mathbb{K}^n$ under the transformation $\frac{at+b}{ct+d}:\mathbb{K}(\alpha)\to\mathbb{K}(\alpha)$. The aim of this article is to extend, to the case of hypercircles, some of the specific properties of circles. We show that hypercircles are precisely, via $\mathbb{K}$-projective transformations, the rational normal curve of a suitable degree. We also obtain a complete description of the points at infinity of these curves (generalizing the cyclic structure at infinity of circles). We characterize hypercircles as those curves of degree equal to the dimension of the ambient affine space and with infinitely many ${\mathbb{K}}$-rational points, passing through these points at infinity. Moreover, we give explicit formulae for the parametrization and implicitation of hypercircles. Besides the intrinsic interest of this very special family of curves, the understanding of its properties has a direct application to the simplification of parametrizations problem, as shown in the last section.",2007,2014-01-27,https://arxiv.org/abs/0704.1384,1
0911.5503,Finitely additive probabilities and the Fundamental Theorem of Asset Pricing,"This work aims at a deeper understanding of the mathematical implications of the economically-sound condition of absence of arbitrages of the first kind in a financial market. In the spirit of the Fundamental Theorem of Asset Pricing (FTAP), it is shown here that absence of arbitrages of the first kind in the market is equivalent to the existence of a finitely additive probability, weakly equivalent to the original and only locally countably additive, under which the discounted wealth processes become ""local martingales"". The aforementioned result is then used to obtain an independent proof of the FTAP of Delbaen and Schachermayer. Finally, an elementary and short treatment of the previous discussion is presented for the case of continuous-path semimartingale asset-price processes.",2009,2009-12-01,https://arxiv.org/abs/0911.5503,1
0704.1455,A Better Good-Turing Estimator for Sequence Probabilities,"We consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. The key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. In this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. To overcome this problem, the traditional approach to probability estimation is to use the classical Good-Turing estimator. We introduce a natural scaling model and use it to show that the Good-Turing sequence probability estimator is not consistent. We then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.",2007,2007-07-13,https://arxiv.org/abs/0704.1455,2
1310.1142,Non-Arbitrage up to Random Horizon for Semimartingale Models,"This paper addresses the question of how an arbitrage-free semimartingale model is affected when stopped at a random horizon. We focus on No-Unbounded-Profit-with-Bounded-Risk (called NUPBR hereafter) concept, which is also known in the literature as the first kind of non-arbitrage. For this non-arbitrage notion, we obtain two principal results. The first result lies in describing the pairs of market model and random time for which the resulting stopped model fulfills NUPBR condition. The second main result characterises the random time models that preserve the NUPBR property after stopping for any market model. These results are elaborated in a very general market model, and we also pay attention to some particular and practical models. The analysis that drives these results is based on new stochastic developments in semimartingale theory with progressive enlargement. Furthermore, we construct explicit martingale densities (deflators) for some classes of local martingales when stopped at random time.",2013,2014-02-21,https://arxiv.org/abs/1310.1142,2
1801.01792,Dynamic and granular loss reserving with copulae,"An intensive research sprang up for stochastic methods in insurance during the past years. To meet all future claims rising from policies, it is requisite to quantify the outstanding loss liabilities. Loss reserving methods based on aggregated data from run-off triangles are predominantly used to calculate the claims reserves. Conventional reserving techniques have some disadvantages: loss of information from the policy and the claim's development due to the aggregation, zero or negative cells in the triangle; usually small number of observations in the triangle; only few observations for recent accident years; and sensitivity to the most recent paid claims. To overcome these dilemmas, granular loss reserving methods for individual claim-by-claim data will be derived. Reserves' estimation is a crucial part of the risk valuation process, which is now a front burner in economics. Since there is a growing demand for prediction of total reserves for different types of claims or even multiple lines of business, a time-varying copula framework for granular reserving will be established.",2018,2018-01-08,https://arxiv.org/abs/1801.01792,1
0704.1473,Existence of Universal Entangler,"A gate is called entangler if it transforms some (pure) product states to entangled states. A universal entangler is a gate which transforms all product states to entangled states. In practice, a universal entangler is a very powerful device for generating entanglements, and thus provides important physical resources for accomplishing many tasks in quantum computing and quantum information. This Letter demonstrates that a universal entangler always exists except for a degenerated case. Nevertheless, the problem how to find a universal entangler remains open.",2007,2009-11-13,https://arxiv.org/abs/0704.1473,2
0709.1023,Constraint optimization and landscapes,"We describe an effective landscape introduced in [1] for the analysis of Constraint Satisfaction problems, such as Sphere Packing, K-SAT and Graph Coloring. This geometric construction reexpresses these problems in the more familiar terms of optimization in rugged energy landscapes. In particular, it allows one to understand the puzzling fact that unsophisticated programs are successful well beyond what was considered to be the `hard' transition, and suggests an algorithm defining a new, higher, easy-hard frontier.",2007,2008-09-25,https://arxiv.org/abs/0709.1023,1
0709.1699,Efficient Tabling Mechanisms for Transaction Logic Programs,"In this paper we present efficient evaluation algorithms for the Horn Transaction Logic (a generalization of the regular Horn logic programs with state updates). We present two complementary methods for optimizing the implementation of Transaction Logic. The first method is based on tabling and we modified the proof theory to table calls and answers on states (practically, equivalent to dynamic programming). The call-answer table is indexed on the call and a signature of the state in which the call was made. The answer columns contain the answer unification and a signature of the state after the call was executed. The states are signed efficiently using a technique based on tries and counting. The second method is based on incremental evaluation and it applies when the data oracle contains derived relations. The deletions and insertions (executed in the transaction oracle) change the state of the database. Using the heuristic of inertia (only a part of the state changes in response to elementary updates), most of the time it is cheaper to compute only the changes in the state than to recompute the entire state from scratch. The two methods are complementary by the fact that the first method optimizes the evaluation when a call is repeated in the same state, and the second method optimizes the evaluation of a new state when a call-state pair is not found by the tabling mechanism (i.e. the first method). The proof theory of Transaction Logic with the application of tabling and incremental evaluation is sound and complete with respect to its model theory.",2007,2007-09-12,https://arxiv.org/abs/0709.1699,1
1201.4551,Fossil fuel consumption and economic growth: causality relationship in the world,This paper has been withdrawn by the author due to some inaccurate descriptions in the section of INTRODUCTION and CONCLUSIONS.,2012,2013-06-19,https://arxiv.org/abs/1201.4551,3
0704.2471,Tropical spectral curves and integrable cellular automata,"We propose a method to study the integrable cellular automata with periodic boundary conditions, via the tropical spectral curve and its Jacobian. We introduce the tropical version of eigenvector map from the isolevel set to a divisor class on the tropical hyperelliptic curve. We also provide some conjectures related to the divisor class and the Jacobian. Finally, we apply our method to the periodic box and ball system and clarify the algebro-geometrical meaning of the real torus introduced for its initial value problem.",2007,2008-02-07,https://arxiv.org/abs/0704.2471,2
1307.7244,Extracting information from the signature of a financial data stream,"Market events such as order placement and order cancellation are examples of the complex and substantial flow of data that surrounds a modern financial engineer. New mathematical techniques, developed to describe the interactions of complex oscillatory systems (known as the theory of rough paths) provides new tools for analysing and describing these data streams and extracting the vital information. In this paper we illustrate how a very small number of coefficients obtained from the signature of financial data can be sufficient to classify this data for subtle underlying features and make useful predictions. This paper presents financial examples in which we learn from data and then proceed to classify fresh streams. The classification is based on features of streams that are specified through the coordinates of the signature of the path. At a mathematical level the signature is a faithful transform of a multidimensional time series. (Ben Hambly and Terry Lyons \cite{uniqueSig}), Hao Ni and Terry Lyons \cite{NiLyons} introduced the possibility of its use to understand financial data and pointed to the potential this approach has for machine learning and prediction. We evaluate and refine these theoretical suggestions against practical examples of interest and present a few motivating experiments which demonstrate information the signature can easily capture in a non-parametric way avoiding traditional statistical modelling of the data. In the first experiment we identify atypical market behaviour across standard 30-minute time buckets sampled from the WTI crude oil future market (NYMEX). The second and third experiments aim to characterise the market ""impact"" of and distinguish between parent orders generated by two different trade execution algorithms on the FTSE 100 Index futures market listed on NYSE Liffe.",2013,2014-07-16,https://arxiv.org/abs/1307.7244,2
1105.5850,Calibration and filtering for multi factor commodity models with seasonality: incorporating panel data from futures contracts,We examine a general multi-factor model for commodity spot prices and futures valuation. We extend the multi-factor long-short model in Schwartz and Smith (2000) and Yan (2002) in two important aspects: firstly we allow for both the long and short term dynamic factors to be mean reverting incorporating stochastic volatility factors and secondly we develop an additive structural seasonality model. Then a Milstein discretized non-linear stochastic volatility state space representation for the model is developed which allows for futures and options contracts in the observation equation. We then develop numerical methodology based on an advanced Sequential Monte Carlo algorithm utilising Particle Markov chain Monte Carlo to perform calibration of the model jointly with the filtering of the latent processes for the long-short dynamics and volatility factors. In this regard we explore and develop a novel methodology based on an adaptive Rao-Blackwellised version of the Particle Markov chain Monte Carlo methodology. In doing this we deal accurately with the non-linearities in the state-space model which are therefore introduced into the filtering framework. We perform analysis on synthetic and real data for oil commodities.,2011,2011-05-31,https://arxiv.org/abs/1105.5850,1
1003.3093,Relationship between preexponent and distribution over activation barrier energies for enzymatic reactions,A relationship between the preexponent of the rate constant and the distribution over activation barrier energies for enzymatic/protein reactions is revealed. We consider an enzyme solution as an ensemble of individual molecules with different values of the activation barrier energy described by the distribution. From the solvent viscosity effect on the preexponent we derive the integral equation for the distribution and find its approximate solution. Our approach enables us to attain a twofold purpose. On the one hand it yields a simple interpretation of the solvent viscosity dependence for enzymatic/protein reactions that requires neither a modification of the Kramers' theory nor that of the Stokes law. On the other hand our approach enables us to deduce the form of the distribution over activation barrier energies. The obtained function has a familiar bell-shaped form and is in qualitative agreement with the results of single enzyme kinetics measurements. General formalism is exemplified by the analysis of literature experimental data.,2010,2012-11-16,https://arxiv.org/abs/1003.3093,2
1710.10450,Shift-enabled graphs: Graphs where shift-invariant filters are representable as polynomials of shift operations,"In digital signal processing, shift-invariant filters can be represented as a polynomial expansion of a shift operation,that is, the Z-transform representation. When extended to graph signal processing (GSP), this would mean that a shift-invariant graph filter can be represented as a polynomial of the adjacency (shift) matrix of the graph. However, the characteristic and minimum polynomials of the adjacency matrix must be identical for the property to hold. While it has been suggested that this condition might be ignored as it is always possible to find a polynomial transform to represent the original adjacency matrix by another adjacency matrix that satisfies the condition, this letter shows that a filter that is shift invariant in terms of the original graph may not be shift invariant anymore under the modified graph and vice versa. We introduce the notion of ""shift-enabled graph"" for graphs that satisfy the aforementioned condition, and present a concrete example of a graph that is not ""shift-enabled"" and a shift-invariant filter that is not a polynomial of the shift operation matrix. The result provides a deeper understanding of shift-invariant filters when applied in GSP and shows that further investigation of shift-enabled graphs is needed to make it applicable to practical scenarios.",2017,2018-08-15,https://arxiv.org/abs/1710.10450,2
0704.1301,IRAS 18317-0757: A Cluster of Embedded Massive Stars and Protostars,"We present high-resolution, multiwavelength continuum and molecular-line images of the massive star forming region IRAS 18317-0757. The IR through mm spectral energy distribution can be approximated by a two-temperature model (25 and 63 K) with a total luminosity of approximately log(L/Lsun)=5.2. Previous submm imaging resolved this region into a cluster of 5 dust cores, one of which is associated with the UCHII region G23.96+0.15, and another with an H2O maser. In our new 2.7 mm continuum image, only the UCHII region is detected, with total flux and morphology in good agreement with the free-free emission in VLA cm-wave maps. For the other four objects, the nondetections at 2.7 mm and in the MSX mid-IR bands are consistent with cool dust emission with a temperature of 13-40K and luminosity of 1000-40000 Lsun. By combining single-dish and interferometric data, we have identified over two dozen virialized C18O cores in this region that contain ~40% of the total molecular gas mass present. While the overall extent of the C18O and dust emission is similar, the emission peaks do not correlate well in detail. At least 11 of the 123 stars identified by 2MASS in this region are likely to be within the star-forming cluster. Two stars (both associated with the UCHII region) were previously identified as O stars via IR spectroscopy. Most of the rest of the reddened stars have no obvious correlation with the C18O cores or the dust cores. In summary, our observations indicate that considerable fragmentation of the molecular cloud has taken place during the time required for the UCHII region to form and the O stars to become detectable at IR wavelengths. Additional star formation appears to be ongoing on the periphery of the central region, where up to four B-type (proto)stars have formed among a substantial number of C18O molecular cores.",2007,2009-11-16,https://arxiv.org/abs/0704.1301,1
0911.0280,Causal Inference on Discrete Data using Additive Noise Models,"Inferring the causal structure of a set of random variables from a finite sample of the joint distribution is an important problem in science. Recently, methods using additive noise models have been suggested to approach the case of continuous variables. In many situations, however, the variables of interest are discrete or even have only finitely many states. In this work we extend the notion of additive noise models to these cases. We prove that whenever the joint distribution $\prob^{(X,Y)}$ admits such a model in one direction, e.g. $Y=f(X)+N, N \independent X$, it does not admit the reversed model $X=g(Y)+\tilde N, \tilde N \independent Y$ as long as the model is chosen in a generic way. Based on these deliberations we propose an efficient new algorithm that is able to distinguish between cause and effect for a finite sample of discrete variables. In an extensive experimental study we show that this algorithm works both on synthetic and real data sets.",2009,2012-07-24,https://arxiv.org/abs/0911.0280,1
0912.1277,Linear model for fast background subtraction in oligonucleotide microarrays,One important preprocessing step in the analysis of microarray data is background subtraction. In high-density oligonucleotide arrays this is recognized as a crucial step for the global performance of the data analysis from raw intensities to expression values. We propose here an algorithm for background estimation based on a model in which the cost function is quadratic in a set of fitting parameters such that minimization can be performed through linear algebra. The model incorporates two effects: 1) Correlated intensities between neighboring features in the chip and 2) sequence-dependent affinities for non-specific hybridization fitted by an extended nearest-neighbor model. The algorithm has been tested on 360 GeneChips from publicly available data of recent expression experiments. The algorithm is fast and accurate. Strong correlations between the fitted values for different experiments as well as between the free-energy parameters and their counterparts in aqueous solution indicate that the model captures a significant part of the underlying physical chemistry.,2009,2009-12-08,https://arxiv.org/abs/0912.1277,1
0704.2002,Fluctuation dynamo and turbulent induction at low magnetic Prandtl numbers,"This paper is a detailed report on a programme of simulations used to settle a long-standing issue in the dynamo theory and demonstrate that the fluctuation dynamo exists in the limit of large magnetic Reynolds number Rm>>1 and small magnetic Prandtl number Pm<<1. The dependence of the critical Rm_c vs. the hydrodynamic Reynolds number Re is obtained for 1<Re<6700. In the limit Pm<<1, Rm_c is ~3 times larger than for Pm>1. The stability curve Rm_c(Re) (and, it is argued, the nature of the dynamo) is substantially different from the case of the simulations and liquid-metal experiments with a mean flow. It is not as yet possible to determine numerically whether the growth rate is ~Rm^{1/2} in the limit Re>>Rm>>1, as should be the case if the dynamo is driven by the inertial-range motions. The magnetic-energy spectrum in the low-Pm regime is qualitatively different from the Pm>1 case and appears to develop a negative spectral slope, although current resolutions are insufficient to determine its asymptotic form. At 1<Rm<Rm_c, the magnetic fluctuations induced via the tangling by turbulence of a weak mean field are investigated and the possibility of a k^{-1} spectrum above the resistive scale is examined. At low Rm<1, the induced fluctuations are well described by the quasistatic approximation; the k^{-11/3} spectrum is confirmed for the first time in direct numerical simulations.",2007,2014-11-18,https://arxiv.org/abs/0704.2002,1
0710.2604,Efficient Skyline Querying with Variable User Preferences on Nominal Attributes,"Current skyline evaluation techniques assume a fixed ordering on the attributes. However, dynamic preferences on nominal attributes are more realistic in known applications. In order to generate online response for any such preference issued by a user, we propose two methods of different characteristics. The first one is a semi-materialization method and the second is an adaptive SFS method. Finally, we conduct experiments to show the efficiency of our proposed algorithms.",2007,2007-10-16,https://arxiv.org/abs/0710.2604,1
0811.3514,The Alternative Choice of Constitutive Exons throughout Evolution,"Alternative cassette exons are known to originate from two processes exonization of intronic sequences and exon shuffling. Herein, we suggest an additional mechanism by which constitutively spliced exons become alternative cassette exons during evolution. We compiled a dataset of orthologous exons from human and mouse that are constitutively spliced in one species but alternatively spliced in the other. Examination of these exons suggests that the common ancestors were constitutively spliced. We show that relaxation of the 59 splice site during evolution is one of the molecular mechanisms by which exons shift from constitutive to alternative splicing. This shift is associated with the fixation of exonic splicing regulatory sequences (ESRs) that are essential for exon definition and control the inclusion level only after the transition to alternative splicing. The effect of each ESR on splicing and the combinatorial effects between two ESRs are conserved from fish to human. Our results uncover an evolutionary pathway that increases transcriptome diversity by shifting exons from constitutive to alternative splicing",2008,2008-11-24,https://arxiv.org/abs/0811.3514,1
1808.08786,Transparent Tx and Rx Waveform Processing for 5G New Radio Mobile Communications,"Several different waveform processing techniques have been studied and proposed for the 5G new radio (NR) physical layer, to support new mixed numerology and asynchronous services. The evaluation and comparison of these different techniques is commonly based on matched waveform processing in the transmitter and receiver units. In this article, it is shown that different techniques can be flexibly mixed, allowing to separately optimize complexity-performance trade-offs for transmitter and receiver implementations. Mixing of different waveform processing techniques is possible by setting adequate requirements for transmitter and receiver baseband processing allowing transparent design. The basic framework of how transmitter and receiver units can be independently applied and evaluated in the context of transparent design and an extensive set of examples of the achievable radio link performance with unmatched transmitter and receiver waveform processing are provided. The discussed approach and solutions simplify standardization, improve transparent transmitter and receiver coexistence, and allow future-proof evolution path for performance improvements in 5G NR physical layer hardware and algorithm design.",2018,2018-08-28,https://arxiv.org/abs/1808.08786,1
0705.3992,Average Stopping Set Weight Distribution of Redundant Random Matrix Ensembles,"In this paper, redundant random matrix ensembles (abbreviated as redundant random ensembles) are defined and their stopping set (SS) weight distributions are analyzed. A redundant random ensemble consists of a set of binary matrices with linearly dependent rows. These linearly dependent rows (redundant rows) significantly reduce the number of stopping sets of small size. An upper and lower bound on the average SS weight distribution of the redundant random ensembles are shown. From these bounds, the trade-off between the number of redundant rows (corresponding to decoding complexity of BP on BEC) and the critical exponent of the asymptotic growth rate of SS weight distribution (corresponding to decoding performance) can be derived. It is shown that, in some cases, a dense matrix with linearly dependent rows yields asymptotically (i.e., in the regime of small erasure probability) better performance than regular LDPC matrices with comparable parameters.",2007,2016-11-15,https://arxiv.org/abs/0705.3992,1
0711.0538,Spreadsheet Engineering: A Research Framework,"Spreadsheet engineering adapts the lessons of software engineering to spreadsheets, providing eight principles as a framework for organizing spreadsheet programming recommendations. Spreadsheets raise issues inadequately addressed by software engineering. Spreadsheets are a powerful modeling language, allowing strategic rapid model change, and enabling exploratory modeling. Spreadsheets users learn slowly with experience because they focus on the problem domain not programming. The heterogeneity of spreadsheet users requires a taxonomy to guide recommendations. Deployment of best practices is difficult and merits research.",2007,2024-12-31,https://arxiv.org/abs/0711.0538,1
1905.01798,Non-standard inference for augmented double autoregressive models with null volatility coefficients,"This paper considers an augmented double autoregressive (DAR) model, which allows null volatility coefficients to circumvent the over-parameterization problem in the DAR model. Since the volatility coefficients might be on the boundary, the statistical inference methods based on the Gaussian quasi-maximum likelihood estimation (GQMLE) become non-standard, and their asymptotics require the data to have a finite sixth moment, which narrows applicable scope in studying heavy-tailed data. To overcome this deficiency, this paper develops a systematic statistical inference procedure based on the self-weighted GQMLE for the augmented DAR model. Except for the Lagrange multiplier test statistic, the Wald, quasi-likelihood ratio and portmanteau test statistics are all shown to have non-standard asymptotics. The entire procedure is valid as long as the data is stationary, and its usefulness is illustrated by simulation studies and one real example.",2019,2019-05-07,https://arxiv.org/abs/1905.01798,1
1811.01980,Content-adaptive non-parametric texture similarity measure,"In this paper, we introduce a non-parametric texture similarity measure based on the singular value decomposition of the curvelet coefficients followed by a content-based truncation of the singular values. This measure focuses on images with repeating structures and directional content such as those found in natural texture images. Such textural content is critical for image perception and its similarity plays a vital role in various computer vision applications. In this paper, we evaluate the effectiveness of the proposed measure using a retrieval experiment. The proposed measure outperforms the state-of-the-art texture similarity metrics on CURet and PerTEx texture databases, respectively.",2018,2019-02-01,https://arxiv.org/abs/1811.01980,2
0905.4092,Information processing and signal integration in bacterial quorum sensing,"Bacteria communicate using secreted chemical signaling molecules called autoinducers in a process known as quorum sensing. The quorum-sensing network of the marine bacterium {\it Vibrio harveyi} employs three autoinducers, each known to encode distinct ecological information. Yet how cells integrate and interpret the information contained within the three autoinducer signals remains a mystery. Here, we develop a new framework for analyzing signal integration based on Information Theory and use it to analyze quorum sensing in {\it V. harveyi}. We quantify how much the cells can learn about individual autoinducers and explain the experimentally observed input-output relation of the {\it V. harveyi} quorum-sensing circuit. Our results suggest that the need to limit interference between input signals places strong constraints on the architecture of bacterial signal-integration networks, and that bacteria likely have evolved active strategies for minimizing this interference. Here we analyze two such strategies: manipulation of autoinducer production and feedback on receptor number ratios.",2009,2009-05-27,https://arxiv.org/abs/0905.4092,1
0710.1976,Solving Infinite Kolam in Knot Theory,"In south India, there are traditional patterns of line-drawings encircling dots, called ``Kolam'', among which one-line drawings or the ``infinite Kolam'' provide very interesting questions in mathematics. For example, we address the following simple question: how many patterns of infinite Kolam can we draw for a given grid pattern of dots? The simplest way is to draw possible patterns of Kolam while judging if it is infinite Kolam. Such a search problem seems to be NP complete. However, it is certainly not. In this paper, we focus on diamond-shaped grid patterns of dots, (1-3-5-3-1) and (1-3-5-7-5-3-1) in particular. By using the knot-theory description of the infinite Kolam, we show how to find the solution, which inevitably gives a sketch of the proof for the statement ``infinite Kolam is not NP complete.'' Its further discussion will be given in the final section.",2007,2012-02-23,https://arxiv.org/abs/0710.1976,1
0707.2090,A Training based Distributed Non-Coherent Space-Time Coding Strategy,"Unitary space-time modulation is known to be an efficient means to communicate over non-coherent Multiple Input Multiple Output (MIMO) channels. In this letter, differential unitary space-time coding and non-coherent space-time coding for the training based approach of Kim and Tarokh are addressed. For this approach, necessary and sufficient conditions for multi-group decodability are derived in a simple way assuming a Generalized Likelihood Ratio Test receiver and a unitary codebook. Extending Kim and Tarokh's approach for colocated MIMO systems, a novel training based approach to distributed non-coherent space-time coding for wireless relay networks is proposed. An explicit construction of two-group decodable distributed non-coherent space-time codes achieving full cooperative diversity for all even number of relays is provided.",2007,2007-07-17,https://arxiv.org/abs/0707.2090,1
1905.03959,Identifying Present-Bias from the Timing of Choices,"Timing decisions are common: when to file your taxes, finish a referee report, or complete a task at work. We ask whether time preferences can be inferred when \textsl{only} task completion is observed. To answer this question, we analyze the following model: each period a decision maker faces the choice whether to complete the task today or to postpone it to later. Cost and benefits of task completion cannot be directly observed by the analyst, but the analyst knows that net benefits are drawn independently between periods from a time-invariant distribution and that the agent has time-separable utility. Furthermore, we suppose the analyst can observe the agent's exact stopping probability. We establish that for any agent with quasi-hyperbolic $\beta,\delta$-preferences and given level of partial naivete $\hat{\beta}$, the probability of completing the task conditional on not having done it earlier increases towards the deadline. And conversely, for any given preference parameters $\beta,\delta$ and (weakly increasing) profile of task completion probability, there exists a stationary payoff distribution that rationalizes her behavior as long as the agent is either sophisticated or fully naive. An immediate corollary being that, without parametric assumptions, it is impossible to rule out time-consistency even when imposing an a priori assumption on the permissible long-run discount factor. We also provide an exact partial identification result when the analyst can, in addition to the stopping probability, observe the agent's continuation value.",2019,2019-05-13,https://arxiv.org/abs/1905.03959,1
0704.0539,Integral representations for convolutions of non-central multivariate gamma distributions,"Three types of integral representations for the cumulative distribution functions of convolutions of non-central p-variate gamma distributions are given by integration of elementary complex functions over the p-cube Cp = (-pi,pi]x...x(-pi,pi]. In particular, the joint distribution of the diagonal elements of a generalized quadratic form XAX' with n independent normally distributed column vectors in X is obtained. For a single p-variate gamma distribution function (p-1)-variate integrals over Cp-1 are derived. The integrals are numerically more favourable than integrals obtained from the Fourier or laplace inversion formula.",2007,2007-05-23,https://arxiv.org/abs/0704.0539,1
0704.2331,Symmetries in the system of type $D_4^{(1)}$,"In this paper, we propose a 4-parameter family of coupled Painlev\'e III systems in dimension four with affine Weyl group symmetry of type $D_4^{(1)}$. We also propose its symmetric form in which the $D_4^{(1)}$-symmetries become clearly visible.",2007,2007-05-23,https://arxiv.org/abs/0704.2331,1
1302.0574,Inflation-rate Derivatives: From Market Model to Foreign Currency Analogy,"In this paper, we establish a market model for the term structure of forward inflation rates based on the risk-neutral dynamics of nominal and real zero-coupon bonds. Under the market model, we can price inflation caplets as well as inflation swaptions with a formula similar to the Black's formula, thus justify the current market practice. We demonstrate how to further extend the market model to cope with volatility smiles. Moreover, we establish a consistency condition on the volatility of real zero-coupon bonds using arbitrage arguments, and with that re-derive the model of Jarrow and Yildirim (2003) with real forward rates based on ""foreign currency analogy"", and thus interconnect the two modeling paradigms.",2013,2013-02-05,https://arxiv.org/abs/1302.0574,1
0704.0941,Recovering galaxy star formation and metallicity histories from spectra using VESPA,"We introduce VErsatile SPectral Analysis (VESPA): a new method which aims to recover robust star formation and metallicity histories from galactic spectra. VESPA uses the full spectral range to construct a galaxy history from synthetic models. We investigate the use of an adaptative parametrization grid to recover reliable star formation histories on a galaxy-by-galaxy basis. Our goal is robustness as opposed to high resolution histories, and the method is designed to return high time resolution only where the data demand it. In this paper we detail the method and we present our findings when we apply VESPA to synthetic and real Sloan Digital Sky Survey (SDSS) spectroscopic data. We show that the number of parameters that can be recovered from a spectrum depends strongly on the signal-to-noise, wavelength coverage and presence or absence of a young population. For a typical SDSS sample of galaxies, we can normally recover between 2 to 5 stellar populations. We find very good agreement between VESPA and our previous analysis of the SDSS sample with MOPED.",2007,2009-11-13,https://arxiv.org/abs/0704.0941,2
0704.0717,Incommmensurability and unconventional superconductor to insulator transition in the Hubbard model with bond-charge interaction,"We determine the quantum phase diagram of the one-dimensional Hubbard model with bond-charge interaction X in addition to the usual Coulomb repulsion U at half-filling. For large enough X and positive U the model shows three phases. For large U the system is in the spin-density wave phase already known in the usual Hubbard model. As U decreases, there is first a spin transition to a spontaneously dimerized bond-ordered wave phase and then a charge transition to a novel phase in which the dominant correlations at large distances correspond to an incommensurate singlet superconductor.",2007,2007-11-14,https://arxiv.org/abs/0704.0717,2
1810.13237,Machine Learning Estimation of Heterogeneous Causal Effects: Empirical Monte Carlo Evidence,"We investigate the finite sample performance of causal machine learning estimators for heterogeneous causal effects at different aggregation levels. We employ an Empirical Monte Carlo Study that relies on arguably realistic data generation processes (DGPs) based on actual data. We consider 24 different DGPs, eleven different causal machine learning estimators, and three aggregation levels of the estimated effects. In the main DGPs, we allow for selection into treatment based on a rich set of observable covariates. We provide evidence that the estimators can be categorized into three groups. The first group performs consistently well across all DGPs and aggregation levels. These estimators have multiple steps to account for the selection into the treatment and the outcome process. The second group shows competitive performance only for particular DGPs. The third group is clearly outperformed by the other estimators.",2018,2021-10-19,https://arxiv.org/abs/1810.13237,2
1808.00166,Focused blind deconvolution,"We introduce a novel multichannel blind deconvolution (BD) method that extracts sparse and front-loaded impulse responses from the channel outputs, i.e., their convolutions with a single arbitrary source. A crucial feature of this formulation is that it doesn't encode support restrictions on the unknowns, unlike most prior work on BD. The indeterminacy inherent to BD, which is difficult to resolve with a traditional L1 penalty on the impulse responses, is resolved in our method because it seeks a first approximation where the impulse responses are: ""maximally white"" -- encoded as the energy focusing near zero lag of the impulse-response auto-correlations; and ""maximally front-loaded"" -- encoded as the energy focusing near zero time of the impulse responses. Hence we call the method focused blind deconvolution (FBD). The focusing constraints are relaxed as the iterations progress. Note that FBD requires the duration of the channel outputs to be longer than that of the unknown impulse responses. A multichannel blind deconvolution problem that is appropriately formulated by sparse and front-loaded impulse responses arises in seismic inversion, where the impulse responses are the Green's function evaluations at different receiver locations, and the operation of a drill bit inputs the noisy and correlated source signature into the subsurface. We demonstrate the benefits of FBD using seismic-while-drilling numerical experiments, where the noisy data recorded at the receivers are hard to interpret, but FBD can provide the processing essential to separate the drill-bit (source) signature from the interpretable Green's function.",2018,2019-05-22,https://arxiv.org/abs/1808.00166,1
0704.0531,Gravitational Duality Transformations on (A)dS4,"We discuss the implementation of electric-magnetic duality transformations in four-dimensional gravity linearized around Minkowski or (A)dS4 backgrounds. In the presence of a cosmological constant duality generically modifies the Hamiltonian, nevertheless the bulk dynamics is unchanged. We pay particular attention to the boundary terms generated by the duality transformations and discuss their implications for holography.",2007,2008-11-26,https://arxiv.org/abs/0704.0531,3
1803.06401,Evaluating Conditional Cash Transfer Policies with Machine Learning Methods,"This paper presents an out-of-sample prediction comparison between major machine learning models and the structural econometric model. Over the past decade, machine learning has established itself as a powerful tool in many prediction applications, but this approach is still not widely adopted in empirical economic studies. To evaluate the benefits of this approach, I use the most common machine learning algorithms, CART, C4.5, LASSO, random forest, and adaboost, to construct prediction models for a cash transfer experiment conducted by the Progresa program in Mexico, and I compare the prediction results with those of a previous structural econometric study. Two prediction tasks are performed in this paper: the out-of-sample forecast and the long-term within-sample simulation. For the out-of-sample forecast, both the mean absolute error and the root mean square error of the school attendance rates found by all machine learning models are smaller than those found by the structural model. Random forest and adaboost have the highest accuracy for the individual outcomes of all subgroups. For the long-term within-sample simulation, the structural model has better performance than do all of the machine learning models. The poor within-sample fitness of the machine learning model results from the inaccuracy of the income and pregnancy prediction models. The result shows that the machine learning model performs better than does the structural model when there are many data to learn; however, when the data are limited, the structural model offers a more sensible prediction. The findings of this paper show promise for adopting machine learning in economic policy analyses in the era of big data.",2018,2018-03-20,https://arxiv.org/abs/1803.06401,1
0704.0533,Non-resonant and Resonant X-ray Scattering Studies on Multiferroic TbMn2O5,"Comprehensive x-ray scattering studies, including resonant scattering at Mn L-edge, Tb L- and M-edges, were performed on single crystals of TbMn2O5. X-ray intensities were observed at a forbidden Bragg position in the ferroelectric phases, in addition to the lattice and the magnetic modulation peaks. Temperature dependences of their intensities and the relation between the modulation wave vectors provide direct evidences of exchange striction induced ferroelectricity. Resonant x-ray scattering results demonstrate the presence of multiple magnetic orders by exhibiting their different temperature dependences. The commensurate-to-incommensurate phase transition around 24 K is attributed to discommensuration through phase slipping of the magnetic orders in spin frustrated geometries. We proposed that the low temperature incommensurate phase consists of the commensurate magnetic domains separated by anti-phase domain walls which reduce spontaneous polarizations abruptly at the transition.",2007,2009-11-13,https://arxiv.org/abs/0704.0533,1
1302.3771,Pricing Step Options under the CEV and other Solvable Diffusion Models,"We consider a special family of occupation-time derivatives, namely proportional step options introduced by Linetsky in [Math. Finance, 9, 55--96 (1999)]. We develop new closed-form spectral expansions for pricing such options under a class of nonlinear volatility diffusion processes which includes the constant-elasticity-of-variance (CEV) model as an example. In particular, we derive a general analytically exact expression for the resolvent kernel (i.e. Green's function) of such processes with killing at an exponential stopping time (independent of the process) of occupation above or below a fixed level. Moreover, we succeed in Laplace inverting the resolvent kernel and thereby derive newly closed-form spectral expansion formulae for the transition probability density of such processes with killing. The spectral expansion formulae are rapidly convergent and easy-to-implement as they are based simply on knowledge of a pair of fundamental solutions for an underlying solvable diffusion process. We apply the spectral expansion formulae to the pricing of proportional step options for four specific families of solvable nonlinear diffusion asset price models that include the CEV diffusion model and three other multi-parameter state-dependent local volatility confluent hypergeometric diffusion processes.",2013,2013-02-18,https://arxiv.org/abs/1302.3771,1
1805.11932,How do public research labs use funding for research? A case study,"This paper discusses how public research organizations consume funding for research, applying a new approach based on economic metabolism of research labs, in a broad analogy with biology. This approach is applied to a case study in Europe represented by one of the biggest European public research organizations, the National Research council of Italy. Results suggest that funding for research (state subsidy and public contracts) of this public research organization is mainly consumed for the cost of personnel. In addition, the analysis shows a disproportionate growth of the cost of personnel in public research labs in comparison with total revenue from government. In the presence of shrinking public research lab budgets, this organizational behavior generates inefficiencies and stress. R&D management and public policy implications are suggested for improving economic performance of public research organizations in turbulent markets.",2018,2018-07-23,https://arxiv.org/abs/1805.11932,1
1810.07674,Dynkin games with incomplete and asymmetric information,"We study the value and the optimal strategies for a two-player zero-sum optimal stopping game with incomplete and asymmetric information. In our Bayesian set-up, the drift of the underlying diffusion process is unknown to one player (incomplete information feature), but known to the other one (asymmetric information feature). We formulate the problem and reduce it to a fully Markovian setup where the uninformed player optimises over stopping times and the informed one uses randomised stopping times in order to hide their informational advantage. Then we provide a general verification result which allows us to find the value of the game and players' optimal strategies by solving suitable quasi-variational inequalities with some non-standard constraints. Finally, we study an example with linear payoffs, in which an explicit solution of the corresponding quasi-variational inequalities can be obtained.",2018,2020-07-15,https://arxiv.org/abs/1810.07674,5
1406.0044,Can Turnover Go to Zero?,"Internal crossing of trades between multiple alpha streams results in portfolio turnover reduction. Turnover reduction can be modeled using the correlation structure of the alpha streams. As more and more alphas are added, generally turnover reduces. In this note we use a factor model approach to address the question of whether the turnover goes to zero or a finite limit as the number of alphas N goes to infinity. We argue that the limiting turnover value is determined by the number of alpha clusters F, not the number of alphas N. This limiting value behaves according to the ""power law"" ~ F^(-3/2). So, to achieve zero limiting turnover, the number of alpha clusters must go to infinity along with the number of alphas. We further argue on general grounds that, if the number of underlying tradable instruments is finite, then the turnover cannot go to zero, which implies that the number of alpha clusters also appears to be finite.",2014,2014-11-10,https://arxiv.org/abs/1406.0044,4
0901.1598,constNJ: an algorithm to reconstruct sets of phylogenetic trees satisfying pairwise topological constraints,"This paper introduces constNJ, the first algorithm for phylogenetic reconstruction of sets of trees with constrained pairwise rooted subtree-prune regraft (rSPR) distance. We are motivated by the problem of constructing sets of trees which must fit into a recombination, hybridization, or similar network. Rather than first finding a set of trees which are optimal according to a phylogenetic criterion (e.g. likelihood or parsimony) and then attempting to fit them into a network, constNJ estimates the trees while enforcing specified rSPR distance constraints. The primary input for constNJ is a collection of distance matrices derived from sequence blocks which are assumed to have evolved in a tree-like manner, such as blocks of an alignment which do not contain any recombination breakpoints. The other input is a set of rSPR constraints for any set of pairs of trees. ConstNJ is consistent and a strict generalization of the neighbor-joining algorithm; it uses the new notion of ""maximum agreement partitions"" to assure that the resulting trees satisfy the given rSPR distance constraints.",2009,2009-09-30,https://arxiv.org/abs/0901.1598,2
0711.2652,An information-theoretic analog of a result of Perelman,"Each compact manifold M of finite dimension k is differentiable and supports an intrinsic probability measure. There then exists a measurable transformation of M to the k-dimensional ""surface"" of the (k+1)-dimensional ball.",2007,2007-11-19,https://arxiv.org/abs/0711.2652,1
0802.4259,Molecular Systems with Infinite and Finite Degrees of Freedom. Part I: Multi-Scale Analysis,"The paper analyses stochastic systems describing reacting molecular systems with a combination of two types of state spaces, a finite-dimensional, and an infinite dimenional part. As a typical situation consider the interaction of larger macro-molecules, finite and small in numbers per cell (like protein complexes), with smaller, very abundant molecules, for example metabolites. We study the construction of the continuum approximation of the associated Master Equation (ME) by using the Trotter approximation [27]. The continuum limit shows regimes where the finite degrees of freedom evolve faster than the infinite ones. Then we develop a rigourous asymptotic adiabatic theory upon the condition that the jump process arising from the finite degrees of freedom of the Markov Chain (MC, typically describing conformational changes of the macro-molecules) occurs with large frequency. In a second part of this work, the theory is applied to derive typical enzyme kinetics in an alternative way and interpretation within this framework.",2008,2009-09-29,https://arxiv.org/abs/0802.4259,2
0910.1656,"Non-Euclidean statistics for covariance matrices, with applications to diffusion tensor imaging","The statistical analysis of covariance matrix data is considered and, in particular, methodology is discussed which takes into account the non-Euclidean nature of the space of positive semi-definite symmetric matrices. The main motivation for the work is the analysis of diffusion tensors in medical image analysis. The primary focus is on estimation of a mean covariance matrix and, in particular, on the use of Procrustes size-and-shape space. Comparisons are made with other estimation techniques, including using the matrix logarithm, matrix square root and Cholesky decomposition. Applications to diffusion tensor imaging are considered and, in particular, a new measure of fractional anisotropy called Procrustes Anisotropy is discussed.",2009,2009-10-12,https://arxiv.org/abs/0910.1656,1
1002.3315,High-dimensional variable selection for Cox's proportional hazards model,"Variable selection in high dimensional space has challenged many contemporary statistical problems from many frontiers of scientific disciplines. Recent technology advance has made it possible to collect a huge amount of covariate information such as microarray, proteomic and SNP data via bioimaging technology while observing survival information on patients in clinical studies. Thus, the same challenge applies to the survival analysis in order to understand the association between genomics information and clinical information about the survival time. In this work, we extend the sure screening procedure Fan and Lv (2008) to Cox's proportional hazards model with an iterative version available. Numerical simulation studies have shown encouraging performance of the proposed method in comparison with other techniques such as LASSO. This demonstrates the utility and versatility of the iterative sure independent screening scheme.",2010,2010-05-20,https://arxiv.org/abs/1002.3315,2
0811.3499,Kernel Regression by Mode Calculation of the Conditional Probability Distribution,"The most direct way to express arbitrary dependencies in datasets is to estimate the joint distribution and to apply afterwards the argmax-function to obtain the mode of the corresponding conditional distribution. This method is in practice difficult, because it requires a global optimization of a complicated function, the joint distribution by fixed input variables. This article proposes a method for finding global maxima if the joint distribution is modeled by a kernel density estimation. Some experiments show advantages and shortcomings of the resulting regression method in comparison to the standard Nadaraya-Watson regression technique, which approximates the optimum by the expectation value.",2008,2008-11-24,https://arxiv.org/abs/0811.3499,1
0811.1005,Non-driven polymer translocation through a nanopore: computational evidence that the escape and relaxation processes are coupled,"Most of the theoretical models describing the translocation of a polymer chain through a nanopore use the hypothesis that the polymer is always relaxed during the complete process. In other words, models generally assume that the characteristic relaxation time of the chain is small enough compared to the translocation time that non-equilibrium molecular conformations can be ignored. In this paper, we use Molecular Dynamics simulations to directly test this hypothesis by looking at the escape time of unbiased polymer chains starting with different initial conditions. We find that the translocation process is not quite in equilibrium for the systems studied, even though the translocation time tau is about 10 times larger than the relaxation time tau_r. Our most striking result is the observation that the last half of the chain escapes in less than ~12% of the total escape time, which implies that there is a large acceleration of the chain at the end of its escape from the channel.",2008,2009-11-13,https://arxiv.org/abs/0811.1005,1
0708.0741,Characterising Web Site Link Structure,"The topological structures of the Internet and the Web have received considerable attention. However, there has been little research on the topological properties of individual web sites. In this paper, we consider whether web sites (as opposed to the entire Web) exhibit structural similarities. To do so, we exhaustively crawled 18 web sites as diverse as governmental departments, commercial companies and university departments in different countries. These web sites consisted of as little as a few thousand pages to millions of pages. Statistical analysis of these 18 sites revealed that the internal link structure of the web sites are significantly different when measured with first and second-order topological properties, i.e. properties based on the connectivity of an individual or a pairs of nodes. However, examination of a third-order topological property that consider the connectivity between three nodes that form a triangle, revealed a strong correspondence across web sites, suggestive of an invariant. Comparison with the Web, the AS Internet, and a citation network, showed that this third-order property is not shared across other types of networks. Nor is the property exhibited in generative network models such as that of Barabasi and Albert.",2007,2016-11-17,https://arxiv.org/abs/0708.0741,1
0704.0740,Vertical dynamics of disk galaxies in MOND,"We investigate the possibility of discriminating between Modified Newtonian Dynamics (MOND) and Newtonian gravity with dark matter, by studying the vertical dynamics of disk galaxies. We consider models with the same circular velocity in the equatorial plane (purely baryonic disks in MOND and the same disks in Newtonian gravity embedded in spherical dark matter haloes), and we construct their intrinsic and projected kinematical fields by solving the Jeans equations under the assumption of a two-integral distribution function. We found that the vertical velocity dispersion of deep-MOND disks can be much larger than in the equivalent spherical Newtonian models. However, in the more realistic case of high-surface density disks this effect is significantly reduced, casting doubts on the possibility of discriminating between MOND and Newtonian gravity with dark matter by using current observations.",2007,2009-06-23,https://arxiv.org/abs/0704.0740,2
1711.05596,Pitch and timbre discrimination at wave-to-spike transition in the cochlea,"A new definition of musical pitch is proposed. A Finite-Difference Time Domain (FDTM) model of the cochlea is used to calculate spike trains caused by tone complexes and by a recorded classical guitar tone. All harmonic tone complexes, musical notes, show a narrow-band Interspike Interval (ISI) pattern at the respective fundamental frequency of the tone complex. Still this fundamental frequency is not only present at the bark band holding the respective best frequency of this fundamental frequency, but rather at all bark bands driven by the tone complex partials. This is caused by drop-outs in the basically regular, periodic spike train in the respective bands. These drop-outs are caused by the energy distribution in the wave form, where time spans of low energy are not able to drive spikes. The presence of the fundamental periodicity in all bark bands can be interpreted as pitch. Contrary to pitch, timbre is represented as a wide distribution of different ISIs over bark bands. The definition of pitch is shown to also works with residue pitches. The spike drop-outs in times of low energy of the wave form also cause undertones, integer multiple subdivisions in periodicity, but in no case overtones can appear. This might explain the musical minor scale, which was proposed to be caused by undertones already in 1880 by Hugo Riemann, still until now without knowledge about any physical realization of such undertones.",2017,2017-11-16,https://arxiv.org/abs/1711.05596,1
0705.0100,On Hadwiger Conjecture,We propose an algorithm to reduce a k-chromatic graph to a complete graph of largest possible order through a well defined sequence of contractions. We introduce a new matrix called transparency matrix and state its properties. We then define correct contraction procedure to be executed to get largest possible complete graph from given connected graph. We finally give a characterization for k-chromatic graphs and use it to settle Hadwigers conjecture.,2007,2007-06-21,https://arxiv.org/abs/0705.0100,5
1005.1514,PIntron: a Fast Method for Gene Structure Prediction via Maximal Pairings of a Pattern and a Text,"Current computational methods for exon-intron structure prediction from a cluster of transcript (EST, mRNA) data do not exhibit the time and space efficiency necessary to process large clusters of over than 20,000 ESTs and genes longer than 1Mb. Guaranteeing both accuracy and efficiency seems to be a computational goal quite far to be achieved, since accuracy is strictly related to exploiting the inherent redundancy of information present in a large cluster. We propose a fast method for the problem that combines two ideas: a novel algorithm of proved small time complexity for computing spliced alignments of a transcript against a genome, and an efficient algorithm that exploits the inherent redundancy of information in a cluster of transcripts to select, among all possible factorizations of EST sequences, those allowing to infer splice site junctions that are highly confirmed by the input data. The EST alignment procedure is based on the construction of maximal embeddings that are sequences obtained from paths of a graph structure, called Embedding Graph, whose vertices are the maximal pairings of a genomic sequence T and an EST P. The procedure runs in time linear in the size of P, T and of the output. PIntron, the software tool implementing our methodology, is able to process in a few seconds some critical genes that are not manageable by other gene structure prediction tools. At the same time, PIntron exhibits high accuracy (sensitivity and specificity) when compared with ENCODE data. Detailed experimental data, additional results and PIntron software are available at http://www.algolab.eu/PIntron.",2010,2010-05-11,https://arxiv.org/abs/1005.1514,1
0704.1225,Patterns of dominant flows in the world trade web,"The large-scale organization of the world economies is exhibiting increasingly levels of local heterogeneity and global interdependency. Understanding the relation between local and global features calls for analytical tools able to uncover the global emerging organization of the international trade network. Here we analyze the world network of bilateral trade imbalances and characterize its overall flux organization, unraveling local and global high-flux pathways that define the backbone of the trade system. We develop a general procedure capable to progressively filter out in a consistent and quantitative way the dominant trade channels. This procedure is completely general and can be applied to any weighted network to detect the underlying structure of transport flows. The trade fluxes properties of the world trade web determines a ranking of trade partnerships that highlights global interdependencies, providing information not accessible by simple local analysis. The present work provides new quantitative tools for a dynamical approach to the propagation of economic crises.",2007,2008-12-10,https://arxiv.org/abs/0704.1225,1
0705.1789,Random Linear Network Coding: A free cipher?,"We consider the level of information security provided by random linear network coding in network scenarios in which all nodes comply with the communication protocols yet are assumed to be potential eavesdroppers (i.e. ""nice but curious""). For this setup, which differs from wiretapping scenarios considered previously, we develop a natural algebraic security criterion, and prove several of its key properties. A preliminary analysis of the impact of network topology on the overall network coding security, in particular for complete directed acyclic graphs, is also included.",2007,2007-07-13,https://arxiv.org/abs/0705.1789,1
0909.1690,The scale of market quakes,"We define a methodology to quantify market activity on a 24 hour basis by defining a scale, the so-called scale of market quakes (SMQ). The SMQ is designed within a framework where we analyse the dynamics of excess price moves from one directional change of price to the next. We use the SMQ to quantify the FX market and evaluate the performance of the proposed methodology at major news announcements. The evolution of SMQ magnitudes from 2003 to 2009 is analysed across major currency pairs.",2009,2009-09-10,https://arxiv.org/abs/0909.1690,1
0704.1831,Multiplicity Fluctuations in Au+Au Collisions at RHIC,The preliminary data of the PHENIX collaboration for the scaled variances of charged hadron multiplicity fluctuations in Au+Au at $\sqrt{s}=200$ GeV are analyzed within the model of independent sources. We use the HSD transport model to calculate the participant number fluctuations and the number of charged hadrons per nucleon participant in different centrality bins. This combined picture leads to a good agreement with the PHENIX data and suggests that the measured multiplicity fluctuations result dominantly from participant number fluctuations. The role of centrality selection and acceptance is discussed separately.,2007,2008-11-26,https://arxiv.org/abs/0704.1831,1
1101.0947,Subsampling Methods for genomic inference,"Large-scale statistical analysis of data sets associated with genome sequences plays an important role in modern biology. A key component of such statistical analyses is the computation of $p$-values and confidence bounds for statistics defined on the genome. Currently such computation is commonly achieved through ad hoc simulation measures. The method of randomization, which is at the heart of these simulation procedures, can significantly affect the resulting statistical conclusions. Most simulation schemes introduce a variety of hidden assumptions regarding the nature of the randomness in the data, resulting in a failure to capture biologically meaningful relationships. To address the need for a method of assessing the significance of observations within large scale genomic studies, where there often exists a complex dependency structure between observations, we propose a unified solution built upon a data subsampling approach. We propose a piecewise stationary model for genome sequences and show that the subsampling approach gives correct answers under this model. We illustrate the method on three simulation studies and two real data examples.",2011,2011-01-06,https://arxiv.org/abs/1101.0947,1
0704.1461,Trajectory of neutron$-$neutron$-^{18}C$ excited three-body state,"The trajectory of the first excited Efimov state is investigated by using a renormalized zero-range three-body model for a system with two bound and one virtual two-body subsystems. The approach is applied to $n-n-^{18}$C, where the $n-n$ virtual energy and the three-body ground state are kept fixed. It is shown that such three-body excited state goes from a bound to a virtual state when the $n-^{18}$C binding energy is increased. Results obtained for the $n-^{19}$C elastic cross-section at low energies also show dominance of an $S-$matrix pole corresponding to a bound or virtual Efimov state. It is also presented a brief discussion of these findings in the context of ultracold atom physics with tunable scattering lengths.",2007,2011-03-28,https://arxiv.org/abs/0704.1461,4
1803.01141,An Application of Neural Networks to Channel Estimation of the ISDB-TB FBMC System,"Due to the evolution of technology and the diffusion of digital television, many researchers are studying more efficient transmission and reception methods. This fact occurs because of the demand of transmitting videos with better quality using new standards such 8K SUPER Hi-VISION. In this scenario, modulation techniques such as Filter Bank Multi Carrier, associated with advanced coding and synchronization methods, are being applied, aiming to achieve the desired data rate to support ultra-high definition videos. Simultaneously, it is also important to investigate ways of channel estimation that enable a better reception of the transmitted signal. This task is not always trivial, depending on the characteristics of the channel. Thus, the use of artificial intelligence can contribute to estimate the channel frequency response, from the transmitted pilots. A classical algorithm called Back-propagation Training can be applied to find the channel equalizer coefficients, making possible the correct reception of TV signals. Therefore, this work presents a method of channel estimation that uses neural network techniques to obtain the channel response in the Brazilian Digital System Television, called ISDB-TB, using Filter Bank Multi Carrier.",2018,2018-03-06,https://arxiv.org/abs/1803.01141,1
0704.1472,Emergence of U(1) symmetry in the 3D XY model with Zq anisotropy,"We study the three-dimensional XY model with a Z_q anisotropic term. At temperatures T < Tc this dangerously irrelevant perturbation is relevant only above a length scale Lambda, which diverges as a power of the correlation length; Lambda ~ xi^a_q. Below Lambda the order parameter is U(1) symmetric. We derive the full scaling function controlling the emergence of U(1) symmetry and use Monte Carlo results to extract the exponent a_q for q=4,...,8. We find that a_q = a_4 (q/4)^2, with a_4 only marginally larger than 1. We discuss these results in the context of U(1) symmetry at ""deconfined"" quantum critical points separating antiferromagnetic and valence-bond-solid states in quantum spin systems.",2007,2010-10-13,https://arxiv.org/abs/0704.1472,2
0705.0506,Space-time percolation,"The contact model for the spread of disease may be viewed as a directed percolation model on $\ZZ \times \RR$ in which the continuum axis is oriented in the direction of increasing time. Techniques from percolation have enabled a fairly complete analysis of the contact model at and near its critical point. The corresponding process when the time-axis is unoriented is an undirected percolation model to which now standard techniques may be applied. One may construct in similar vein a random-cluster model on $\ZZ \times \RR$, with associated continuum Ising and Potts models. These models are of independent interest, in addition to providing a path-integral representation of the quantum Ising model with transverse field. This representation may be used to obtain a bound on the entanglement of a finite set of spins in the quantum Ising model on $\ZZ$, where this entanglement is measured via the entropy of the reduced density matrix. The mean-field version of the quantum Ising model gives rise to a random-cluster model on $K_n \times \RR$, thereby extending the Erdos-Renyi random graph on the complete graph $K_n$.",2007,2007-05-23,https://arxiv.org/abs/0705.0506,1
0806.0744,Bacterial tracking of motile algae assisted by algal cell's vorticity field,"Previously published experimental work by other authors has shown that certain motile marine bacteria are able to track free swimming algae by executing a zigzag path and steering toward the algae at each turn. Here, we propose that the apparent steering behaviour could be a hydrodynamic effect, whereby an algal cell's vorticity and strain-rate fields rotate a pursuing bacterial cell in the appropriate direction. Using simplified models for the bacterial and algal cells, we numerically compute the trajectory of a bacterial cell and demonstrate the plausibility of this hypothesis.",2008,2008-06-05,https://arxiv.org/abs/0806.0744,1
1811.02214,Cuffless Blood Pressure Estimation from Electrocardiogram and Photoplethysmogram Using Waveform Based ANN-LSTM Network,"Goal: Although photoplethysmogram (PPG) and electrocardiogram (ECG) signals can be used to estimate blood pressure (BP) by extracting various features, the changes in morphological contours of both PPG and ECG signals due to various diseases of circulatory system and interaction of other physiological systems make the extraction of such features very difficult. Methods: In this work, we propose a waveform-based hierarchical Artificial Neural Network - Long Short Term Memory (ANN-LSTM) model for BP estimation. The model consists of two hierarchy levels, where the lower hierarchy level uses ANNs to extract necessary morphological features from ECG and PPG waveforms and the upper hierarchy level uses LSTM layers to account for the time domain variation of the features extracted by lower hierarchy level. Results: The proposed model is evaluated on 39 subjects using the Association for the Advancement of Medical Instrumentations (AAMI) standard and the British Hypertension Society (BHS) standard. The method satisfies both the standards in the estimation of systolic blood pressure (SBP) and diastolic blood pressure (DBP). For the proposed network, the mean absolute error (MAE) and the root mean square error (RMSE) for SBP estimation are 1.10 and 1.56 mmHg, respectively, and for DBP estimation are 0.58 and 0.85 mmHg, respectively. Conclusion: The performance of the proposed hierarchical ANN-LSTM model is found to be better than the other feature engineering-based networks. It is shown that the proposed model is able to automatically extract the necessary features and their time domain variations to estimate BP reliably in a noninvasive continuous manner. Significance: The method is expected to greatly facilitate the presently available mobile health-care gadgets in continuous BP estimation.",2018,2018-11-07,https://arxiv.org/abs/1811.02214,1
1908.09976,Optimal life-cycle consumption and investment decisions under age-dependent risk preferences,"In this article we solve the problem of maximizing the expected utility of future consumption and terminal wealth to determine the optimal pension or life-cycle fund strategy for a cohort of pension fund investors. The setup is strongly related to a DC pension plan where additionally (individual) consumption is taken into account. The consumption rate is subject to a time-varying minimum level and terminal wealth is subject to a terminal floor. Moreover, the preference between consumption and terminal wealth as well as the intertemporal coefficient of risk aversion are time-varying and therefore depend on the age of the considered pension cohort. The optimal consumption and investment policies are calculated in the case of a Black-Scholes financial market framework and hyperbolic absolute risk aversion (HARA) utility functions. We generalize Ye (2008) (2008 American Control Conference, 356-362) by adding an age-dependent coefficient of risk aversion and extend Steffensen (2011) (Journal of Economic Dynamics and Control, 35(5), 659-667), Hentschel (2016) (Doctoral dissertation, Ulm University) and Aase (2017) (Stochastics, 89(1), 115-141) by considering consumption in combination with terminal wealth and allowing for consumption and terminal wealth floors via an application of HARA utility functions. A case study on fitting several models to realistic, time-dependent life-cycle consumption and relative investment profiles shows that only our extended model with time-varying preference parameters provides sufficient flexibility for an adequate fit. This is of particular interest to life-cycle products for (private) pension investments or pension insurance in general.",2019,2020-08-03,https://arxiv.org/abs/1908.09976,1
1706.05982,"On Heckits, LATE, and Numerical Equivalence","Structural econometric methods are often criticized for being sensitive to functional form assumptions. We study parametric estimators of the local average treatment effect (LATE) derived from a widely used class of latent threshold crossing models and show they yield LATE estimates algebraically equivalent to the instrumental variables (IV) estimator. Our leading example is Heckman's (1979) two-step (""Heckit"") control function estimator which, with two-sided non-compliance, can be used to compute estimates of a variety of causal parameters. Equivalence with IV is established for a semi-parametric family of control function estimators and shown to hold at interior solutions for a class of maximum likelihood estimators. Our results suggest differences between structural and IV estimates often stem from disagreements about the target parameter rather than from functional form assumptions per se. In cases where equivalence fails, reporting structural estimates of LATE alongside IV provides a simple means of assessing the credibility of structural extrapolation exercises.",2017,2018-10-26,https://arxiv.org/abs/1706.05982,4
1805.05533,"Discovering Transforms: A Tutorial on Circulant Matrices, Circular Convolution, and the Discrete Fourier Transform","How could the Fourier and other transforms be naturally discovered if one didn't know how to postulate them? In the case of the Discrete Fourier Transform (DFT), we show how it arises naturally out of analysis of circulant matrices. In particular, the DFT can be derived as the change of basis that simultaneously diagonalizes all circulant matrices. In this way, the DFT arises naturally from a linear algebra question about a set of matrices. Rather than thinking of the DFT as a signal transform, it is more natural to think of it as a single change of basis that renders an entire set of mutually-commuting matrices into simple, diagonal forms. The DFT can then be ""discovered"" by solving the eigenvalue/eigenvector problem for a special element in that set. A brief outline is given of how this line of thinking can be generalized to families of linear operators, leading to the discovery of the other common Fourier-type transforms, as well as its connections with group representations theory.",2018,2022-04-27,https://arxiv.org/abs/1805.05533,4
1901.04265,Designing An Industrial Policy For Developing Countries: A New Approach,"In this study, the prevalent methodology for design of the industrial policy in developing countries was critically assessed, and it was shown that the mechanism and content of classical method is fundamentally contradictory to the goals and components of the endogenous growth theories. This study, by proposing a new approach, along settling Schumpeter's economic growth theory as a policy framework, designed the process of entering, analyzing and processing data as the mechanism of the industrial policy in order to provide ""theoretical consistency"" and ""technical and Statistical requirements"" for targeting the growth stimulant factor effectively.",2019,2019-01-15,https://arxiv.org/abs/1901.04265,1
1007.0471,Technical performance and interpretation of physical experiment in problems of cell biology,"The lecture summarises main results of my team over last five years in the field of technical experiment design and interpretation of results of experiments for cell bi-ology. I introduce the theoretical concept of the experiment, based mainly on ideqas of stochastic systems theory, and confront it with general ideas of systems theory. In the next part I introduce available experiments and discuss their information con-tent. Namely, I show that light microscopy may be designed to give resolution com-parable to that of electron microscopy and that may be used for experiments using living cells. I show avenues to objective analysis of cell behavior observation. I pro-pose new microscope design, which shall combine advantages of all methods, and steps to be taken to build a model of living cells with predictive power for practical use",2010,2010-07-06,https://arxiv.org/abs/1007.0471,1
1011.3411,Bayesian inference for double Pareto lognormal queues,In this article we describe a method for carrying out Bayesian estimation for the double Pareto lognormal (dPlN) distribution which has been proposed as a model for heavy-tailed phenomena. We apply our approach to estimate the $\mathit{dPlN}/M/1$ and $M/\mathit{dPlN}/1$ queueing systems. These systems cannot be analyzed using standard techniques due to the fact that the dPlN distribution does not possess a Laplace transform in closed form. This difficulty is overcome using some recent approximations for the Laplace transform of the interarrival distribution for the $\mathit{Pareto}/M/1$ system. Our procedure is illustrated with applications in internet traffic analysis and risk theory.,2010,2010-11-16,https://arxiv.org/abs/1011.3411,1
0704.0676,A very massive runaway star from Cygnus OB2,"Aims: We analyze the available information on the star BD+43 3654 to investigate the possibility that it may have had its origin in the massive OB association Cygnus OB2. Methods: We present new spectroscopic observations allowing a reliable spectral classification of the star, and discuss existing MSX observations of its associated bow shock and astrometric information not previously studied. Results: Our observations reveal that BD+43 3654 is a very early and luminous star of spectral type O4If, with an estimated mass of (70 +/- 15) solar masses and an age of about 1.6 Myr. The high spatial resolution of the MSX observations allows us to determine its direction of motion in the plane of the sky by means of the symmetry axis of the well-defined bow shock, which matches well the orientation expected from the proper motion. Tracing back its path across the sky we find that BD+43 3654 was located near the central, densest region of Cygnus OB2 at a time in the past similar to its estimated age. Conclusions: BD+43 3654 turns out to be one of the three most massive runaway stars known, and it most likely formed in the central region of Cygnus OB2. A runaway formation mechanism by means of dynamical ejection is consistent with our results.",2007,2015-05-13,https://arxiv.org/abs/0704.0676,1
1106.5598,Some notes on biasedness and unbiasedness of two-sample Kolmogorov-Smirnov test,This paper deals with two-sample Kolmogorov-Smirnov test and its biasedness. This test is not unbiased in general in case of different sample sizes. We found out most biased distribution for some values of significance level $\alpha$. Moreover we discovered that there exists number of observation and significance level $\alpha$ such that this test is unbiased at level $\alpha$.,2011,2011-06-29,https://arxiv.org/abs/1106.5598,1
0710.5072,A Tribute to Bill Kruskal,"Discussion of ``The William Kruskal Legacy: 1919--2005'' by Stephen E. Fienberg, Stephen M. Stigler and Judith M. Tanur [arXiv:0710.5063]",2007,2009-09-29,https://arxiv.org/abs/0710.5072,1
1801.07567,A Novel Algorithm for Joint Bit and Power Loading for OFDM Systems with Unknown Interference,"In this paper, a novel low complexity bit and power loading algorithm is formulated for orthogonal frequency division multiplexing (OFDM) systems operating in fading environments and in the presence of unknown interference. The proposed non-iterative algorithm jointly maximizes the throughput and minimizes the transmitted power, while guaranteeing a target bit error rate (BER) per subcarrier. Closed-form expressions are derived for the optimal bit and power distributions per subcarrier. The performance of the proposed algorithm is investigated through extensive simulations. A performance comparison with the algorithm in [1] shows the superiority of the proposed algorithm with reduced computational effort.",2018,2019-02-12,https://arxiv.org/abs/1801.07567,2
1407.4452,New Pricing Framework: Options and Bonds,"A unified analytical pricing framework with involvement of the shot noise random process has been introduced and elaborated. Two exactly solvable new models have been developed. The first model has been designed to value options. It is assumed that asset price stochastic dynamics follows a Geometric Shot Noise motion. A new arbitrage-free integro-differential option pricing equation has been found and solved. The put-call parity has been proved and the Greeks have been calculated. Three additional new Greeks associated with market model parameters have been introduced and evaluated. It has been shown that in diffusion approximation the developed option pricing model incorporates the well-known Black-Scholes equation and its solution. The stochastic dynamic origin of the Black-Scholes volatility has been uncovered. The new option pricing model has been generalized based on asset price dynamics modeled by the superposition of Geometric Brownian motion and Geometric Shot Noise. To model stochastic dynamics of a short term interest rate, the second model has been introduced and developed based on Langevin type equation with shot noise. A new bond pricing formula has been obtained. It has been shown that in diffusion approximation the developed bond pricing formula goes into the well-known Vasicek solution. The stochastic dynamic origin of the long-term mean and instantaneous volatility of the Vasicek model has been uncovered. A generalized bond pricing model has been introduced and developed based on short term interest rate stochastic dynamics modeled by superposition of a standard Wiener process and shot noise. Despite the non-Gaussianity of probability distributions involved, all newly elaborated models have the same degree of analytical tractability as the Black-Scholes model and the Vasicek model.",2014,2014-10-15,https://arxiv.org/abs/1407.4452,2
1811.07499,Optimal Iterative Threshold-Kernel Estimation of Jump Diffusion Processes,"In this paper, we propose a new threshold-kernel jump-detection method for jump-diffusion processes, which iteratively applies thresholding and kernel methods in an approximately optimal way to achieve improved finite-sample performance. We use the expected number of jump misclassifications as the objective function to optimally select the threshold parameter of the jump detection scheme. We prove that the objective function is quasi-convex and obtain a new second-order infill approximation of the optimal threshold in closed form. The approximate optimal threshold depends not only on the spot volatility, but also the jump intensity and the value of the jump density at the origin. Estimation methods for these quantities are then developed, where the spot volatility is estimated by a kernel estimator with thresholding and the value of the jump density at the origin is estimated by a density kernel estimator applied to those increments deemed to contain jumps by the chosen thresholding criterion. Due to the interdependency between the model parameters and the approximate optimal estimators built to estimate them, a type of iterative fixed-point algorithm is developed to implement them. Simulation studies for a prototypical stochastic volatility model show that it is not only feasible to implement the higher-order local optimal threshold scheme but also that this is superior to those based only on the first order approximation and/or on average values of the parameters over the estimation time period.",2018,2020-04-07,https://arxiv.org/abs/1811.07499,4
1108.4739,"Variable selection and sensitivity analysis using dynamic trees, with an application to computer code performance tuning","We investigate an application in the automatic tuning of computer codes, an area of research that has come to prominence alongside the recent rise of distributed scientific processing and heterogeneity in high-performance computing environments. Here, the response function is nonlinear and noisy and may not be smooth or stationary. Clearly needed are variable selection, decomposition of influence, and analysis of main and secondary effects for both real-valued and binary inputs and outputs. Our contribution is a novel set of tools for variable selection and sensitivity analysis based on the recently proposed dynamic tree model. We argue that this approach is uniquely well suited to the demands of our motivating example. In illustrations on benchmark data sets, we show that the new techniques are faster and offer richer feature sets than do similar approaches in the static tree and computer experiment literature. We apply the methods in code-tuning optimization, examination of a cold-cache effect, and detection of transformation errors.",2011,2013-04-17,https://arxiv.org/abs/1108.4739,4
0711.3605,Very strict selectional restrictions,"We discuss the characteristics and behaviour of two parallel classes of verbs in two Romance languages, French and Portuguese. Examples of these verbs are Port. abater [gado] and Fr. abattre [b\'etail], both meaning ""slaughter [cattle]"". In both languages, the definition of the class of verbs includes several features: - They have only one essential complement, which is a direct object. - The nominal distribution of the complement is very limited, i.e., few nouns can be selected as head nouns of the complement. However, this selection is not restricted to a single noun, as would be the case for verbal idioms such as Fr. monter la garde ""mount guard"". - We excluded from the class constructions which are reductions of more complex constructions, e.g. Port. afinar [instrumento] com ""tune [instrument] with"".",2007,2007-11-26,https://arxiv.org/abs/0711.3605,1
0710.5582,Computing Equilibria in Anonymous Games,"We present efficient approximation algorithms for finding Nash equilibria in anonymous games, that is, games in which the players utilities, though different, do not differentiate between other players. Our results pertain to such games with many players but few strategies. We show that any such game has an approximate pure Nash equilibrium, computable in polynomial time, with approximation O(s^2 L), where s is the number of strategies and L is the Lipschitz constant of the utilities. Finally, we show that there is a PTAS for finding an epsilon",2007,2007-10-31,https://arxiv.org/abs/0710.5582,1
1109.6090,Robust Parametric Classification and Variable Selection by a Minimum Distance Criterion,"We investigate a robust penalized logistic regression algorithm based on a minimum distance criterion. Influential outliers are often associated with the explosion of parameter vector estimates, but in the context of standard logistic regression, the bias due to outliers always causes the parameter vector to implode, that is shrink towards the zero vector. Thus, using LASSO-like penalties to perform variable selection in the presence of outliers can result in missed detections of relevant covariates. We show that by choosing a minimum distance criterion together with an Elastic Net penalty, we can simultaneously find a parsimonious model and avoid estimation implosion even in the presence of many outliers in the important small $n$ large $p$ situation. Implementation using an MM algorithm is described and performance evaluated.",2011,2014-02-21,https://arxiv.org/abs/1109.6090,3
1110.5558,Geographic Concentration in Portugal and Regional Specific Factors,"This paper pretends to analyze the importance which the natural advantages and local resources are in the manufacturing industry location, in relation with the ""spillovers"" effects and industrial policies. To this, we estimate the Rybczynski equation matrix for the various manufacturing industries in Portugal, at regional level (NUTS II) and for the period 1980 to 1999. Estimations are displayed with the model mentioned and for four different periods, namely 1980 to 1985, from 1986 to 1994, from 1980 to 1994 and from 1995 to 1999. The consideration of the various periods until 1994, aims to capture the effects of our entrance at the, in that time, EEC (European Economic Community) and the consideration of a period from 1995 is because the change in methodology for compiling statistical data taken from this time in Portugal. As a summary conclusion, noted that the location of manufacturing in Portugal is still mostly explained by specific factors, with a tendency to increase in some cases the explanation by these factors, having the effect ""spillovers"" and industrial policies little importance in this context.",2011,2011-10-26,https://arxiv.org/abs/1110.5558,1
0704.0060,Coulomb excitation of unstable nuclei at intermediate energies,"We investigate the Coulomb excitation of low-lying states of unstable nuclei in intermediate energy collisions ($E_{lab}\sim10-500$ MeV/nucleon). It is shown that the cross sections for the $E1$ and $E2$ transitions are larger at lower energies, much less than 10 MeV/nucleon. Retardation effects and Coulomb distortion are found to be both relevant for energies as low as 10 MeV/nucleon and as high as 500 MeV/nucleon. Implications for studies at radioactive beam facilities are discussed.",2007,2008-11-26,https://arxiv.org/abs/0704.0060,2
1908.04569,Forecast Encompassing Tests for the Expected Shortfall,"We introduce new forecast encompassing tests for the risk measure Expected Shortfall (ES). The ES currently receives much attention through its introduction into the Basel III Accords, which stipulate its use as the primary market risk measure for the international banking regulation. We utilize joint loss functions for the pair ES and Value at Risk to set up three ES encompassing test variants. The tests are built on misspecification robust asymptotic theory and we investigate the finite sample properties of the tests in an extensive simulation study. We use the encompassing tests to illustrate the potential of forecast combination methods for different financial assets.",2019,2020-08-31,https://arxiv.org/abs/1908.04569,3
0808.1283,Peptide strings clues to the genesis and treatment of rheumatoid arthritis: rebuilding self-protective immunity amid fungal ruins,"A recent application of the peptide strings concept has yielded novel perceptions on cell growth regulation, for instance that of oncoprotein metastasis. Here, this interdisciplinary approach at the boundary between physics and biology has been applied to gain a more profound insight into rheumatoid arthritis. As a result of the present investigation, this disease could be viewed as due to a metabolic dysregulation/syndrome-associated breakdown in the immunoglobulin A-based surveillance of the potentially pathogenic fungus Candida albicans that subsequently engenders a widespread self-destruction through cross-reactive auto-epitopes, ultimately amounting to the systemic predominance of a pro-inflammatory peptide string. Its therapeutic counterpart equally proposed in this report might serve as a model for future strategies against autoimmunity.",2008,2008-08-12,https://arxiv.org/abs/0808.1283,1
0704.1019,Cyclic cohomology of certain nuclear Fr\'echet and DF algebras,"We give explicit formulae for the continuous Hochschild and cyclic homology and cohomology of certain topological algebras. To this end we show that, for a continuous morphism $\phi: \X\to \Y$ of complexes of complete nuclear $DF$-spaces, the isomorphism of cohomology groups $H^n(\phi): H^n(\X) \to H^n(\Y)$ is automatically topological. The continuous cyclic-type homology and cohomology are described up to topological isomorphism for the following classes of biprojective $\hat{\otimes}$-algebras: the tensor algebra $E \hat{\otimes} F$ generated by the duality $(E, F, < \cdot, \cdot >)$ for nuclear Fr\'echet spaces $E$ and $F$ or for nuclear $DF$-spaces $E$ and $F$; nuclear biprojective K\""{o}the algebras $\lambda(P)$ which are Fr\'echet spaces or $DF$-spaces; the algebra of distributions $\mathcal{E}^*(G)$ on a compact Lie group $G$.",2007,2007-09-12,https://arxiv.org/abs/0704.1019,2
0705.3734,An analogue of the space of conformal blocks in (4k+2)-dimensions,"Based on projective representations of smooth Deligne cohomology groups, we introduce an analogue of the space of conformal blocks to compact oriented (4k+2)-dimensional Riemannian manifolds with boundary. For the standard (4k+2)-dimensional disk, we compute the space concretely to prove that its dimension is finite.",2007,2007-05-28,https://arxiv.org/abs/0705.3734,1
1110.2563,Confidence Intervals for Low-Dimensional Parameters in High-Dimensional Linear Models,"The purpose of this paper is to propose methodologies for statistical inference of low-dimensional parameters with high-dimensional data. We focus on constructing confidence intervals for individual coefficients and linear combinations of several of them in a linear regression model, although our ideas are applicable in a much broad context. The theoretical results presented here provide sufficient conditions for the asymptotic normality of the proposed estimators along with a consistent estimator for their finite-dimensional covariance matrices. These sufficient conditions allow the number of variables to far exceed the sample size. The simulation results presented here demonstrate the accuracy of the coverage probability of the proposed confidence intervals, strongly supporting the theoretical results.",2011,2012-11-05,https://arxiv.org/abs/1110.2563,2
1805.05500,Social diversity for reducing the impact of information cascades on social learning,"Collective behavior in online social media and networks is known to be capable of generating non-intuitive dynamics associated with crowd wisdom and herd behaviour. Even though these topics have been well-studied in social science, the explosive growth of Internet computing and e-commerce makes urgent to understand their effects within the digital society. In this work we explore how the stochasticity introduced by social diversity can help agents involved in a inference process to improve their collective performance. Our results show how social diversity can reduce the undesirable effects of information cascades, in which rational agents choose to ignore personal knowledge in order to follow a predominant social behaviour. Situations where social diversity is never desirable are also distinguished, and consequences of these findings for engineering and social scenarios are discussed.",2018,2018-05-16,https://arxiv.org/abs/1805.05500,1
0704.2556,Families of varieties of general type over compact bases,"Let f: X -> Y be a smooth family of canonically polarized complex varieties over a smooth base. Generalizing the classical Shafarevich hyperbolicity conjecture, Viehweg conjectured that Y is necessarily of log general type if the family has maximal variation. A somewhat stronger and more precise version of Viehweg's conjecture was shown by the authors in arXiv:math/0511378 in the case where Y is a quasi-projective surface. Assuming that the minimal model program holds, this very short paper proves the same result for projective base manifolds Y of arbitrary dimension.",2007,2007-05-23,https://arxiv.org/abs/0704.2556,1
0704.1119,Massive Stars: From the VLT to the ELT,"New facilities and technologies have advanced our understanding of massive stars significantly over the past 30 years. Here I introduce a new large survey of massive stars using VLT-FLAMES, noting the target fields and observed binary fractions. These data have been used for the first empirical test of the metallicity dependence of the intensity of stellar winds, finding good agreement with theory -- an important result for the evolutionary models that are used to interpret distant clusters, starbursts, and star-forming galaxies. Looking ahead, plans for future Extremely Large Telescopes (ELTs) are now undergoing significant development, and offer the exciting prospect of observing spatially-resolved massive stars well beyond the Local Group.",2007,2007-05-23,https://arxiv.org/abs/0704.1119,1
1008.1636,Censoring Outdegree Compromises Inferences of Social Network Peer Effects and Autocorrelation,"I examine the consequences of modelling contagious influence in a social network with incomplete edge information, namely in the situation where each individual may name a limited number of friends, so that extra outbound ties are censored. In particular, I consider a prototypical time series configuration where a property of the ""ego"" is affected in a causal fashion by the properties of their ""alters"" at a previous time point, both in the total number of alters as well as the deviation from a central value. This is considered with three potential methods for naming one's friends: a strict upper limit on the number of declarations, a flexible limit, and an instruction where a person names a prespecified fraction of their friends. I find that one of two effects is present in the estimation of these effects: either that the size of the effect is inflated in magnitude, or that the estimators instead are centered about zero rather than related to the true effect. The degree of heterogeneity in friend count is one of the major factors into whether such an analysis can be salvaged by post-hoc adjustments.",2010,2011-01-07,https://arxiv.org/abs/1008.1636,2
0801.0124,Simulating Genomes and Populations in the Mutation Space: An example with the evolution of HIV drug resistance,"When simulating biological populations under different evolutionary genetic models, backward or forward strategies can be followed. Backward simulations, also called coalescent-based simulations, are computationally very efficient. However, this framework imposes several limitations that forward simulation does not. In this work, a new simple and efficient model to perform forward simulation of populations and/or genomes is proposed. The basic idea considers an individual as the differences (mutations) between this individual and a reference or consensus genotype. Thus, this individual is no longer represented by its complete sequence or genotype. An example of the efficiency of the new model with respect to a more classical forward one is demonstrated. This example models the evolution of HIV resistance using the B_FR.HXB2 reference sequence to study the emergence of known resistance mutants to Zidovudine and Didanosine drugs",2007,2008-01-03,https://arxiv.org/abs/0801.0124,1
1010.4236,Maximum Likelihood Joint Tracking and Association in a Strong Clutter without Combinatorial Complexity,"We have developed an efficient algorithm for the maximum likelihood joint tracking and association problem in a strong clutter for GMTI data. By using an iterative procedure of the dynamic logic process ""from vague-to-crisp,"" the new tracker overcomes combinatorial complexity of tracking in highly-cluttered scenarios and results in a significant improvement in signal-to-clutter ratio.",2010,2010-10-21,https://arxiv.org/abs/1010.4236,1
0705.1585,HMM Speaker Identification Using Linear and Non-linear Merging Techniques,"Speaker identification is a powerful, non-invasive and in-expensive biometric technique. The recognition accuracy, however, deteriorates when noise levels affect a specific band of frequency. In this paper, we present a sub-band based speaker identification that intends to improve the live testing performance. Each frequency sub-band is processed and classified independently. We also compare the linear and non-linear merging techniques for the sub-bands recognizer. Support vector machines and Gaussian Mixture models are the non-linear merging techniques that are investigated. Results showed that the sub-band based method used with linear merging techniques enormously improved the performance of the speaker identification over the performance of wide-band recognizers when tested live. A live testing improvement of 9.78% was achieved",2007,2007-05-23,https://arxiv.org/abs/0705.1585,1
0705.3767,Contracted ideals and the Groebner fan of the rational normal curve,"The paper has two goals: the study the associated graded ring of contracted homogeneous ideals in $K[x,y]$ and the study of the Groebner fan of the ideal $P$ of the rational normal curve in ${\bf P}^d$. These two problems are, quite surprisingly, very tightly related. We completely classify the contracted ideals with a Cohen-Macaulay associated graded rings in terms of the numerical invariants arising from Zariski's factorization. We determine explicitly all the initial ideals (monomial or not) of $P$ that are Cohen-Macaulay.",2007,2007-10-11,https://arxiv.org/abs/0705.3767,2
1902.03310,Preserve or retreat? Willingness-to-pay for Coastline Protection in New South Wales,"Coastal erosion is a global and pervasive phenomenon that predicates a need for a strategic approach to the future management of coastal values and assets (both built and natural), should we invest in protective structures like seawalls that aim to preserve specific coastal features, or allow natural coastline retreat to preserve sandy beaches and other coastal ecosystems. Determining the most suitable management approach in a specific context requires a better understanding of the full suite of economic values the populations holds for coastal assets, including non-market values. In this study, we characterise New South Wales residents willingness to pay to maintain sandy beaches (width and length). We use an innovative application of a Latent Class Binary Logit model to deal with Yea-sayers and Nay-sayers, as well as revealing the latent heterogeneity among sample members. We find that 65% of the population would be willing to pay some amount of levy, dependent on the policy setting. In most cases, there is no effect of degree of beach deterioration characterised as loss of width and length of sandy beaches of between 5% and 100% on respondents willingness to pay for a management levy. This suggests that respondents who agreed to pay a management levy were motivated to preserve sandy beaches in their current state irrespective of the severity of sand loss likely to occur as a result of coastal erosion. Willingness to pay also varies according to beach type (amongst Iconic, Main, Bay and Surf beaches) a finding that can assist with spatial prioritisation of coastal management. Not recognizing the presence of nay-sayers in the data or recognizing them but eliminating them from the estimation will result in biased WTP results and, consequently, biased policy propositions by coastal managers.",2019,2019-05-28,https://arxiv.org/abs/1902.03310,1
0803.1838,Python - All a Scientist Needs,"Any cutting-edge scientific research project requires a myriad of computational tools for data generation, management, analysis and visualization. Python is a flexible and extensible scientific programming platform that offered the perfect solution in our recent comparative genomics investigation (J. B. Lucks, D. R. Nelson, G. Kudla, J. B. Plotkin. Genome landscapes and bacteriophage codon usage, PLoS Computational Biology, 4, 1000001, 2008). In this paper, we discuss the challenges of this project, and how the combined power of Biopython, Matplotlib and SWIG were utilized for the required computational tasks. We finish by discussing how python goes beyond being a convenient programming language, and promotes good scientific practice by enabling clean code, integration with professional programming techniques such as unit testing, and strong data provenance.",2008,2008-03-14,https://arxiv.org/abs/0803.1838,1
0704.2377,A note on Seshadri constants on general $K3$ surfaces,"We prove a lower bound on the Seshadri constant $\epsilon (L)$ on a $K3$ surface $S$ with $\Pic S \simeq \ZZ[L]$. In particular, we obtain that $\epsilon (L)=\alpha$ if $L^2=\alpha^2$ for an integer $\alpha$.",2007,2007-05-23,https://arxiv.org/abs/0704.2377,1
0707.0763,The Requirements for Ontologies in Medical Data Integration: A Case Study,"Evidence-based medicine is critically dependent on three sources of information: a medical knowledge base, the patients medical record and knowledge of available resources, including where appropriate, clinical protocols. Patient data is often scattered in a variety of databases and may, in a distributed model, be held across several disparate repositories. Consequently addressing the needs of an evidence-based medicine community presents issues of biomedical data integration, clinical interpretation and knowledge management. This paper outlines how the Health-e-Child project has approached the challenge of requirements specification for (bio-) medical data integration, from the level of cellular data, through disease to that of patient and population. The approach is illuminated through the requirements elicitation and analysis of Juvenile Idiopathic Arthritis (JIA), one of three diseases being studied in the EC-funded Health-e-Child project.",2007,2007-07-06,https://arxiv.org/abs/0707.0763,1
0711.3205,Relay Subset Selection in Wireless Networks Using Partial Decode-and-Forward Transmission,"This paper considers the problem of selecting a subset of nodes in a two-hop wireless network to act as relays in aiding the communication between the source-destination pair. Optimal relay subset selection with the objective of maximizing the overall throughput is a difficult problem that depends on multiple factors including node locations, queue lengths and power consumption. A partial decode-and-forward strategy is applied in this paper to improve the tractability of the relay selection problem and performance of the overall network. Note that the number of relays selected ultimately determines the performance of the network. This paper benchmarks this performance by determining the net diversity achieved using the relays selected and the partial decode-and-forward strategy. This framework is subsequently used to further transform relay selection into a simpler relay placement problem, and two proximity-based approximation algorithms are developed to determine the appropriate set of relays to be selected in the network. Other selection strategies such as random relay selection and a greedy algorithm that relies on channel state information are also presented. This paper concludes by showing that the proposed proximity-based relay selection strategies yield near-optimal expected rates for a small number of selected relays.",2007,2008-05-23,https://arxiv.org/abs/0711.3205,2
1805.01759,A fast and accurate basis pursuit denoising algorithm with application to super-resolving tomographic SAR,"$L_1$ regularization is used for finding sparse solutions to an underdetermined linear system. As sparse signals are widely expected in remote sensing, this type of regularization scheme and its extensions have been widely employed in many remote sensing problems, such as image fusion, target detection, image super-resolution, and others and have led to promising results. However, solving such sparse reconstruction problems is computationally expensive and has limitations in its practical use. In this paper, we proposed a novel efficient algorithm for solving the complex-valued $L_1$ regularized least squares problem. Taking the high-dimensional tomographic synthetic aperture radar (TomoSAR) as a practical example, we carried out extensive experiments, both with simulation data and real data, to demonstrate that the proposed approach can retain the accuracy of second order methods while dramatically speeding up the processing by one or two orders. Although we have chosen TomoSAR as the example, the proposed method can be generally applied to any spectral estimation problems.",2018,2018-05-07,https://arxiv.org/abs/1805.01759,1
1809.07747,Shapley-like values without symmetry,"Following the work of Lloyd Shapley on the Shapley value, and tangentially the work of Guillermo Owen, we offer an alternative non-probabilistic formulation of part of the work of Robert J. Weber in his 1978 paper ""Probabilistic values for games."" Specifically, we focus upon efficient but not symmetric allocations of value for cooperative games. We retain standard efficiency and linearity, and offer an alternative condition, ""reasonableness,"" to replace the other usual axioms. In the pursuit of the result, we discover properties of the linear maps that describe the allocations. This culminates in a special class of games for which any other map that is ""reasonable, efficient"" can be written as a convex combination of members of this special class of allocations, via an application of the Krein-Milman theorem.",2018,2019-05-13,https://arxiv.org/abs/1809.07747,2
1301.4442,USLV: Unspanned Stochastic Local Volatility Model,"We propose a new framework for modeling stochastic local volatility, with potential applications to modeling derivatives on interest rates, commodities, credit, equity, FX etc., as well as hybrid derivatives. Our model extends the linearity-generating unspanned volatility term structure model by Carr et al. (2011) by adding a local volatility layer to it. We outline efficient numerical schemes for pricing derivatives in this framework for a particular four-factor specification (two ""curve"" factors plus two ""volatility"" factors). We show that the dynamics of such a system can be approximated by a Markov chain on a two-dimensional space (Z_t,Y_t), where coordinates Z_t and Y_t are given by direct (Kroneker) products of values of pairs of curve and volatility factors, respectively. The resulting Markov chain dynamics on such partly ""folded"" state space enables fast pricing by the standard backward induction. Using a nonparametric specification of the Markov chain generator, one can accurately match arbitrary sets of vanilla option quotes with different strikes and maturities. Furthermore, we consider an alternative formulation of the model in terms of an implied time change process. The latter is specified nonparametrically, again enabling accurate calibration to arbitrary sets of vanilla option quotes.",2013,2013-03-29,https://arxiv.org/abs/1301.4442,2
0911.0985,"Comments on ""Particle Markov chain Monte Carlo"" by C. Andrieu, A. Doucet, and R. Hollenstein","This is the compilation of our comments submitted to the Journal of the Royal Statistical Society, Series B, to be published within the discussion of the Read Paper of Andrieu, Doucet and Hollenstein.",2009,2009-11-06,https://arxiv.org/abs/0911.0985,1
1808.10753,Spectral pre-modulation of training examples enhances the spatial resolution of the Phase Extraction Neural Network (PhENN),"The Phase Extraction Neural Network (PhENN) is a computational architecture, based on deep machine learning, for lens-less quantitative phase retrieval from raw intensity data. PhENN is a deep convolutional neural network trained through examples consisting of pairs of true phase objects and their corresponding intensity diffraction patterns; thereafter, given a test raw intensity pattern PhENN is capable of reconstructing the original phase object robustly, in many cases even for objects outside the database where the training examples were drawn from. Here, we show that the spatial frequency content of the training examples is an important factor limiting PhENN's spatial frequency response. For example, if the training database is relatively sparse in high spatial frequencies, as most natural scenes are, PhENN's ability to resolve fine spatial features in test patterns will be correspondingly limited. To combat this issue, we propose ""flattening"" the power spectral density of the training examples before presenting them to PhENN. For phase objects following the statistics of natural scenes, we demonstrate experimentally that the spectral pre-modulation method enhances the spatial resolution of PhENN by a factor of 2.",2018,2018-11-14,https://arxiv.org/abs/1808.10753,1
1110.4304,Readouts for Echo-state Networks Built using Locally Regularized Orthogonal Forward Regression,"Echo state network (ESN) is viewed as a temporal non-orthogonal expansion with pseudo-random parameters. Such expansions naturally give rise to regressors of various relevance to a teacher output. We illustrate that often only a certain amount of the generated echo-regressors effectively explain the variance of the teacher output and also that sole local regularization is not able to provide in-depth information concerning the importance of the generated regressors. The importance is therefore determined by a joint calculation of the individual variance contributions and Bayesian relevance using locally regularized orthogonal forward regression (LROFR) algorithm. This information can be advantageously used in a variety of ways for an in-depth analysis of an ESN structure and its state-space parameters in relation to the unknown dynamics of the underlying problem. We present locally regularized linear readout built using LROFR. The readout may have a different dimensionality than an ESN model itself, and besides improving robustness and accuracy of an ESN it relates the echo-regressors to different features of the training data and may determine what type of an additional readout is suitable for a task at hand. Moreover, as flexibility of the linear readout has limitations and might sometimes be insufficient for certain tasks, we also present a radial basis function (RBF) readout built using LROFR. It is a flexible and parsimonious readout with excellent generalization abilities and is a viable alternative to readouts based on a feed-forward neural network (FFNN) or an RBF net built using relevance vector machine (RVM).",2011,2012-07-03,https://arxiv.org/abs/1110.4304,3
0811.3122,A multiscale view on inverse statistics and gain/loss asymmetry in financial time series,"Researchers have studied the first passage time of financial time series and observed that the smallest time interval needed for a stock index to move a given distance is typically shorter for negative than for positive price movements. The same is not observed for the index constituents, the individual stocks. We use the discrete wavelet transform to illustrate that this is a long rather than short time scale phenomenon -- if enough low frequency content of the price process is removed, the asymmetry disappears. We also propose a new model, which explain the asymmetry by prolonged, correlated down movements of individual stocks.",2008,2009-03-23,https://arxiv.org/abs/0811.3122,1
1205.4008,Price manipulation in a market impact model with dark pool,"For a market impact model, price manipulation and related notions play a role that is similar to the role of arbitrage in a derivatives pricing model. Here, we give a systematic investigation into such regularity issues when orders can be executed both at a traditional exchange and in a dark pool. To this end, we focus on a class of dark-pool models whose market impact at the exchange is described by an Almgren--Chriss model. Conditions for the absence of price manipulation for all Almgren--Chriss models include the absence of temporary cross-venue impact, the presence of full permanent cross-venue impact, and the additional penalization of orders executed in the dark pool. When a particular Almgren--Chriss model has been fixed, we show by a number of examples that the regularity of the dark-pool model hinges in a subtle way on the interplay of all model parameters and on the liquidation time constraint. The paper can also be seen as a case study for the regularity of market impact models in general.",2012,2014-05-08,https://arxiv.org/abs/1205.4008,4
0706.1162,The multiple viewpoints as approach to information retrieval within collaborative development context,"Nowadays, to achieve competitive advantage, the industrial companies are considering that success is sustained to great product development. That is to manage the product throughout its entire lifecycle. Achieving this goal requires a tight collaboration between actors from a wide variety of domains, using different software tools producing various product data types and formats. The actors' collaboration is mainly based on the exchange /share product information. The representation of the actors' viewpoints is the underlying requirement of the collaborative product development. The multiple viewpoints approach was designed to provide an organizational framework following the actors' perspectives in the collaboration, and their relationships. The approach acknowledges the inevitability of multiple integration of product information as different views, promotes gathering of actors' interest, and encourages retrieved adequate information while providing support for integration through PLM and/or SCM collaboration. In this paper, a multiple viewpoints representation is proposed. The product, process, organization information models are discussed. A series of issues referring to the viewpoints representation are discussed in detail. Based on XML standard, taking electrical connector as an example, an application case of part of product information modeling is stated.",2007,2008-02-19,https://arxiv.org/abs/0706.1162,1
0909.1234,High-dimensional Graphical Model Search with gRapHD R Package,"This paper presents the R package gRapHD for efficient selection of high-dimensional undirected graphical models. The package provides tools for selecting trees, forests and decomposable models minimizing information criteria such as AIC or BIC, and for displaying the independence graphs of the models. It has also some useful tools for analysing graphical structures. It supports the use of discrete, continuous, or both types of variables simultaneously.",2009,2019-09-24,https://arxiv.org/abs/0909.1234,4
0704.1555,An information-based traffic control in a public conveyance system: reduced clustering and enhanced efficiency,"A new public conveyance model applicable to buses and trains is proposed in this paper by using stochastic cellular automaton. We have found the optimal density of vehicles, at which the average velocity becomes maximum, significantly depends on the number of stops and passengers behavior of getting on a vehicle at stops. The efficiency of the hail-and-ride system is also discussed by comparing the different behavior of passengers. Moreover, we have found that a big cluster of vehicles is divided into small clusters, by incorporating information of the number of vehicles between successive stops.",2007,2007-09-19,https://arxiv.org/abs/0704.1555,2
0708.3127,Question on Conditional Entropy,"The problems of conditional entropy's definition and the formula to compute conditional entropy are analyzed from various perspectives, and the corrected computing formula is presented. Examples are given to prove the conclusion that conditional entropy never be increased is not absolute, thus the representation that information is to decrease uncertainty in the definition of information is not absolutely correct.",2007,2007-08-24,https://arxiv.org/abs/0708.3127,1
1012.3795,Estimating Networks With Jumps,"We study the problem of estimating a temporally varying coefficient and varying structure (VCVS) graphical model underlying nonstationary time series data, such as social states of interacting individuals or microarray expression profiles of gene networks, as opposed to i.i.d. data from an invariant model widely considered in current literature of structural estimation. In particular, we consider the scenario in which the model evolves in a piece-wise constant fashion. We propose a procedure that minimizes the so-called TESLA loss (i.e., temporally smoothed L1 regularized regression), which allows jointly estimating the partition boundaries of the VCVS model and the coefficient of the sparse precision matrix on each block of the partition. A highly scalable proximal gradient method is proposed to solve the resultant convex optimization problem; and the conditions for sparsistent estimation and the convergence rate of both the partition boundaries and the network structure are established for the first time for such estimators.",2010,2010-12-21,https://arxiv.org/abs/1012.3795,2
0903.3796,A general multivariate latent growth model with applications in student careers Data warehouses,The evaluation of the formative process in the University system has been assuming an ever increasing importance in the European countries. Within this context the analysis of student performance and capabilities plays a fundamental role. In this work we propose a multivariate latent growth model for studying the performances of a cohort of students of the University of Bologna. The model proposed is innovative since it is composed by: (1) multivariate growth models that allow to capture the different dynamics of student performance indicators over time and (2) a factor model that allows to measure the general latent student capability. The flexibility of the model proposed allows its applications in several fields such as socio-economic settings in which personal behaviours are studied by using panel data.,2009,2012-06-26,https://arxiv.org/abs/0903.3796,1
0711.2914,Image Classification Using SVMs: One-against-One Vs One-against-All,"Support Vector Machines (SVMs) are a relatively new supervised classification technique to the land cover mapping community. They have their roots in Statistical Learning Theory and have gained prominence because they are robust, accurate and are effective even when using a small training sample. By their nature SVMs are essentially binary classifiers, however, they can be adopted to handle the multiple classification tasks common in remote sensing studies. The two approaches commonly used are the One-Against-One (1A1) and One-Against-All (1AA) techniques. In this paper, these approaches are evaluated in as far as their impact and implication for land cover mapping. The main finding from this research is that whereas the 1AA technique is more predisposed to yielding unclassified and mixed pixels, the resulting classification accuracy is not significantly different from 1A1 approach. It is the authors conclusion therefore that ultimately the choice of technique adopted boils down to personal preference and the uniqueness of the dataset at hand.",2007,2007-11-20,https://arxiv.org/abs/0711.2914,1
1811.02764,Receiver Design for Faster-than-Nyquist Signaling: Deep-learning-based Architectures,"Faster-than-Nyquist (FTN) is a promising paradigm to improve bandwidth utilization at the expense of additional intersymbol interference (ISI). In this paper, we apply state-of-the-art deep learning (DL) technology into receiver design for FTN signaling and propose two DL-based new architectures. Firstly, we propose an FTN signal detection based on DL and connect it with the successive interference cancellation (SIC) to replace traditional detection algorithms. Simulation results show that this architecture can achieve near-optimal performance in both uncoded and coded scenarios. Additionally, we propose a DL-based joint signal detection and decoding for FTN signaling to replace the complete baseband part in traditional FTN receivers. The performance of this new architecture has also been illustrated by simulation results. Finally, both the proposed DL-based receiver architecture has the robustness to signal to noise ratio (SNR). In a nutshell, DL has been proved to be a powerful tool for the FTN receiver design.",2018,2020-08-03,https://arxiv.org/abs/1811.02764,7
0704.0895,Gorenstein locus of minuscule Schubert varieties,"In this article, we describe explicitely the Gorenstein locus of all minuscule Schubert varieties. This proves a special case of a conjecture of A. Woo and A. Yong (see math.AG/0603273) on the Gorenstein locus of Schubert varieties.",2007,2007-05-23,https://arxiv.org/abs/0704.0895,1
1712.03993,Learning Based Segmentation of CT Brain Images: Application to Post-Operative Hydrocephalic Scans,"Objective: Hydrocephalus is a medical condition in which there is an abnormal accumulation of cerebrospinal fluid (CSF) in the brain. Segmentation of brain imagery into brain tissue and CSF (before and after surgery, i.e. pre-op vs. postop) plays a crucial role in evaluating surgical treatment. Segmentation of pre-op images is often a relatively straightforward problem and has been well researched. However, segmenting post-operative (post-op) computational tomographic (CT)-scans becomes more challenging due to distorted anatomy and subdural hematoma collections pressing on the brain. Most intensity and feature based segmentation methods fail to separate subdurals from brain and CSF as subdural geometry varies greatly across different patients and their intensity varies with time. We combat this problem by a learning approach that treats segmentation as supervised classification at the pixel level, i.e. a training set of CT scans with labeled pixel identities is employed. Methods: Our contributions include: 1.) a dictionary learning framework that learns class (segment) specific dictionaries that can efficiently represent test samples from the same class while poorly represent corresponding samples from other classes, 2.) quantification of associated computation and memory footprint, and 3.) a customized training and test procedure for segmenting post-op hydrocephalic CT images. Results: Experiments performed on infant CT brain images acquired from the CURE Children's Hospital of Uganda reveal the success of our method against the state-of-the-art alternatives. We also demonstrate that the proposed algorithm is computationally less burdensome and exhibits a graceful degradation against number of training samples, enhancing its deployment potential.",2017,2018-09-11,https://arxiv.org/abs/1712.03993,1
